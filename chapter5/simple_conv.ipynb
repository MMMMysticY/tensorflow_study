{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 卷积网络\n",
    "## 感受野 receptive filed\n",
    "卷积网络各输出特征图汇总每个像素点 在原始图片上映射回去的区域大小\n",
    "对于一个5*5的图像来说 两个3 * 3的卷积核和一个5 * 5的卷积核最后都将特征变为1 * 1\n",
    "按照感受野理论来说 他们的效果是一样的\n",
    "但是两个3 * 3的参数量小于一个5 * 5的 所以多层的conv比较合适\n",
    "## 填充 padding\n",
    "使用填充padding 一般能够使得conv之后维度不变\n",
    "当使用padding时 输出的维度是 padding = SAME 输入长度/步长(向上取整) 不使用padding时 输出的维度是 padding = VALID 输入长度-核长+1 / 步长 (向上取整)\n",
    "## Conv2D\n",
    "filters 卷积核个数\n",
    "kernel_size 卷积核尺寸\n",
    "strides 滑动步长\n",
    "padding same / valid\n",
    "activation # 如果有batch norm不写这个\n",
    "input_shape\n",
    "## batch norm\n",
    "batch norm的作用是norm 拉回0均值1方差的标准分布\n",
    "batch norm一般用在卷积之后 激活函数之前\n",
    "在conv中维度本来是[batch, kernel, width, height] batch norm的效果是在batch * width * height维度上进行norm 就是在多个卷积核的每个卷积核上各自归一化\n",
    "\n",
    "## 池化pool\n",
    "**最大池化可以保留图片轮廓 均值池化可以保留背景特征**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('pooled output shape', result['pooled_output'].shape)\n",
    "print('sequence output shape: ', result['sequence_output'].shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.datasets import load_iris\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow.keras as keras\n",
    "from keras import Model\n",
    "from tensorflow.keras import Model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from PIL import Image\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1' # 使用 GPU 1\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "physical_devices = tf.config.list_physical_devices(\"GPU\")\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0],True)\n",
    "logical_devices = tf.config.list_logical_devices(\"GPU\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# dataset\n",
    "cifar10 = keras.datasets.cifar10\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train_shape:  (50000, 32, 32, 3)\n",
      "y_train_shape:  (50000, 1)\n",
      "x_test_shape:  (10000, 32, 32, 3)\n",
      "y_test_shape:  (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "print('x_train_shape: ', x_train.shape)\n",
    "print('y_train_shape: ', y_train.shape)\n",
    "print('x_test_shape: ', x_test.shape)\n",
    "print('y_test_shape: ', y_test.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "(32, 32, 3)"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pic = x_train[6]\n",
    "test_pic.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2]\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(test_pic)\n",
    "print(y_train[6])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# 归一化\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# model\n",
    "class picModel(Model):\n",
    "    def __init__(self, filters, kernel_size, padding, conv_stride, pool_size, pool_stride, hidden_size, dropout_rate):\n",
    "        super(picModel, self).__init__()\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = padding\n",
    "        self.conv_stride = conv_stride\n",
    "        self.pool_size = pool_size\n",
    "        self.pool_stride = pool_stride\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        self.conv1 = keras.layers.Conv2D(filters = self.filters, kernel_size=self.kernel_size, strides=self.conv_stride,\n",
    "                                         padding=self.padding, input_shape=(32,32,3), data_format='channels_last')\n",
    "        self.bn = keras.layers.BatchNormalization()\n",
    "        self.act = keras.layers.Activation('relu')\n",
    "        self.maxpool = keras.layers.MaxPool2D(pool_size=self.pool_size, strides=self.pool_stride)\n",
    "        self.flatten = keras.layers.Flatten()\n",
    "        self.d1 = keras.layers.Dense(units=128, activation='relu')\n",
    "        self.d2 = keras.layers.Dense(units=10, activation='softmax')\n",
    "        self.dropout = keras.layers.Dropout(self.dropout_rate)\n",
    "\n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.bn(x)\n",
    "        x = self.act(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.d1(x)\n",
    "        x = self.dropout(x)\n",
    "        y = self.d2(x)\n",
    "        return y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "model = picModel(filters=6,kernel_size=(5,5), padding='same',conv_stride=1,pool_size=(2,2),pool_stride=2,hidden_size=128,dropout_rate=0.2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    metrics=['sparse_categorical_accuracy']\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "checkpoint_save_path = 'tensorflow_study/chapter5/model_dir/simple_model/baseline.ckpt'\n",
    "\n",
    "cp_callback = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_save_path,\n",
    "    save_weights_only=True,\n",
    "    save_best_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode = 'max',\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "776/782 [============================>.] - ETA: 0s - loss: 1.6920 - sparse_categorical_accuracy: 0.3922WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 10s 4ms/step - loss: 1.6911 - sparse_categorical_accuracy: 0.3927\n",
      "Epoch 2/200\n",
      "778/782 [============================>.] - ETA: 0s - loss: 1.4171 - sparse_categorical_accuracy: 0.4918WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.4169 - sparse_categorical_accuracy: 0.4919\n",
      "Epoch 3/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 1.3371 - sparse_categorical_accuracy: 0.5189WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.3370 - sparse_categorical_accuracy: 0.5191\n",
      "Epoch 4/200\n",
      "775/782 [============================>.] - ETA: 0s - loss: 1.2904 - sparse_categorical_accuracy: 0.5390WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.2904 - sparse_categorical_accuracy: 0.5389\n",
      "Epoch 5/200\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 1.2513 - sparse_categorical_accuracy: 0.5514 - val_loss: 1.2916 - val_sparse_categorical_accuracy: 0.5398\n",
      "Epoch 6/200\n",
      "771/782 [============================>.] - ETA: 0s - loss: 1.2168 - sparse_categorical_accuracy: 0.5650WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.2173 - sparse_categorical_accuracy: 0.5649\n",
      "Epoch 7/200\n",
      "772/782 [============================>.] - ETA: 0s - loss: 1.1868 - sparse_categorical_accuracy: 0.5750WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.1868 - sparse_categorical_accuracy: 0.5751\n",
      "Epoch 8/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 1.1591 - sparse_categorical_accuracy: 0.5870WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.1596 - sparse_categorical_accuracy: 0.5869\n",
      "Epoch 9/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 1.1464 - sparse_categorical_accuracy: 0.5895WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.1466 - sparse_categorical_accuracy: 0.5895\n",
      "Epoch 10/200\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.1299 - sparse_categorical_accuracy: 0.5977 - val_loss: 1.1785 - val_sparse_categorical_accuracy: 0.5931\n",
      "Epoch 11/200\n",
      "777/782 [============================>.] - ETA: 0s - loss: 1.1130 - sparse_categorical_accuracy: 0.6037WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.1134 - sparse_categorical_accuracy: 0.6035\n",
      "Epoch 12/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 1.0962 - sparse_categorical_accuracy: 0.6098WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.0962 - sparse_categorical_accuracy: 0.6098\n",
      "Epoch 13/200\n",
      "774/782 [============================>.] - ETA: 0s - loss: 1.0846 - sparse_categorical_accuracy: 0.6130WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.0846 - sparse_categorical_accuracy: 0.6130\n",
      "Epoch 14/200\n",
      "773/782 [============================>.] - ETA: 0s - loss: 1.0682 - sparse_categorical_accuracy: 0.6174WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.0676 - sparse_categorical_accuracy: 0.6175\n",
      "Epoch 15/200\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.0577 - sparse_categorical_accuracy: 0.6227 - val_loss: 1.1483 - val_sparse_categorical_accuracy: 0.5945\n",
      "Epoch 16/200\n",
      "773/782 [============================>.] - ETA: 0s - loss: 1.0421 - sparse_categorical_accuracy: 0.6291WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.0424 - sparse_categorical_accuracy: 0.6288\n",
      "Epoch 17/200\n",
      "772/782 [============================>.] - ETA: 0s - loss: 1.0355 - sparse_categorical_accuracy: 0.6322WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.0358 - sparse_categorical_accuracy: 0.6319\n",
      "Epoch 18/200\n",
      "772/782 [============================>.] - ETA: 0s - loss: 1.0188 - sparse_categorical_accuracy: 0.6347WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.0202 - sparse_categorical_accuracy: 0.6343\n",
      "Epoch 19/200\n",
      "771/782 [============================>.] - ETA: 0s - loss: 1.0091 - sparse_categorical_accuracy: 0.6408WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.0095 - sparse_categorical_accuracy: 0.6406\n",
      "Epoch 20/200\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.0017 - sparse_categorical_accuracy: 0.6439 - val_loss: 1.1552 - val_sparse_categorical_accuracy: 0.6017\n",
      "Epoch 21/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.9896 - sparse_categorical_accuracy: 0.6468WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.9896 - sparse_categorical_accuracy: 0.6469\n",
      "Epoch 22/200\n",
      "773/782 [============================>.] - ETA: 0s - loss: 0.9816 - sparse_categorical_accuracy: 0.6510WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.9815 - sparse_categorical_accuracy: 0.6510\n",
      "Epoch 23/200\n",
      "771/782 [============================>.] - ETA: 0s - loss: 0.9697 - sparse_categorical_accuracy: 0.6532WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.9705 - sparse_categorical_accuracy: 0.6527\n",
      "Epoch 24/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.9632 - sparse_categorical_accuracy: 0.6564WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.9632 - sparse_categorical_accuracy: 0.6564\n",
      "Epoch 25/200\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.9562 - sparse_categorical_accuracy: 0.6608 - val_loss: 1.1257 - val_sparse_categorical_accuracy: 0.6133\n",
      "Epoch 26/200\n",
      "774/782 [============================>.] - ETA: 0s - loss: 0.9465 - sparse_categorical_accuracy: 0.6637WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.9475 - sparse_categorical_accuracy: 0.6634\n",
      "Epoch 27/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.9379 - sparse_categorical_accuracy: 0.6639WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.9377 - sparse_categorical_accuracy: 0.6640\n",
      "Epoch 28/200\n",
      "771/782 [============================>.] - ETA: 0s - loss: 0.9273 - sparse_categorical_accuracy: 0.6707WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.9290 - sparse_categorical_accuracy: 0.6702\n",
      "Epoch 29/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.9230 - sparse_categorical_accuracy: 0.6717WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.9227 - sparse_categorical_accuracy: 0.6718\n",
      "Epoch 30/200\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.9202 - sparse_categorical_accuracy: 0.6728 - val_loss: 1.1322 - val_sparse_categorical_accuracy: 0.6099\n",
      "Epoch 31/200\n",
      "778/782 [============================>.] - ETA: 0s - loss: 0.9139 - sparse_categorical_accuracy: 0.6753WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.9144 - sparse_categorical_accuracy: 0.6752\n",
      "Epoch 32/200\n",
      "773/782 [============================>.] - ETA: 0s - loss: 0.9080 - sparse_categorical_accuracy: 0.6758WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.9088 - sparse_categorical_accuracy: 0.6754\n",
      "Epoch 33/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.9032 - sparse_categorical_accuracy: 0.6769WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.9033 - sparse_categorical_accuracy: 0.6769\n",
      "Epoch 34/200\n",
      "778/782 [============================>.] - ETA: 0s - loss: 0.9037 - sparse_categorical_accuracy: 0.6776WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.9038 - sparse_categorical_accuracy: 0.6775\n",
      "Epoch 35/200\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.8930 - sparse_categorical_accuracy: 0.6803 - val_loss: 1.2260 - val_sparse_categorical_accuracy: 0.5886\n",
      "Epoch 36/200\n",
      "775/782 [============================>.] - ETA: 0s - loss: 0.8830 - sparse_categorical_accuracy: 0.6852WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.8828 - sparse_categorical_accuracy: 0.6852\n",
      "Epoch 37/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.8815 - sparse_categorical_accuracy: 0.6853WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.8813 - sparse_categorical_accuracy: 0.6852\n",
      "Epoch 38/200\n",
      "774/782 [============================>.] - ETA: 0s - loss: 0.8716 - sparse_categorical_accuracy: 0.6877WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.8719 - sparse_categorical_accuracy: 0.6876\n",
      "Epoch 39/200\n",
      "775/782 [============================>.] - ETA: 0s - loss: 0.8635 - sparse_categorical_accuracy: 0.6920WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.8643 - sparse_categorical_accuracy: 0.6918\n",
      "Epoch 40/200\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.8563 - sparse_categorical_accuracy: 0.6944 - val_loss: 1.0901 - val_sparse_categorical_accuracy: 0.6301\n",
      "Epoch 41/200\n",
      "778/782 [============================>.] - ETA: 0s - loss: 0.8526 - sparse_categorical_accuracy: 0.6961WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.8525 - sparse_categorical_accuracy: 0.6961\n",
      "Epoch 42/200\n",
      "776/782 [============================>.] - ETA: 0s - loss: 0.8513 - sparse_categorical_accuracy: 0.6959WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.8511 - sparse_categorical_accuracy: 0.6957\n",
      "Epoch 43/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.8460 - sparse_categorical_accuracy: 0.6957WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.8463 - sparse_categorical_accuracy: 0.6954\n",
      "Epoch 44/200\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.8416 - sparse_categorical_accuracy: 0.7007WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.8416 - sparse_categorical_accuracy: 0.7007\n",
      "Epoch 45/200\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.8373 - sparse_categorical_accuracy: 0.7004 - val_loss: 1.0912 - val_sparse_categorical_accuracy: 0.6299\n",
      "Epoch 46/200\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.8318 - sparse_categorical_accuracy: 0.7030WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.8318 - sparse_categorical_accuracy: 0.7030\n",
      "Epoch 47/200\n",
      "773/782 [============================>.] - ETA: 0s - loss: 0.8310 - sparse_categorical_accuracy: 0.7050WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.8306 - sparse_categorical_accuracy: 0.7050\n",
      "Epoch 48/200\n",
      "771/782 [============================>.] - ETA: 0s - loss: 0.8225 - sparse_categorical_accuracy: 0.7067WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.8223 - sparse_categorical_accuracy: 0.7067\n",
      "Epoch 49/200\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.8190 - sparse_categorical_accuracy: 0.7079WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.8190 - sparse_categorical_accuracy: 0.7079\n",
      "Epoch 50/200\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.8150 - sparse_categorical_accuracy: 0.7084 - val_loss: 1.1176 - val_sparse_categorical_accuracy: 0.6238\n",
      "Epoch 51/200\n",
      "773/782 [============================>.] - ETA: 0s - loss: 0.8074 - sparse_categorical_accuracy: 0.7116WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.8084 - sparse_categorical_accuracy: 0.7113\n",
      "Epoch 52/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.8060 - sparse_categorical_accuracy: 0.7122WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.8060 - sparse_categorical_accuracy: 0.7122\n",
      "Epoch 53/200\n",
      "777/782 [============================>.] - ETA: 0s - loss: 0.8021 - sparse_categorical_accuracy: 0.7130WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.8028 - sparse_categorical_accuracy: 0.7129\n",
      "Epoch 54/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.7973 - sparse_categorical_accuracy: 0.7155WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7974 - sparse_categorical_accuracy: 0.7155\n",
      "Epoch 55/200\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.7933 - sparse_categorical_accuracy: 0.7168 - val_loss: 1.1608 - val_sparse_categorical_accuracy: 0.6135\n",
      "Epoch 56/200\n",
      "770/782 [============================>.] - ETA: 0s - loss: 0.7919 - sparse_categorical_accuracy: 0.7175WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7924 - sparse_categorical_accuracy: 0.7172\n",
      "Epoch 57/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.7857 - sparse_categorical_accuracy: 0.7195WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7857 - sparse_categorical_accuracy: 0.7195\n",
      "Epoch 58/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.7851 - sparse_categorical_accuracy: 0.7182WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7851 - sparse_categorical_accuracy: 0.7182\n",
      "Epoch 59/200\n",
      "777/782 [============================>.] - ETA: 0s - loss: 0.7788 - sparse_categorical_accuracy: 0.7242WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7785 - sparse_categorical_accuracy: 0.7243\n",
      "Epoch 60/200\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.7773 - sparse_categorical_accuracy: 0.7224 - val_loss: 1.1389 - val_sparse_categorical_accuracy: 0.6209\n",
      "Epoch 61/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.7770 - sparse_categorical_accuracy: 0.7200WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7773 - sparse_categorical_accuracy: 0.7199\n",
      "Epoch 62/200\n",
      "774/782 [============================>.] - ETA: 0s - loss: 0.7712 - sparse_categorical_accuracy: 0.7247WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7718 - sparse_categorical_accuracy: 0.7244\n",
      "Epoch 63/200\n",
      "777/782 [============================>.] - ETA: 0s - loss: 0.7627 - sparse_categorical_accuracy: 0.7280WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7626 - sparse_categorical_accuracy: 0.7281\n",
      "Epoch 64/200\n",
      "775/782 [============================>.] - ETA: 0s - loss: 0.7661 - sparse_categorical_accuracy: 0.7291WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7669 - sparse_categorical_accuracy: 0.7289\n",
      "Epoch 65/200\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.7575 - sparse_categorical_accuracy: 0.7277 - val_loss: 1.1355 - val_sparse_categorical_accuracy: 0.6263\n",
      "Epoch 66/200\n",
      "773/782 [============================>.] - ETA: 0s - loss: 0.7571 - sparse_categorical_accuracy: 0.7298WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7577 - sparse_categorical_accuracy: 0.7296\n",
      "Epoch 67/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.7557 - sparse_categorical_accuracy: 0.7292WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7557 - sparse_categorical_accuracy: 0.7292\n",
      "Epoch 68/200\n",
      "778/782 [============================>.] - ETA: 0s - loss: 0.7537 - sparse_categorical_accuracy: 0.7316WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7540 - sparse_categorical_accuracy: 0.7316\n",
      "Epoch 69/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.7488 - sparse_categorical_accuracy: 0.7317WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7489 - sparse_categorical_accuracy: 0.7317\n",
      "Epoch 70/200\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.7409 - sparse_categorical_accuracy: 0.7347 - val_loss: 1.1783 - val_sparse_categorical_accuracy: 0.6140\n",
      "Epoch 71/200\n",
      "776/782 [============================>.] - ETA: 0s - loss: 0.7443 - sparse_categorical_accuracy: 0.7335WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7447 - sparse_categorical_accuracy: 0.7332\n",
      "Epoch 72/200\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.7484 - sparse_categorical_accuracy: 0.7320WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7484 - sparse_categorical_accuracy: 0.7320\n",
      "Epoch 73/200\n",
      "775/782 [============================>.] - ETA: 0s - loss: 0.7376 - sparse_categorical_accuracy: 0.7386WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7376 - sparse_categorical_accuracy: 0.7386\n",
      "Epoch 74/200\n",
      "772/782 [============================>.] - ETA: 0s - loss: 0.7369 - sparse_categorical_accuracy: 0.7362WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7372 - sparse_categorical_accuracy: 0.7361\n",
      "Epoch 75/200\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.7388 - sparse_categorical_accuracy: 0.7357 - val_loss: 1.1590 - val_sparse_categorical_accuracy: 0.6157\n",
      "Epoch 76/200\n",
      "771/782 [============================>.] - ETA: 0s - loss: 0.7332 - sparse_categorical_accuracy: 0.7381WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7333 - sparse_categorical_accuracy: 0.7381\n",
      "Epoch 77/200\n",
      "775/782 [============================>.] - ETA: 0s - loss: 0.7313 - sparse_categorical_accuracy: 0.7402WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7314 - sparse_categorical_accuracy: 0.7401\n",
      "Epoch 78/200\n",
      "773/782 [============================>.] - ETA: 0s - loss: 0.7280 - sparse_categorical_accuracy: 0.7389WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7293 - sparse_categorical_accuracy: 0.7389\n",
      "Epoch 79/200\n",
      "777/782 [============================>.] - ETA: 0s - loss: 0.7291 - sparse_categorical_accuracy: 0.7400WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7296 - sparse_categorical_accuracy: 0.7398\n",
      "Epoch 80/200\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.7287 - sparse_categorical_accuracy: 0.7418 - val_loss: 1.1913 - val_sparse_categorical_accuracy: 0.6107\n",
      "Epoch 81/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.7274 - sparse_categorical_accuracy: 0.7407WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7274 - sparse_categorical_accuracy: 0.7407\n",
      "Epoch 82/200\n",
      "778/782 [============================>.] - ETA: 0s - loss: 0.7242 - sparse_categorical_accuracy: 0.7415WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7243 - sparse_categorical_accuracy: 0.7415\n",
      "Epoch 83/200\n",
      "772/782 [============================>.] - ETA: 0s - loss: 0.7190 - sparse_categorical_accuracy: 0.7444WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7189 - sparse_categorical_accuracy: 0.7447\n",
      "Epoch 84/200\n",
      "776/782 [============================>.] - ETA: 0s - loss: 0.7211 - sparse_categorical_accuracy: 0.7422WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7211 - sparse_categorical_accuracy: 0.7422\n",
      "Epoch 85/200\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.7094 - sparse_categorical_accuracy: 0.7468 - val_loss: 1.2146 - val_sparse_categorical_accuracy: 0.6013\n",
      "Epoch 86/200\n",
      "771/782 [============================>.] - ETA: 0s - loss: 0.7120 - sparse_categorical_accuracy: 0.7476WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7122 - sparse_categorical_accuracy: 0.7475\n",
      "Epoch 87/200\n",
      "771/782 [============================>.] - ETA: 0s - loss: 0.7154 - sparse_categorical_accuracy: 0.7465WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7155 - sparse_categorical_accuracy: 0.7464\n",
      "Epoch 88/200\n",
      "773/782 [============================>.] - ETA: 0s - loss: 0.7057 - sparse_categorical_accuracy: 0.7484WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7061 - sparse_categorical_accuracy: 0.7482\n",
      "Epoch 89/200\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.7047 - sparse_categorical_accuracy: 0.7516WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7047 - sparse_categorical_accuracy: 0.7516\n",
      "Epoch 90/200\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.7034 - sparse_categorical_accuracy: 0.7500 - val_loss: 1.1978 - val_sparse_categorical_accuracy: 0.6197\n",
      "Epoch 91/200\n",
      "778/782 [============================>.] - ETA: 0s - loss: 0.7048 - sparse_categorical_accuracy: 0.7482WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7046 - sparse_categorical_accuracy: 0.7484\n",
      "Epoch 92/200\n",
      "777/782 [============================>.] - ETA: 0s - loss: 0.7022 - sparse_categorical_accuracy: 0.7491WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7029 - sparse_categorical_accuracy: 0.7490\n",
      "Epoch 93/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.6974 - sparse_categorical_accuracy: 0.7508WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6973 - sparse_categorical_accuracy: 0.7508\n",
      "Epoch 94/200\n",
      "773/782 [============================>.] - ETA: 0s - loss: 0.7014 - sparse_categorical_accuracy: 0.7498WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7021 - sparse_categorical_accuracy: 0.7497\n",
      "Epoch 95/200\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.6998 - sparse_categorical_accuracy: 0.7505 - val_loss: 1.1977 - val_sparse_categorical_accuracy: 0.6105\n",
      "Epoch 96/200\n",
      "774/782 [============================>.] - ETA: 0s - loss: 0.6950 - sparse_categorical_accuracy: 0.7525WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6954 - sparse_categorical_accuracy: 0.7523\n",
      "Epoch 97/200\n",
      "771/782 [============================>.] - ETA: 0s - loss: 0.6995 - sparse_categorical_accuracy: 0.7512WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6993 - sparse_categorical_accuracy: 0.7514\n",
      "Epoch 98/200\n",
      "772/782 [============================>.] - ETA: 0s - loss: 0.6912 - sparse_categorical_accuracy: 0.7532WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6915 - sparse_categorical_accuracy: 0.7530\n",
      "Epoch 99/200\n",
      "775/782 [============================>.] - ETA: 0s - loss: 0.6920 - sparse_categorical_accuracy: 0.7526WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6931 - sparse_categorical_accuracy: 0.7521\n",
      "Epoch 100/200\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.6879 - sparse_categorical_accuracy: 0.7546 - val_loss: 1.2082 - val_sparse_categorical_accuracy: 0.6099\n",
      "Epoch 101/200\n",
      "772/782 [============================>.] - ETA: 0s - loss: 0.6831 - sparse_categorical_accuracy: 0.7568WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6831 - sparse_categorical_accuracy: 0.7566\n",
      "Epoch 102/200\n",
      "772/782 [============================>.] - ETA: 0s - loss: 0.6921 - sparse_categorical_accuracy: 0.7548WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6916 - sparse_categorical_accuracy: 0.7551\n",
      "Epoch 103/200\n",
      "773/782 [============================>.] - ETA: 0s - loss: 0.6893 - sparse_categorical_accuracy: 0.7532WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6901 - sparse_categorical_accuracy: 0.7527\n",
      "Epoch 104/200\n",
      "771/782 [============================>.] - ETA: 0s - loss: 0.6777 - sparse_categorical_accuracy: 0.7571WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6781 - sparse_categorical_accuracy: 0.7569\n",
      "Epoch 105/200\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.6855 - sparse_categorical_accuracy: 0.7536 - val_loss: 1.1720 - val_sparse_categorical_accuracy: 0.6202\n",
      "Epoch 106/200\n",
      "778/782 [============================>.] - ETA: 0s - loss: 0.6811 - sparse_categorical_accuracy: 0.7560WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6812 - sparse_categorical_accuracy: 0.7559\n",
      "Epoch 107/200\n",
      "778/782 [============================>.] - ETA: 0s - loss: 0.6775 - sparse_categorical_accuracy: 0.7578WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6776 - sparse_categorical_accuracy: 0.7577\n",
      "Epoch 108/200\n",
      "773/782 [============================>.] - ETA: 0s - loss: 0.6835 - sparse_categorical_accuracy: 0.7562WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6837 - sparse_categorical_accuracy: 0.7563\n",
      "Epoch 109/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.6810 - sparse_categorical_accuracy: 0.7558WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6810 - sparse_categorical_accuracy: 0.7559\n",
      "Epoch 110/200\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.6807 - sparse_categorical_accuracy: 0.7565 - val_loss: 1.1750 - val_sparse_categorical_accuracy: 0.6251\n",
      "Epoch 111/200\n",
      "775/782 [============================>.] - ETA: 0s - loss: 0.6804 - sparse_categorical_accuracy: 0.7563WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6806 - sparse_categorical_accuracy: 0.7563\n",
      "Epoch 112/200\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.6713 - sparse_categorical_accuracy: 0.7597WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6713 - sparse_categorical_accuracy: 0.7597\n",
      "Epoch 113/200\n",
      "771/782 [============================>.] - ETA: 0s - loss: 0.6766 - sparse_categorical_accuracy: 0.7574WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6758 - sparse_categorical_accuracy: 0.7575\n",
      "Epoch 114/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.6713 - sparse_categorical_accuracy: 0.7615WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6718 - sparse_categorical_accuracy: 0.7613\n",
      "Epoch 115/200\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.6707 - sparse_categorical_accuracy: 0.7615 - val_loss: 1.1961 - val_sparse_categorical_accuracy: 0.6203\n",
      "Epoch 116/200\n",
      "778/782 [============================>.] - ETA: 0s - loss: 0.6708 - sparse_categorical_accuracy: 0.7623WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6712 - sparse_categorical_accuracy: 0.7622\n",
      "Epoch 117/200\n",
      "774/782 [============================>.] - ETA: 0s - loss: 0.6660 - sparse_categorical_accuracy: 0.7628WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6663 - sparse_categorical_accuracy: 0.7627\n",
      "Epoch 118/200\n",
      "778/782 [============================>.] - ETA: 0s - loss: 0.6662 - sparse_categorical_accuracy: 0.7622WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6669 - sparse_categorical_accuracy: 0.7621\n",
      "Epoch 119/200\n",
      "777/782 [============================>.] - ETA: 0s - loss: 0.6645 - sparse_categorical_accuracy: 0.7639WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6651 - sparse_categorical_accuracy: 0.7637\n",
      "Epoch 120/200\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.6645 - sparse_categorical_accuracy: 0.7621 - val_loss: 1.2127 - val_sparse_categorical_accuracy: 0.6217\n",
      "Epoch 121/200\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.6691 - sparse_categorical_accuracy: 0.7619WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6691 - sparse_categorical_accuracy: 0.7619\n",
      "Epoch 122/200\n",
      "774/782 [============================>.] - ETA: 0s - loss: 0.6631 - sparse_categorical_accuracy: 0.7635WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6630 - sparse_categorical_accuracy: 0.7636\n",
      "Epoch 123/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.6673 - sparse_categorical_accuracy: 0.7634WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6671 - sparse_categorical_accuracy: 0.7634\n",
      "Epoch 124/200\n",
      "772/782 [============================>.] - ETA: 0s - loss: 0.6644 - sparse_categorical_accuracy: 0.7628WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6638 - sparse_categorical_accuracy: 0.7631\n",
      "Epoch 125/200\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.6625 - sparse_categorical_accuracy: 0.7642 - val_loss: 1.1895 - val_sparse_categorical_accuracy: 0.6209\n",
      "Epoch 126/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.6589 - sparse_categorical_accuracy: 0.7647WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6586 - sparse_categorical_accuracy: 0.7648\n",
      "Epoch 127/200\n",
      "773/782 [============================>.] - ETA: 0s - loss: 0.6592 - sparse_categorical_accuracy: 0.7648WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6592 - sparse_categorical_accuracy: 0.7648\n",
      "Epoch 128/200\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.6635 - sparse_categorical_accuracy: 0.7633WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6635 - sparse_categorical_accuracy: 0.7633\n",
      "Epoch 129/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.6567 - sparse_categorical_accuracy: 0.7660WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6567 - sparse_categorical_accuracy: 0.7661\n",
      "Epoch 130/200\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.6541 - sparse_categorical_accuracy: 0.7672 - val_loss: 1.2527 - val_sparse_categorical_accuracy: 0.6135\n",
      "Epoch 131/200\n",
      "773/782 [============================>.] - ETA: 0s - loss: 0.6538 - sparse_categorical_accuracy: 0.7652WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6544 - sparse_categorical_accuracy: 0.7650\n",
      "Epoch 132/200\n",
      "775/782 [============================>.] - ETA: 0s - loss: 0.6549 - sparse_categorical_accuracy: 0.7677WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6551 - sparse_categorical_accuracy: 0.7676\n",
      "Epoch 133/200\n",
      "773/782 [============================>.] - ETA: 0s - loss: 0.6605 - sparse_categorical_accuracy: 0.7665WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6607 - sparse_categorical_accuracy: 0.7664\n",
      "Epoch 134/200\n",
      "770/782 [============================>.] - ETA: 0s - loss: 0.6643 - sparse_categorical_accuracy: 0.7637WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6644 - sparse_categorical_accuracy: 0.7636\n",
      "Epoch 135/200\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.6491 - sparse_categorical_accuracy: 0.7697 - val_loss: 1.2301 - val_sparse_categorical_accuracy: 0.6165\n",
      "Epoch 136/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.6494 - sparse_categorical_accuracy: 0.7683WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6493 - sparse_categorical_accuracy: 0.7683\n",
      "Epoch 137/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.6555 - sparse_categorical_accuracy: 0.7672WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6554 - sparse_categorical_accuracy: 0.7671\n",
      "Epoch 138/200\n",
      "776/782 [============================>.] - ETA: 0s - loss: 0.6548 - sparse_categorical_accuracy: 0.7683WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6546 - sparse_categorical_accuracy: 0.7684\n",
      "Epoch 139/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.6454 - sparse_categorical_accuracy: 0.7702WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6455 - sparse_categorical_accuracy: 0.7702\n",
      "Epoch 140/200\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.6450 - sparse_categorical_accuracy: 0.7713 - val_loss: 1.2471 - val_sparse_categorical_accuracy: 0.6212\n",
      "Epoch 141/200\n",
      "773/782 [============================>.] - ETA: 0s - loss: 0.6516 - sparse_categorical_accuracy: 0.7671WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6516 - sparse_categorical_accuracy: 0.7669\n",
      "Epoch 142/200\n",
      "778/782 [============================>.] - ETA: 0s - loss: 0.6436 - sparse_categorical_accuracy: 0.7707WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6436 - sparse_categorical_accuracy: 0.7706\n",
      "Epoch 143/200\n",
      "778/782 [============================>.] - ETA: 0s - loss: 0.6440 - sparse_categorical_accuracy: 0.7701WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6436 - sparse_categorical_accuracy: 0.7704\n",
      "Epoch 144/200\n",
      "772/782 [============================>.] - ETA: 0s - loss: 0.6497 - sparse_categorical_accuracy: 0.7673WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6503 - sparse_categorical_accuracy: 0.7672\n",
      "Epoch 145/200\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.6439 - sparse_categorical_accuracy: 0.7700 - val_loss: 1.2058 - val_sparse_categorical_accuracy: 0.6169\n",
      "Epoch 146/200\n",
      "776/782 [============================>.] - ETA: 0s - loss: 0.6461 - sparse_categorical_accuracy: 0.7718WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6463 - sparse_categorical_accuracy: 0.7718\n",
      "Epoch 147/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.6379 - sparse_categorical_accuracy: 0.7747WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6378 - sparse_categorical_accuracy: 0.7747\n",
      "Epoch 148/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.6400 - sparse_categorical_accuracy: 0.7719WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6401 - sparse_categorical_accuracy: 0.7719\n",
      "Epoch 149/200\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.6369 - sparse_categorical_accuracy: 0.7715WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6369 - sparse_categorical_accuracy: 0.7715\n",
      "Epoch 150/200\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.6312 - sparse_categorical_accuracy: 0.7754 - val_loss: 1.2431 - val_sparse_categorical_accuracy: 0.6201\n",
      "Epoch 151/200\n",
      "776/782 [============================>.] - ETA: 0s - loss: 0.6401 - sparse_categorical_accuracy: 0.7719WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6399 - sparse_categorical_accuracy: 0.7720\n",
      "Epoch 152/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.6333 - sparse_categorical_accuracy: 0.7743WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6336 - sparse_categorical_accuracy: 0.7743\n",
      "Epoch 153/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.6336 - sparse_categorical_accuracy: 0.7754WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6335 - sparse_categorical_accuracy: 0.7755\n",
      "Epoch 154/200\n",
      "778/782 [============================>.] - ETA: 0s - loss: 0.6333 - sparse_categorical_accuracy: 0.7748WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6337 - sparse_categorical_accuracy: 0.7746\n",
      "Epoch 155/200\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.6330 - sparse_categorical_accuracy: 0.7751 - val_loss: 1.2389 - val_sparse_categorical_accuracy: 0.6107\n",
      "Epoch 156/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.6364 - sparse_categorical_accuracy: 0.7719WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6363 - sparse_categorical_accuracy: 0.7719\n",
      "Epoch 157/200\n",
      "775/782 [============================>.] - ETA: 0s - loss: 0.6327 - sparse_categorical_accuracy: 0.7745WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6326 - sparse_categorical_accuracy: 0.7744\n",
      "Epoch 158/200\n",
      "776/782 [============================>.] - ETA: 0s - loss: 0.6334 - sparse_categorical_accuracy: 0.7750WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6342 - sparse_categorical_accuracy: 0.7747\n",
      "Epoch 159/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.6339 - sparse_categorical_accuracy: 0.7768WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6339 - sparse_categorical_accuracy: 0.7768\n",
      "Epoch 160/200\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.6335 - sparse_categorical_accuracy: 0.7746 - val_loss: 1.2025 - val_sparse_categorical_accuracy: 0.6163\n",
      "Epoch 161/200\n",
      "772/782 [============================>.] - ETA: 0s - loss: 0.6295 - sparse_categorical_accuracy: 0.7758WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6296 - sparse_categorical_accuracy: 0.7757\n",
      "Epoch 162/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.6325 - sparse_categorical_accuracy: 0.7735WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6325 - sparse_categorical_accuracy: 0.7735\n",
      "Epoch 163/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.6344 - sparse_categorical_accuracy: 0.7732WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6342 - sparse_categorical_accuracy: 0.7733\n",
      "Epoch 164/200\n",
      "776/782 [============================>.] - ETA: 0s - loss: 0.6275 - sparse_categorical_accuracy: 0.7764WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6281 - sparse_categorical_accuracy: 0.7762\n",
      "Epoch 165/200\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.6301 - sparse_categorical_accuracy: 0.7750 - val_loss: 1.2808 - val_sparse_categorical_accuracy: 0.6003\n",
      "Epoch 166/200\n",
      "773/782 [============================>.] - ETA: 0s - loss: 0.6254 - sparse_categorical_accuracy: 0.7788WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6251 - sparse_categorical_accuracy: 0.7790\n",
      "Epoch 167/200\n",
      "776/782 [============================>.] - ETA: 0s - loss: 0.6228 - sparse_categorical_accuracy: 0.7775WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6231 - sparse_categorical_accuracy: 0.7773\n",
      "Epoch 168/200\n",
      "775/782 [============================>.] - ETA: 0s - loss: 0.6208 - sparse_categorical_accuracy: 0.7803WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6219 - sparse_categorical_accuracy: 0.7800\n",
      "Epoch 169/200\n",
      "774/782 [============================>.] - ETA: 0s - loss: 0.6180 - sparse_categorical_accuracy: 0.7807WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6174 - sparse_categorical_accuracy: 0.7810\n",
      "Epoch 170/200\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.6176 - sparse_categorical_accuracy: 0.7822 - val_loss: 1.2311 - val_sparse_categorical_accuracy: 0.6181\n",
      "Epoch 171/200\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.6216 - sparse_categorical_accuracy: 0.7794WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6216 - sparse_categorical_accuracy: 0.7794\n",
      "Epoch 172/200\n",
      "772/782 [============================>.] - ETA: 0s - loss: 0.6237 - sparse_categorical_accuracy: 0.7765WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6233 - sparse_categorical_accuracy: 0.7765\n",
      "Epoch 173/200\n",
      "776/782 [============================>.] - ETA: 0s - loss: 0.6228 - sparse_categorical_accuracy: 0.7782WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6227 - sparse_categorical_accuracy: 0.7783\n",
      "Epoch 174/200\n",
      "771/782 [============================>.] - ETA: 0s - loss: 0.6167 - sparse_categorical_accuracy: 0.7806WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6170 - sparse_categorical_accuracy: 0.7805\n",
      "Epoch 175/200\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.6153 - sparse_categorical_accuracy: 0.7813 - val_loss: 1.2244 - val_sparse_categorical_accuracy: 0.6166\n",
      "Epoch 176/200\n",
      "776/782 [============================>.] - ETA: 0s - loss: 0.6207 - sparse_categorical_accuracy: 0.7791WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6207 - sparse_categorical_accuracy: 0.7790\n",
      "Epoch 177/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.6174 - sparse_categorical_accuracy: 0.7800WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6174 - sparse_categorical_accuracy: 0.7799\n",
      "Epoch 178/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.6174 - sparse_categorical_accuracy: 0.7797WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6175 - sparse_categorical_accuracy: 0.7798\n",
      "Epoch 179/200\n",
      "774/782 [============================>.] - ETA: 0s - loss: 0.6214 - sparse_categorical_accuracy: 0.7778WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6218 - sparse_categorical_accuracy: 0.7776\n",
      "Epoch 180/200\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.6148 - sparse_categorical_accuracy: 0.7823 - val_loss: 1.2914 - val_sparse_categorical_accuracy: 0.6061\n",
      "Epoch 181/200\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.6135 - sparse_categorical_accuracy: 0.7828WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6135 - sparse_categorical_accuracy: 0.7828\n",
      "Epoch 182/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.6145 - sparse_categorical_accuracy: 0.7815WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6149 - sparse_categorical_accuracy: 0.7813\n",
      "Epoch 183/200\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.6131 - sparse_categorical_accuracy: 0.7826WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6131 - sparse_categorical_accuracy: 0.7825\n",
      "Epoch 184/200\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.6139 - sparse_categorical_accuracy: 0.7812WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6139 - sparse_categorical_accuracy: 0.7812\n",
      "Epoch 185/200\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.6132 - sparse_categorical_accuracy: 0.7806 - val_loss: 1.2228 - val_sparse_categorical_accuracy: 0.6223\n",
      "Epoch 186/200\n",
      "773/782 [============================>.] - ETA: 0s - loss: 0.6069 - sparse_categorical_accuracy: 0.7852WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6069 - sparse_categorical_accuracy: 0.7852\n",
      "Epoch 187/200\n",
      "777/782 [============================>.] - ETA: 0s - loss: 0.6040 - sparse_categorical_accuracy: 0.7867WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6048 - sparse_categorical_accuracy: 0.7865\n",
      "Epoch 188/200\n",
      "775/782 [============================>.] - ETA: 0s - loss: 0.6040 - sparse_categorical_accuracy: 0.7865WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6043 - sparse_categorical_accuracy: 0.7865\n",
      "Epoch 189/200\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.6095 - sparse_categorical_accuracy: 0.7834WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6095 - sparse_categorical_accuracy: 0.7834\n",
      "Epoch 190/200\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.5998 - sparse_categorical_accuracy: 0.7854 - val_loss: 1.2664 - val_sparse_categorical_accuracy: 0.6066\n",
      "Epoch 191/200\n",
      "771/782 [============================>.] - ETA: 0s - loss: 0.6040 - sparse_categorical_accuracy: 0.7838WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6050 - sparse_categorical_accuracy: 0.7835\n",
      "Epoch 192/200\n",
      "776/782 [============================>.] - ETA: 0s - loss: 0.6071 - sparse_categorical_accuracy: 0.7842WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6074 - sparse_categorical_accuracy: 0.7840\n",
      "Epoch 193/200\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.6063 - sparse_categorical_accuracy: 0.7849WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6063 - sparse_categorical_accuracy: 0.7849\n",
      "Epoch 194/200\n",
      "779/782 [============================>.] - ETA: 0s - loss: 0.6028 - sparse_categorical_accuracy: 0.7859WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6027 - sparse_categorical_accuracy: 0.7860\n",
      "Epoch 195/200\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.6007 - sparse_categorical_accuracy: 0.7855 - val_loss: 1.2710 - val_sparse_categorical_accuracy: 0.6104\n",
      "Epoch 196/200\n",
      "778/782 [============================>.] - ETA: 0s - loss: 0.6025 - sparse_categorical_accuracy: 0.7858WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6025 - sparse_categorical_accuracy: 0.7859\n",
      "Epoch 197/200\n",
      "777/782 [============================>.] - ETA: 0s - loss: 0.6061 - sparse_categorical_accuracy: 0.7845WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6061 - sparse_categorical_accuracy: 0.7845\n",
      "Epoch 198/200\n",
      "775/782 [============================>.] - ETA: 0s - loss: 0.6019 - sparse_categorical_accuracy: 0.7856WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6023 - sparse_categorical_accuracy: 0.7856\n",
      "Epoch 199/200\n",
      "778/782 [============================>.] - ETA: 0s - loss: 0.5960 - sparse_categorical_accuracy: 0.7873WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.5956 - sparse_categorical_accuracy: 0.7875\n",
      "Epoch 200/200\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.6071 - sparse_categorical_accuracy: 0.7848 - val_loss: 1.2834 - val_sparse_categorical_accuracy: 0.6195\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train,batch_size=64,epochs=200,validation_data=(x_test, y_test), validation_freq=5, callbacks=[cp_callback])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"pic_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             multiple                  456       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  multiple                 24        \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " activation (Activation)     multiple                  0         \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  multiple                 0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " flatten (Flatten)           multiple                  0         \n",
      "                                                                 \n",
      " dense (Dense)               multiple                  196736    \n",
      "                                                                 \n",
      " dense_1 (Dense)             multiple                  1290      \n",
      "                                                                 \n",
      " dropout (Dropout)           multiple                  0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 198,506\n",
      "Trainable params: 198,494\n",
      "Non-trainable params: 12\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'sparse_categorical_accuracy', 'val_loss', 'val_sparse_categorical_accuracy'])\n"
     ]
    }
   ],
   "source": [
    "print(history.history.keys())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "acc = history.history['sparse_categorical_accuracy']\n",
    "val_acc = history.history['val_sparse_categorical_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 2 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEICAYAAACzliQjAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAABOaElEQVR4nO3dd3xUVdrA8d+TSSUJCSm0BAhIbwkQQOmIFLEgiBQrYlncRfH11bXuiuvLFmVdddVlWRdQFIKKICpFsGFBJfReDC2EEgIkIX1mzvvHvQmTMEkmdTLJ+X4++WTm3nPvfWZy58mZc889R5RSaJqmaQ2Dl7sD0DRN02qPTvqapmkNiE76mqZpDYhO+pqmaQ2ITvqapmkNiE76mqZpDUiDSfoiskZE7qkDccwWkfdqYL/TROR7h+eXRKSdK2W1hkl/JkovW5/V6aRv/pEKf+wikuPw/I6K7Espdb1S6p2airWqRCRKRKwicpWTdStEZG5F9qeUClJKJVUhnkDzfV5d2X1o1U9/JorW1dpnQkRiRESJiHdFt62L6nTSN/9IQUqpIOA4cJPDsvcLy9WHP4ZS6iTwJXCX43IRCQPGArX94ZwI5AGjRKRFbR64Pvw9a4r+TLj1M1Ev1OmkXxoRGSYiySLypIicBhaKSBMR+UxEUkXkgvk42mGbb0TkfvPxNBH5XkTmmmWPiMj1ZRzvKRH5VUQyRWSviIx3WFfmvkSkrYh8a267Hogo46W9Q4kTHJgC7FFK7SorDicxKxFpbz4OF5FVIpIhIr8AV9ScnLgHmAfsBIrVIEVkkIj8KCIXReSEiEwzlweIyN9F5JiIpJvvS0Dh36vEPo6KyHXm49ki8pGIvCciGcA0EeknIpvMY5wSkTdExNdh+24isl5EzovIGRF5RkSai0i2iIQ7lOtjnhM+Lrxmj6U/E7XymXC2z5bmfs6LyGERecBhXT8RSTSPcUZEXjGX+5vnepp5fm8WkWaVOX5leGTSNzUHwoA2wIMYr2Wh+bw1kAO8Ucb2/YEDGCfcS8B/RURKKfsrMBgIAV4A3pPitd+y9rUE2GKuexEjmZZmBRAhIoMclt0FvOtiHKV5E8gFWgDTzZ9SiUhrYBjwvvlzd4l1a4B/ApFAHLDdXD0X6AMMwPjb/B6wuxAfwDjgIyDUPKYN+B+M9+0aYATwWzOGYGADsBZoCbQHvlRKnQa+ASY57PdOIEEpVeBiHJ5MfyZq6DNRhqVAMsZ5OBH4s4iMMNe9BrymlGqM8U/lA3P5PWa8rYBwYAbG36Z2KKU84gc4ClxnPh4G5AP+ZZSPAy44PP8GuN98PA047LCuEaCA5i7Gsh0YV96+MD5oViDQYf0S4L0y9v02MN983MF8nU1djON7h3UKIxlagAKgs8O6PzuWdbLf54Dt5uOWGAm4l/n8aWCFk228ME7cWCfrhgHJZfw9ZwMby3nPHy08LjAV2FZKucnAD+ZjC3Aa6Ofu87cmfvRnonY+E0CMua13ieWtzM9GsMOyvwCLzMcbMf4RRZTYbjrwI9DTHeeNJ9f0U5VSuYVPRKSRiPzbbFrIwHjDQ0XEUsr2pwsfKKWyzYdBzgqKyN0ist38KnYR6E7xr6Sl7aslxocsy6HssXJe1zvAJBHxx6jRrFVKnXUxDmciAW/gRAViuBujto1SKgX4lsu1sVYYtauSIgD/Uta5wjE+RKSj2Rxx2vx7/pnLr7W0GAA+AbqK0UtjJJCulPqlkjF5Gv2ZqLnPhDMtgfNKqcwS+4kyH98HdAT2m004N5rLFwPrgAQRSRGRl2qz+dGTk37J4UH/F+gE9FfG16kh5vLSvp66RETaAP8BZgLhSqlQYLeL+z0FNBGRQIdlrcvaQCn1HZCG0dxxJ+bX2CrEkYpRs2rlSgwiMgCjNvW0mXBPY3xVnyrGxcETOG//PIfxddnZuiyM2l7hMSwYHzxHJf+e/wL2Ax3Mv+czXH6tpcWAmfQ+wLgOcRfGB6yh0J+JGvhMlCEFCDObGx33c9KM+5BSairQFPgb8JGIBCqlCpRSLyilumI0hd6IQxNqTfPkpF9SMEbzwkUxru4/X037DcT4MKUCiMi9GLWJcimljgGJwAsi4mu2S97kwqbvYpwkocCnVYlDKWUDPgZmmzW/rpTdhnoPsB7oitEcEGcepxFwPcY3gOtEZJKIeJsXxOKUUnZgAfCKeXHLIiLXiIgfcBDwF5EbzBrNc4BfOaEHAxnAJRHpDDzksO4zoLmIPCoifiISLCL9Hda/i/HV/mag2vt/exD9mXAeQ0U/E4X8zIuw/ua3jpMYzTR/MZf1xKjdv2/Gc6eIRJqfjYvmPmwiMlxEepiVnwyMpiabC8evFvUp6b8KBGDUOH/CuMhXZUqpvcDfgU3AGaAH8EMFdnE7Rk35PMaH7t2yi4NZpjWwTCmVVw1xzMT4an0aWIRxce8K5ok8CfinUuq0w88RjBrzPUqp4xjd5f7XfE3bgVhzF48Du4DN5rq/AV5KqXSMi7BvY3xQsjAufpXlcYz3LhOjNrescIX5dXokRrI4DRwChjus/wHjAvJWpdTRco5Tn72K/kyUxqXPRAmXMP6JFv5ci3F9KQaj1r8CeF4ptd4sPwbYIyKXMC7qTjG/iTbH6LSQAezDaD6ttcqJmBcWNK1eEZGvgCVKqbfdHYum1SU66Wv1joj0xWiialXiIpumNXj1qXlH0xCRdzD68D+qE76mXUnX9DVN0xoQXdPXNE1rQOrkoEwREREqJibG3WFo9dSWLVvOKaVK3idQ4/R5rdUkV89rl5K+iIzB6HJkAd5WSv21xPoQjC5Hrc19zlVKLXRlW2diYmJITEx0JTRNqzARqczdl1Wmz2utJrl6XpfbvGPeQPAmxo05XTHuzOxaotjvgL1KqViMMUD+bt544cq2mqZpWi1xpU2/H8bgSUlKqXwgAeN2aEcKCDZH0QvCuOnC6uK2mqZpWi1xJelHUXxgomQuDyhU6A2gC8ZdabuAWeatx65sC4CIPCjG2NOJqampLoavaZqmVYQrbfrOBi8q2c9zNMbt+NdiDIS1XkS+c3FbY6FS84H5APHx8bofqabVEwUFBSQnJ5Obm1t+Ya1c/v7+REdH4+NTuYE5XUn6yRQfjS4ao0bv6F7gr8ro9H9YRI4AnV3cVtO0eiw5OZng4GBiYmIofU4WzRVKKdLS0khOTqZt27aV2ocrzTubgQ5iTHHmizFV2aoSZY5jzGyEGNN+dQKSXNxW07R6LDc3l/DwcJ3wq4GIEB4eXqVvTeXW9JVSVhGZiTHovwVYoJTaIyIzzPXzMKY8WyQiuzCadJ5USp0zg7xi20pHq2maR9IJv/pU9b10qZ++Umo1sLrEsnkOj1OAUa5uq2mVVThsSGkn/uGzmSRfyGFYp6a1GVaVrN51ilPpudw3qHJf1zWtIurkHbma5szhs5e4753NnM3I4+mxnRnSwbj58LUvD5Fvs+PvbWH5VmOY/m8eH0ZMRGBZu6sz1u89w5ZjF3TSryFpaWmMGGHMVX769GksFguRkca588svv+Dr61vqtomJibz77ru8/vrrLh+v8Ca8iIjyZm10D530tTrt3KU8wgOND+WLn+3lbEYeOQU2/viJ0Uro6+1FvtV+xXavf3mIVybH1WaoleYlgs2uO6zVlPDwcLZv3w7A7NmzCQoK4vHHHy9ab7Va8fZ2ngrj4+OJj4+vjTBrjU76mttk5Bbw/Cd7aNUkgMdGdSI9u4BNSecY2bU5Fi/hyY92sizRuM2ja4vG7D2Vwf2D2tIk0JeX1x3A20sY2jGSF27uRosQf1btSKFHVAiHzl6iZ3SIm1+d67y9BKv9yn9cWs2ZNm0aYWFhbNu2jd69ezN58mQeffRRcnJyCAgIYOHChXTq1IlvvvmGuXPn8tlnnzF79myOHz9OUlISx48f59FHH+WRRx5x6XjHjh1j+vTppKamEhkZycKFC2ndujUffvghL7zwAhaLhZCQEDZu3MiePXu49957yc/Px263s3z5cjp06FBtr10nfa3W5VltJKVm8bv3t5J0LguLl3Bd12ZMX5TIuUt5AIzs2oz1e88UbbP3VAYA13ZpyoCrInho6FV4eRVv1x8XZ9z31y4yqJZeSfWwWARbA8n5L3y6h70pGdW6z64tG/P8Td0qvN3BgwfZsGEDFouFjIwMNm7ciLe3Nxs2bOCZZ55h+fLlV2yzf/9+vv76azIzM+nUqRMPPfSQS/3lZ86cyd13380999zDggULeOSRR1i5ciV/+tOfWLduHVFRUVy8eBGAefPmMWvWLO644w7y8/Ox2ap3+lyd9LUal5lbQLD/5Q/Gsyt289EWo+39tSlxPPHRTm5+4wcsXkJIgA/pOQVFCX/5Q9cQ16oJp9Jz2H7iIte0Cwe4IuF7MosINl3Tr3W33XYbFosFgPT0dO655x4OHTqEiFBQUOB0mxtuuAE/Pz/8/Pxo2rQpZ86cITo6utxjbdq0iY8//hiAu+66i9///vcADBw4kGnTpjFp0iQmTJgAwDXXXMOcOXNITk5mwoQJ1VrLB530tSrKsxq1kBPns8nItdLI18KOExdpEx5Iv5gwXll/kDe+Pszobs14bGQn3vj6MJ/uMO7Pm9qvFePiovjxcBrLEk8wc3h7Hhp2FZm5VtbtOc1HW5KJa9UEi5cQ3aQR0U0aufOl1hiLV8Np069MjbymBAZevtD/hz/8geHDh7NixQqOHj3KsGHDnG7j5+dX9NhisWC1Wit17MLeZ/PmzePnn3/m888/Jy4uju3bt3P77bfTv39/Pv/8c0aPHs3bb7/NtddeW6njOKOTvlZpSikmzdsEIuw4cbHMsuv2nGHdnsvNNe9M78eQDkbvhj/c1JVBHSIY26MFFi/B38fCnVe34c6r29Rk+HVGQ0r6dVV6ejpRUUbz4KJFi6p9/wMGDCAhIYG77rqL999/n0GDBgHw66+/0r9/f/r378+nn37KiRMnSE9Pp127djzyyCMkJSWxc+fOak36euYszWUnzmezNyWjKEFt+jWNHcnpVyT8Vx16zUyOb8XyhwbwyLXt8fX24qrIQL5/cjhDO0YW1XaC/Ly5KbYllnrUZFMR3l6CTU9b6la///3vefrppxk4cGC1tKH37NmT6OhooqOjeeyxx3j99ddZuHAhPXv2ZPHixbz22msAPPHEE/To0YPu3bszZMgQYmNjWbZsGd27dycuLo79+/dz9913VzkeR3Vyjtz4+HilJ5twP7td8d/vjzCsUyQZuVZu/dePReu6tmjM+ax8bErRoWkQR89l0at1Eyb3bcWQjpH8cPgcK7ed5C8TeuBtMeoWuQU2RMDP2+KulwSAiGxRStV6P7zSzuuX1u7nP98lcWjO2NoOqVbs27ePLl26uDuMesXZe+rqea2bd7Rizl3K49TFXH4+kkbrsEbMWb2POav3XVGusDfN8oeuoU+bsCvWD2wfwcD2xW9O8fdxb7KvqyxeglU372i1RCd9DaUUyRdyyLPauOH178krcbNTyxB/CuyKt++OZ9ybPxAbHUJKei4dmgY5TfhaxVi8BKWMb1b1qVeSVjfppN+A2O2KC9n5LP3lOBP7tGL1rlOIwEdbktlTRt/prx4fhtWuCPLz5pvHhxEe5IuXCN4WnaCqg8W8tmFTCi+nU1BoWvXRSb8BOHE+m4TNx1m5LYWTF3MAmPvFwaL1EUF+PHpdB9758Sh39G/D9EFtCfSz8PuPdhIVGlCsWcZTxrPxJBbzn6fNrtAtYFpN00m/HrPZFfM3JjHv219JzylgwFXhXHNVOP4+XiSlZnF99+aM6NKM0EY+NPL15rfD2uNjkaJeNa9N6eXmV9AwFNX0dbu+Vgt00vdwdrtCBPKsdvJtdo6nZbPtxEXu7N+ajQdT+dva/QD84cau5Y7i6Oute/ACiMgC4EbgrFKqeyllhgGvAj7AOaXU0Moer7Crqr6Yq9UGnfQ9mM2umLbwFy7lWVEKtp+4WHSjz7++PkxKujG7TosQf8b3cjofvebcIuAN4F1nK0UkFHgLGKOUOi4iVRq839tM+nad9GvEsGHDePrppxk9enTRsldffZWDBw/y1ltvlbrN3LlziY+PZ+zYsSxZsoTQ0NBiZZyN2FnW8rrCpaQvImOA1zBmv3pbKfXXEuufAO5w2GcXIFIpdV5EjgKZgA2wuqN/dH107lIeS34+zneHzhVbbrMrbu/fmiOpWQzpGMmILs0Y2bWZm6L0TEqpjSISU0aR24GPlVLHzfJnq3I8XdOvWVOnTiUhIaFY0k9ISODll192afvVq+vXHFDlfp8XEQvwJnA90BWYKiJdHcsopV5WSsUppeKAp4FvlVLnHYoMN9frhF8Bdrtiyc/HOXE+GzCGIj54JpMRf/+G+P/bwCvrDzKwfThjezQv2ua5G7rw5/E9WPrg1fz11p464deMjkATEflGRLaISKm3TIrIgyKSKCKJqampTstYvIyPob0O3ihZH0ycOJHPPvuMvDxjBNejR4+SkpLCoEGDeOihh4iPj6dbt248//zzTrePiYnh3DmjcjVnzhw6derEddddx4EDB1yOQSnFE088Qffu3enRowfLli0D4NSpUwwZMoS4uDi6d+/Od999h81mY9q0aUVl//GPf1TxHSjOlZp+P+CwUioJQEQSgHHA3lLKTwWWVk94DduPv6bxzIpdxIQ34ua4KF7/8lCx9YM7RDD75m60DAng5thURndrpucirR3eQB9gBBAAbBKRn5RSB0sWVErNB+aDcUeus52ZNyw3jJr+mqfg9K7q3WfzHnD9X0tdHR4eTr9+/Vi7di3jxo0jISGByZMnIyLMmTOHsLAwbDYbI0aMYOfOnfTs2dPpfrZs2UJCQgLbtm3DarXSu3dv+vTp41KIH3/8Mdu3b2fHjh2cO3eOvn37MmTIEJYsWcLo0aN59tlnsdlsZGdns337dk6ePMnu3bsBioZcri6uJP0o4ITD82Sgv7OCItIIGAPMdFisgC9ERAH/Nj8EzrZ9EHgQoHXr1i6EVX+dz8pn9a5TLN+aTLCfN8fPZxcl/N6tQ3nh5u50bdm42Fg1Y7o3L213WvVLxrh4mwVkichGIBa4Ium7oqim3xCSvpsUNvEUJv0FCxYA8MEHHzB//nysViunTp1i7969pSb97777jvHjx9OokTHa68033+zy8b///numTp2KxWKhWbNmDB06lM2bN9O3b1+mT59OQUEBt9xyC3FxcbRr146kpCQefvhhbrjhBkaNcjr9eKW5kvSdVR1LOztvAn4o0bQzUCmVYl7sWi8i+5VSG6/YoQs1ooYg32rn9v/8xP7TmQA8OaYzPaJCOHLuErfFt9JDGdQNnwBviIg34ItRCar0d3DvhtSmX0aNvCbdcsstPPbYY2zdupWcnBx69+7NkSNHmDt3Lps3b6ZJkyZMmzaN3NzcMvdT2W/SpY1xNmTIEDZu3Mjnn3/OXXfdxRNPPMHdd9/Njh07WLduHW+++SYffPBB0T+p6uBKH71koJXD82ggpZSyUyjRtKOUSjF/nwVWYDQXaU5sOXaejs+tYf/pTG6Ja8nzN3XlN0PaMahDBHddE6MTfi0RkaXAJqCTiCSLyH0iMkNEZgAopfYBa4GdwC8YnRt2V/Z4hUMv6IlUak5QUBDDhg1j+vTpTJ06FYCMjAwCAwMJCQnhzJkzrFmzpsx9DBkyhBUrVpCTk0NmZiaffvqpy8cfMmQIy5Ytw2azkZqaysaNG+nXrx/Hjh2jadOmPPDAA9x3331s3bqVc+fOYbfbufXWW3nxxRfZunVrlV57Sa7U9DcDHUSkLXASI7HfXrKQiIQAQ4E7HZYFAl5KqUzz8SjgT9UReH1yOj2XRT8eZcO+y+PNP3V9F5qH+LsxqoZLKTXVhTIvA651/yiHd1HSr469aaWZOnUqEyZMICEhAYDY2Fh69epFt27daNeuHQMHDixz+8K5dOPi4mjTpg2DBw8utez//d//8eqrrxY9P3HiBJs2bSI2NhYR4aWXXqJ58+a88847vPzyy/j4+BAUFMS7777LyZMnuffee7GblYC//OUvVX/xDlwaWllExmLciGIBFiil5jjUeuaZZaZh9Fue4rBdO4zaPRj/YJYopeaUd7z6PrSyUgoRYfPR8/xj/UGSUrM4nWF8rRzeKZIBV0XwwJB2bo6y/qprQyuv3X2aGe9t4fNHBtGtpedM6O4qPbRy9avxoZWVUquB1SWWzSvxfBHGTS2Oy5IwLnBpphPnsxn96kZenhjLnM/3kpKei6/Fi8dGdqRX61AGtY/QPXAamMs3Z7k5EK1B0Hfk1iK7XbHxUCrZ+TZ+t8Rop/vH5Fj6tA6jdXj9nP9VK9/lm7N01tdqnk76teTHX8/xm8VbaN7YaKfv0DSIQR0iGN8r2s2Rae5m8ar/A64VNmlqVVfV2Q510q8F245fYOaSbWTmWsnMvcTgDhEsvs/prQ5aA+Rdz5O+v78/aWlphIeH68RfRUop0tLS8PevfCcPnfRriM2csGT5lmT+ssYY6fKVSbHsOpnO4A4R5WytNSRe9TzpR0dHk5ycTGnDUGgV4+/vT3R05VsIdNKvARm5BUz+90/sO3V5NqpPfjeQ2FahTOitm3O04opq+vV07B0fHx/ati17WG+t9uikXwNWbU9h36kMokIDGHBVOP87qpPuc6+Vyqsh3ZGruZ1O+tXo4aXbuLpdGMu3JtO5eTBrZg3WbZhaufR4+lpt0km/mpzJyOXTHSl8usMYoeKJ0Z10wtdc4iW6pq/VHj0/XjXIzrfy3MriQ6+M0uPYay7yttTvC7la3aJr+lWUW2DjyeW7WL/XGDfntSlxfHswlfZNg9wcmeYp6nuXTa1u0Um/ClIz87jz7Z85cCaTVmEBzBzennFxUYyL0/PRaq4rbN7RSV+rDTrpV8Ff1uzjSFoWb9zei7HdWxT1wtC0ivA2J1HRSV+rDTrpV0J6TgF//nwfH289yYND2nFjz5buDknzYGbO10lfqxU66VfCnz7dyyfbjYT/2MiO7g5H83CFNX3de0erDTrpV4DVZmfuFwdZvjWZ6QPb8sxYPUa4VnWWen5Hrla36KTvIqUUz6zYxQeJyYzp1pyZ17Z3d0haPVGU9PXUWVotcKmfvoiMEZEDInJYRJ5ysv4JEdlu/uwWEZuIhLmyraf4IPEEHyQmM3N4e+bd1YewQF93h6TVE5dr+m4ORGsQyk36ImIB3gSuB7oCU0Wkq2MZpdTLSqk4pVQc8DTwrVLqvCvbeoJT6Tk8v2oPg9pH8D+6DV+rZhY9MbpWi1yp6fcDDiulkpRS+UACMK6M8lOBpZXcts75+sBZrvnLV+QW2JkzvnvRB1TTqoueGF2rTa4k/SjghMPzZHPZFUSkETAGWF6JbR8UkUQRSaxL426v230agJcm9qRNeKCbo9Hqo8s3Z+msr9U8Vy7kOqvaltb6eBPwg1LqfEW3VUrNB+YDxMfHu71187tDqbz93RF2nUxnZNdmTIpv5e6QtHrKWw+trNUiV5J+MuCY8aKBlFLKTuFy005Ft60zTqfn8tv3jYnLL+VZGd2tuZsj0uozLy9BRA+trNUOV5L+ZqCDiLQFTmIk9ttLFhKREGAocGdFt61rXvvyELkFNjY8NpQWIQH4euvBSLWaZRHRNX2tVpSb9JVSVhGZCawDLMACpdQeEZlhrp9nFh0PfKGUyipv2+p+EdUpO9/Kym0nmdgnWrfha7XG4iX65iytVrh0c5ZSajWwusSyeSWeLwIWubJtXfbtgVRyCmzcFKvH09Fqj7eXYNUd9bVaoNstHBw4nclf1uynWWM/+sWEuTsczU1EZIGInBWR3eWU62veiDixqscM8LWQU2Cr6m40rVw66ZvyrXYmzvuR4+ezefaGrnhb9FvTgC3C6HpcKvPGw79hNF1WWSNfb7LzrNWxK00rk85spsRj58nMtfKHG7tys27aadCUUhuB8+UUexjjfpSz1XHMRr4WsvJ1TV+reTrpm77adxYfizC5r+6Pr5VNRKIwOi7Mc6GsSzcdBvl5k6Vr+lot0EkfOHoui4TNJ7iuSzOC/PTAo1q5XgWeVEqVWzVXSs1XSsUrpeIjIyNLLdfIz1vX9LVa0aAznM2u+HhrMs+v2oPVrph1XQd3h6R5hnggQYzhEyKAsSJiVUqtrOwOg/wsnLqYU03haVrpGnTSf2ndfv79bRJXRQaycFo/Woc3cndImgdQSrUtfCwii4DPqpLwwbiQq5t3tNrQYJP++z8f47/fHeH67s15dUocft4Wd4ek1REishQYBkSISDLwPOADV96fUl0C9YVcrZY0yKT/9f6zPLtiN0M7RvLXCT11wteKUUpNrUDZadVxzEDzQq5SCrPZSNNqRINM+p/uSCEiyJf5d/fRCV+rEwL9vLHaFfk2uz4ntRrVIHvvbD1+gd6tm+gPl1ZnBPoa52J2nm7i0WpWg0v6X+8/y9G0bHq1buLuUDStSCOzq/AlfTFXq2ENKumfzczlofe3EBHky409W7g7HE0rEuhrJP1sfTFXq2ENKun/9/sjFNgUH80YQKsw3T1TqzsC/YzmHV3T12pag0n6Sik+33mKIR0iiInQ4+RrdUvjAB8AMnIK3ByJVt81mKS/JyWD5As5jOmupz7U6p6IQD8AUi/luTkSrb5zKemLyBgROSAih0XkqVLKDBOR7SKyR0S+dVh+VER2mesSqyvwivp460l8LV6M6qqTvlb3RAT7AnBOJ32thpXbT98cN/xNYCTGROebRWSVUmqvQ5lQ4C1gjFLquIg0LbGb4Uqpc9UXdsWcycjl423JjOjSlCaBvu4KQ9NK1cjXm0a+Fs5l5rs7FK2ec6Wm3w84rJRKUkrlAwnAuBJlbgc+VkodB1BKVcsY49Xlxc/2km+18z8jO7o7FE0rVUSQn67pazXOlaQfBZxweJ5sLnPUEWgiIt+IyBYRudthnQK+MJc/WNpBXB13vKIu5VlZv/cMt/WJpmOz4Grbr6ZVt4ggX530tRrnyjAMzgYCKTmDszfQBxgBBACbROQnpdRBYKBSKsVs8lkvIvvNmYmK71Cp+cB8gPj4+GqbIXrt7tPkWe3cqGfD0uq4iCA/jqZluTsMrZ5zpaafDDhOJxUNpDgps1YplWW23W8EYgGUUinm77PACozmolqhlGLB90fo0DSI+Db6Dlytjvr1a9iRQESwH2mXaqBN324zjqFpuJb0NwMdRKStiPgCU4BVJcp8AgwWEW8RaQT0B/aJSKCIBAOISCAwCthdfeGXbVNSGntPZXDfoLZ65EKt7tqxFL7+MxFBfpzPzsdqs1fv/n98HRbfAoc3QEEu2Kt5/5pHKTfpK6WswExgHbAP+EAptUdEZojIDLPMPmAtsBP4BXhbKbUbaAZ8LyI7zOWfK6XW1sxLudLiTccIC/Tlll4lL0FoWh3i7Q/WXCKDfFEKzmdXU20/LxP2roJzh4znGafgp7fgH92MdVqD5NLQykqp1cDqEsvmlXj+MvByiWVJmM08tU0pxU9JaVzXpRn+Pno0Ta0OM5N+RJBxg9a5zHyaBvtXfb+fPw47E6Blr8vL9n8Owc3AT3dqaKjq7R25R9OyuZBdQG/dlq/VdT7+UJBLRLCZ9KurB8/F48bv7DTjd/oJOJkInW6onv1rHqneJv1fjhgnem89hLJW13kHgC2PiEBj/J1qS/re5o2IBeaE69/+DXyDoMfE6tm/5pHqZdLPt9p565tf6dA0iA5Ng9wdjqaVzduo4Uf4GxdYqy3pW4z9kn3+8rL4eyGsrfPyWoPg2Uk/7Vf4aDrs/LDY4lU7UjiWls3TYzvj5aV77Wh1nE8AAEEWG37eXpyrTLdNux0OrQflcIuL+c8E5TBGf7CeR6Kh8+ykn/Q17F4OKx4sdrIv3nSUTs2CGd6p5BBAmlYHeRsXbcW8mJuaWYma/q4P4f2JsPoJSD9pLPNy0oEhMLIKgWr1gWdPjG41PxzKDhePwc//Jqfl1ew66c3Maztc7pufnw22PAjQ7ftaHWQmfQpyaBnqz8mLORXfR77ZBXPzf4yfgbMg58KV5XTSb/A8u6Zf4PDh2Pcp/PQWAR/fjV1R/A7cRTfA32KKf/XVtLrCx0z61jyiQgNIqUzS9ypRf/vhNUj65spyQfrbb0Pn2Unfmmv89vaHTW8Zi8QHEYhrHWqssxVAylbj8Zon4dJZ4xtCYXc2R9nnIeEOuFR9A75pWrm8jTZ9rDm0DA3gdHouNnsFKyi56a6V0zX9Bs/Dm3dyjQ9MkzaQuh+AdGlMz6gQGtsyYM9GCHSo2fzyb/j1K2jdH7a9Bz0nw9AnIfwqY/3Wd2D/Z9A4CsLaQYeRl9dpWk0prOkX5BLVJIJm9rNk/vg2oYMecH0fuRmulWsUXvH4tHrFs2v6BbnGB6ZJTNGixraLDOsYDp/8Fj6cZtyBCND/IWgzENIOGQkfYOcy+P4V4/b0T2ddruGnHYK1T0LKtlp9OVoDVdimb80hKjSA933/TOiGxys2VEJuOvgGw4CHiy+PGVz8ubOLu1qDUg9q+v4QYgwCarX442PLZaJtDWSeNsokfW3ckDLmL3B8Eyy8vvg+tr0HKdvhjMM4cOcOG78jOtT8a9A0b4c2/SYBNBPzAqzNySTpF47ByS3QfcLlZQW5kJECgeEQ3r54+cAIePyQ0XR56UzNxK95FM+s6RfkGINIFSZ9sx/yUb/OALT6+QWjRw/A2b3GNwERaNbt8j5u/xCCzTH2z5QY+DPdbO8v+QHSGgQRWSAiZ0XE6YiwInKHiOw0f34UkaqNL2X206cgh6gmAXhhnrs2J/31358IH91rJPnjPxnLFl4PBz4Hv8ZGBQfAy7i7l9wM4+Jt087QbmiVwtTqB89M+htmwxvxkHbYSPqdbwRgbu7N5It5Q8rpnZfLFzb/+IdAaGvoOAY6joJmXUs/RuNo8A2skfC1Om8RMKaM9UeAoUqpnsCLmJP/VFrhTVTWXBr5euMt5s1UVif99dPMb6GvdIEFoyEr7XJHBZ8A8GlkPI40KkB6NE2tJM9M+mf3Gb9Tthlt+u1H8PENO1ib3Zmfp+y8sj++Q5s/D2+DKUuMx4FldF+L0LX8hsqc2e18Get/VEoVdoL/CWNiocor6r1j9EazFE5M5yzp+5SoiBzfdPnxpbNGhWb0n2HyYmNZ15LTWWsNnWe26TtOiGJ+YFbvTaVVWACDOjaDFrFGH2WfRka7fIeRl8tbHF7ymD/D+SQ48dOVx2gzsGZi1+qb+4A1pa0054V+EKB169bOCzn03inGViLpZ5+/fBNWob0rLz/OSAEvL7jmd8bzZ1Iu1/w1zeSZNf2LDvO0e/uhlGLLsQtc3TbcuAu3sC0+dgr8ZiO0G+Z8PwFNYMBM47HFt/i6npOqPWytfhGR4RhJ/8nSyiil5iul4pVS8ZGRpfSRd+i9U2zbkjX9wqad2z+AR83LDYe+uLy+cYlxdXwDi1eQNA0Xa/oiMgZ4DbBgzIr1VydlhgGvAj7AOaXUUFe3rRClID358nOfAH5NzeJCdgHxMWazTlg7s6wL08IV3qwiFrj6dxASBY0iijcJaVoJItITeBu4XimVVqWdWXwBuaKmn5mVRWPHBYU3FIa2MQZOEy+jq2azHnDtc9C0S5XC0BqGcpO+iFiAN4GRGBOgbxaRVUqpvQ5lQoG3gDFKqeMi0tTVbSssK9X82iuAAm8/thwzml/7tAkzyhS21WeX2ix7WWHSV3ajuUfTyiEirYGPgbuUUgerYYdGM0xB8Zr+mfPpl5P+xpfhq/8zHodEG82UQc0g85TRK61TWdedNe0yV5p3+gGHlVJJSql8IAEoeXXoduBjpdRxAKXU2QpsWzGFX3FbmL3kLL4kHr1AaCMf2kUEFl9X8sYUZwIjjN+F3ea0Bk9ElgKbgE4ikiwi9znOCQ38EQgH3hKR7SKSWOWDBjeDzJRii85ddLjLtjDhixf4md0y7Vbjd+urq3x4reFwpXknCnBoRCcZ6F+iTEfAR0S+AYKB15RS77q4LeDiBS+AU2ZXzLaD4dR2sOWz5fgF+rRucnns/MiO8L8HjJpQefwaw5AnoMvN5ZfVGgSl1NRy1t8P3F+tBw1tY9x45eDcxUtODu7QZHnT65BxEnrfU62haPWbK0nf2ZWgkqNBeQN9gBFAALBJRH5ycVtjoVLzMfs7x8fHlz7a1OmdRpNMpNF+WZCfS1JqFhN6RRUvF9y81F0UI2K0h2qaO4W2vjxkiOlCRiZsWQSZZ4xOByWHSu48tvbi0+oNV5J+MtDK4Xk0kOKkzDmlVBaQJSIbgVgXt3VdXiYc+wGa94SAUACys7MA6Ny8cRkbalod16QNZJ+D5ZcHWTuZdgE+NZt1/Mzze2ipHYU0zSWutOlvBjqISFsR8QWmAKtKlPkEGCwi3iLSCKMJZ5+L27rux38aX4H7/6bodvO8nGwAOjYLrvRuNc3tCocE2fVB0aKsrOzL6/My4No/wPBnajkwrb4pt6avlLKKyExgHUa3ywVKqT2FF7WUUvOUUvtEZC2wE7BjdM3cDeBs20pHm5VqDA3bcTSc+AWA/NxsAnwsRDfRF2I1D9ZmwBWLfLEWX6AnQNGqgUv99JVSq4HVJZbNK/H8ZeBlV7atNGv+5XFK/IyafUqeH91aNtYToGuerUkbmP4FLBhVtKhFgI1ieb+sYUM0zUWedUeuLe/ynbORndkZ9zwPZT3Ib4bqiU60eqBJm2JPuwZllrle0yrDs8beseVfTvoifBV0I+flEEM76ingtHqgRBfjGG+jt865sf8hol0vPb+DVi08q6ZvzQfvy2PkpFzMoWmwH77envUyNM0pEegwCnpOAZ9Awm3GPY57Mvx1wteqjWdlS1seWPyKnp68aEwkrWn1xh0fwoR/g7cvfpdOApB4zsfNQWn1iYcl/YJio2GmXMzVSV+rn7z9kYIs7Hjx9Snf8strmos8K+lb84qad5RSnLxoTCStafWOWbnJ8G/BnrO5ZOQ6mS9X0yrBs5K+Q/NOWlY++VY7LUL83RyUptUAs2uyatIOpWDb8YvujUerNzws6ReAxWjfTLloDEOrm3e0ekksAAS27IzFS/gpqWpD9mtaIc9K+ta8ohpQykVjwgndvKPVS+ZQ374tunF1uzDW7T6NUqWPQ6hprvKspG/LL2reKazp6+YdrV66+Z8wZSn0vpuxPVqQdC6LQ2edDLWsaRXkgUnfaN45lZ6Dn7cXYYG6Z4NWDzXvbgyd7GVhcHvj5sNfjrgwE5ymlcOzkr5j80660V1T9MTPWj3XKiyAiCA/th67UH5hTSuHZyV9h2EYUi7m0DJUN+1o9Z+I0KdNKD8fOa/b9bUq89ikf+piLi1C9EVcrWEY1bU5Jy/m6CYerco8J+nb7cZE0N5+FNjsnMnUd+NqDcfYHi0I8vNm5faT7g5F83Cek/RtecZviy9nMnJRClrqnjtaAxHga+HqduH8+Kvur69VjUtJX0TGiMgBETksIk85WT9MRNJFZLv580eHdUdFZJe5PLHSkdryjd8W36I++rqmrzUk11wVzrG07KLuyppWGeUmfRGxAG8C1wNdgaki0tVJ0e+UUnHmz59KrBtuLo+vdKRWM+l7+3EqvfBuXF3T1xqOoR0jAFj80zE3R6J5Mldq+v2Aw0qpJKVUPpAAjKvZsJxwaN4prOnrC7laQ9K+aTC3xLVkwfdHyMm3uTsczUO5kvSjgBMOz5PNZSVdIyI7RGSNiHRzWK6AL0Rki4g8WNpBRORBEUkUkcTU1NQrCxRr3skhJMCHQD/PmvhL06pqXFwUeVY7247rPvta5biS9J3d/VSys/BWoI1SKhb4J7DSYd1ApVRvjOah34nIEGcHUUrNV0rFK6XiIyOdTH9Y1Lzjy6n0HD38gtYgxcc0wUvgJ911U6skV5J+MtDK4Xk0kOJYQCmVoZS6ZD5eDfiISIT5PMX8fRZYgdFcVHFFzTt+nLyYqwda0xqkYH8fekaH8tX+M+4ORfNQriT9zUAHEWkrIr7AFGCVYwERaS7meAgi0s/cb5qIBIpIsLk8EBgF7K5UpDZzEgmLWdPXF3G1GiIiC0TkrIg4PVfF8LrZm22niPSuzfhuim3J7pMZHD6bWZuH1eqJcpO+UsoKzATWAfuAD5RSe0RkhojMMItNBHaLyA7gdWCKMu4XbwZ8by7/BfhcKbW2UpFajZp+rrJwMbtAd9fUatIiYEwZ668HOpg/DwL/qoWYitwU2wJfixcLfjham4fV6gmXroSaTTarSyyb5/D4DeANJ9slAbFVjNFgXshNyzUuMbTUPXe0GqKU2igiMWUUGQe8a1ZsfhKRUBFpoZQ6VRvxNQ32Z1LfaJZtPsH/juxIeJBfbRxWqyc8547cq4bDHy9wqnFPAMKD9JDKmtu42qOt/F5plTSlb2sKbIov952ttn1qDYPnJH0ALy8u5hj9k0MCfNwcjNaAudKjzVhYXq+0SurWsjFRoQF8vqtWvlxo9YhnJX3gYo5xQTc0QNf0Nbcpt0dbTRMRbouP5tuDqew/nVGbh9Y8nOcl/WyjbT+kka7pa26zCrjb7MVzNZBeW+35jqYNiMHfx4v3fzpe24fWPJjHJf30nAK8BIL13bhaDRGRpcAmoJOIJIvIfSV6q60GkoDDwH+A37ojztBGvozo3IzPd52iwGZ3RwiaB/K4zHkxu4CQAB+8vPQ0iVrNUEpNLWe9An5XS+GUaULvKD7fdYrPdqYwvle0u8PRPIDH1fQv5hQQ2ki352sawPBOTenSojGvf3kYq67tay7wvKSfnU9j3XNH0wDw8hJmjejAkXNZrNpRq9eSNQ/lcUk/PaeAUJ30Na3I6G7N6NqiMa9/eUjX9rVyeVzSL2zT1zTNICLMuq4DR9OyWbvntLvD0eo4j0v6l/KsBPt73PVnTatR13VpRquwAN7dpGfV0srmkUk/SHfX1LRiLF7Cnf3b8MuR8/pmLa1MHpX0C2x28q12PWOWpjlxW3wrfL29+M/GI+4ORavDPCrpZ+VZAXTS1zQnwgJ9mTYghuVbk3n/Z93MoznnUUn/kpn0g/wsbo5E0+qmWSM6MLhDBM+u2K1n19Kc8qikn5VnjLCpa/qa5lygnzdv3xNP5+bBPLl8Fxey8t0dklbHuJT0RWSMiBwwp4d7ysn6YSKSLiLbzZ8/urptRVyu6eukr2ml8fO28MqkOC5m53P72z+z9fgFd4ek1SHlJn0RsQBvYkwR1xWYKiJdnRT9TikVZ/78qYLbuiRLJ31Nc0nXlo35n5Ed2XcqgwffTdQ3bWlFXKnp9wMOK6WSlFL5QALGdHGuqMq2V7ikL+RqmsseGnoVL47rxrlL+Ww8VH2zdmmezZWk7+rUcNeIyA4RWSMi3Sq4rUvTyunmHU1znYgwuW9rmjTyYfnWk+4OR6sjXEn6rkwNtxVoo5SKBf4JrKzAtsZCF6aV0102Na1ifL29uDm2Jat3neLHX8+5OxytDnAl6Zc7NZxSKkMpdcl8vBrwEZEIV7atiMtJX3fZ1DRX3TMghsb+Ptz/TiIZuQXuDkdzM1eS/magg4i0FRFfYArGdHFFRKS5iIj5uJ+53zRXtq2IS3k2fCyCn7dO+prmqnaRQbwzvR/Z+TZWbtPNPA1duUlfKWUFZgLrgH3AB0qpPSWmj5sI7BaRHcDrwBRlcLptZYPNLbDh76MTvqZVVFyrUOJahTJ/Y5KeWrGBc6lx3GyyWV1i2TyHx28Ab7i6bWXZ7Aofi0fdT6ZpdcYjI9ozfVEiK7aeZFLfVuVvoNVLHpVBrXaFl+i5cTWtMoZ3akqPqBBe+/IQZzJy3R2O5iYelfRtdjveekJ0TasUEWH2zd04n5XPkJe+Zu3uU+4OSXMDD0v6xrjhmqZVTp82Tfj04YF0bBbMYx/s4MDpTHeHpNUyD0v6dp30Na2K2jcN5u174gn08+aBdxO5mK0HZWtIPCvpK3TzjqZVg2aN/Zl3Zx9Opecw470tnErPcXdIWi3xrKRvt+Olk76mVYs+bZow55Ye/JR0non/2kS+VXflbAg8LOkrXdPXtGo0qW8r3rqjNycv5rD4Jz3bVkPgcUlft+lrWvW6vntzhnSM5MXP9uppFhsAj0r6Vp30tVriwsRBISLyqTmy7B4RudcdcVYHEWH+XX0Y3imSP36yh4NndI+e+syjkr6u6Wu1wcXJf34H7DVHlh0G/N0cX8oj+ftY+PukOIL8vJk6/ye+0+Pv11uel/T1HblazXNl8h8FBJsDDQYB5wFr7YZZvcICfUl48Goigvy4b1Eie1LS3R2SVgM8L+nrmr5W81yZ/OcNoAvGUOG7gFlKqSu6v7gyOVBd0qVFYxIevJomgT488E4i3xw46+6QtGqmk76mXcmVyX9GA9uBlkAc8IaINL5iIxcmB6prmgT68t97+nIpz8q0hZtZt+e0u0PSqpFHJX19IVerJa5M/nMv8LE5hPhh4AjQuZbiq3Hdo0LY9PQIWoUF8JvFW1jw/RF3h6RVE49K+nal++lrtcKVyX+OAyMARKQZ0AlIqtUoa1ignzcJD15Dr9ahzFm9j8c+2M624xfcHZZWRR6V9K02XdPXap6LEwe9CAwQkV3Al8CTSql6NwltVGgA/7qjD0M7RvLx1pPc/04iaZfy3B2WVgUuJf3y+iw7lOsrIjYRmeiw7KiI7BKR7SKSWJVg7Uonfa12KKVWK6U6KqWuUkrNMZfNK5w8SCmVopQapZTqoZTqrpR6z70R15zmIf4smNaXdY8OIS0rnz7/t4HPd+phmT1VuUnfxT7LheX+hlE7Kmm4UipOKRVflWB1m76muU+n5sH0jA4B4HdLtpKerSdZ90Su1PRd6bMM8DCwHKixPl5G7x2PapHStHrlzdt7c0tcSwBi//QFT3+8iwtZemhmT+JKBi23z7KIRAHjgXlcSQFfiMgWEXmwtIO40p9ZD7imae7VKqwR/5gcxyuTYpnarxUfbTnBQ+9vITvfyrcHU1GqZM9Wra5xZWJ0V/osv4pxIcsmV94xO1AplSIiTYH1IrJfKbXxih0qNR+YDxAfH+/0zLG5eY7cgoICkpOTyc3V84t6An9/f6Kjo/Hx8XF3KPWKiDChdzQTekcT1yqUJ5fvYsyr33H8fDbz7uzNmO4t3B2iVgZXkr4rfZbjgQQz4UcAY0XEqpRaqZRKAVBKnRWRFRjNRVckfVe4u6afnJxMcHAwMTExOPnnptUhSinS0tJITk6mbdu27g6n3prYpxX/3pjE0XNZBPpamL1qL1/vT+XhEe2JbtLI3eFpTrjSvFNun2WlVFulVIxSKgb4CPitUmqliASKSDCAiAQCo4DdlQ3WaldunUQlNzeX8PBwnfA9gIgQHh6uv5XVMIuX8Pbd8Xw44xoeGNKO0xm5LEs8wW/f30pS6iXsdt3cU9eUW9NXSllFpLDPsgVYUNhn2VzvrB2/UDNghZkkvYElSqm1lQ22LtycpRO+59B/q9rRLjIIgD5twrh3YFu+P3SOJz7awbV//xZfby+ubhfOI9e2Jz4mzM2RauBa8w5KqdXA6hLLnCZ7pdQ0h8dJQGwV4ivGatMTo2taXRYS4MMNPVvQr20YHySe4GxGLuv2nGHivE3cc00b7h/cjlZhutnHnTyq/2NDHnAtLS2NuLg44uLiaN68OVFRUUXP8/PL7jKXmJjII488UuFjbtu2DRFh3Tpnt15oWukig/343fD2vDCuO+seHUJUaADvbDrG7FV7+ObAWQ6f1RO1uItLNf26wlYHmnfcJTw8nO3btwMwe/ZsgoKCePzxx4vWW61WvL2d/znj4+OJj6/4fXFLly5l0KBBLF26lNGjR1cqblfYbDYsFkuN7V9zr5BGPix94Gruf3czXx04y5f7z9KtZWM+e3iQboJzA89K+m6+kOvohU/3sDclo1r32bVlY56/qZvL5adNm0ZYWBjbtm2jd+/eTJ48mUcffZScnBwCAgJYuHAhnTp14ptvvmHu3Ll89tlnzJ49m+PHj5OUlMTx48d59NFHnX4LUErx0UcfsX79egYPHkxubi7+/v4AvPTSSyxevBgvLy+uv/56/vrXv3L48GFmzJhBamoqFouFDz/8kBMnThQdF2DmzJnEx8czbdo0YmJimD59Ol988QUzZ84kMzOT+fPnk5+fT/v27Vm8eDGNGjXizJkzzJgxg6QkYyyzf/3rX6xZs4aIiAhmzZoFwLPPPkuzZs0q9W1Gqx2twxvx8sRYJs77ke5RIWw7fpG/rNlP02A/+saE0SMqpM58tus7j0v6DbWmX5qDBw+yYcMGLBYLGRkZbNy4EW9vbzZs2MAzzzzD8uXLr9hm//79fP3112RmZtKpUyceeuihK/qy//DDD7Rt25arrrqKYcOGsXr1aiZMmMCaNWtYuXIlP//8M40aNeL8+fMA3HHHHTz11FOMHz+e3Nxc7HY7J06cuOLYjvz9/fn+++8Bo/nqgQceAOC5557jv//9Lw8//DCPPPIIQ4cOZcWKFdhsNi5dukTLli2ZMGECs2bNwm63k5CQwC+//FIdb6dWg2JbhbL7hdFYbYpHlm7jP98l4Xgv18D24Yzo3IxrrgqnS4srpibQqonHJH27XWFXuPXmLEcVqZHXpNtuu62oaSQ9PZ177rmHQ4cOISIUFDgfG+WGG27Az88PPz8/mjZtypkzZ4iOji5WZunSpUyZMgWAKVOmsHjxYiZMmMCGDRu49957adTIuBgXFhZGZmYmJ0+eZPz48QBF3wjKM3ny5KLHu3fv5rnnnuPixYtcunSpqDnpq6++4t133wXAYrEQEhJCSEgI4eHhbNu2jTNnztCrVy/Cw8Ndfcs0N/LztuDnDf+d1pfUzDzScwq48+2fOZ2Ryw+H0/jhcBoAC6bFM7xTUy5kFxAW6LFTD9dJHpP0bWaVQNf0iwsMDCx6/Ic//IHhw4ezYsUKjh49yrBhw5xu4+fnV/TYYrFgtRaf2tVms7F8+XJWrVrFnDlzim50yszMRCl1RTtsabfee3t7Y7dfnkGwZJ95x9inTZvGypUriY2NZdGiRXzzzTdlvu7777+fRYsWcfr0aaZPn15mWa1uigz2IzLYjzWzBpNdYCMp9RJ/XbOfPSkZTF+UyIjOTfly/1keGNyWp6/vopt/qonH9N6xmTd5WCz6D1+a9PR0oqKMYZEWLVpU6f1s2LCB2NhYTpw4wdGjRzl27Bi33norK1euZNSoUSxYsIDs7GwAzp8/T+PGjYmOjmblypUA5OXlkZ2dTZs2bdi7dy95eXmkp6fz5ZdflnrMzMxMWrRoQUFBAe+//37R8hEjRvCvf/0LMP4ZZWQY11HGjx/P2rVr2bx5c41eZNZqXpNAX6JCAxjcIZLPHh7Ewml9uSoykC/3G2M3/ue7I3R4bg3v/XSMWQnb+GLPaQ6dydTj/FSS5yX9OtK8Uxf9/ve/5+mnn2bgwIHYbLZK72fp0qVFTTWFbr31VpYsWcKYMWO4+eabiY+PJy4ujrlz5wKwePFiXn/9dXr27MmAAQM4ffo0rVq1YtKkSfTs2ZM77riDXr16lXrMF198kf79+zNy5Eg6d7486+Brr73G119/TY8ePejTpw979uwBwNfXl+HDhzNp0iTd86ceERGGd27Ke/f3Z3CHCJ4Za5wLNrviuZW7+WR7Cg8u3sLIf2xk3rdJnMnQd1xXlNTF/5bx8fEqMbH4fCsZuQX0nP0Fz93QhfsHt3NLXPv27aNLly5uObZWnN1up3fv3nz44Yd06NCh1HLO/mYisqWqcztUhrPzWiubza74/Uc7GdO9Ocs2H2dop6a0iwhkVsJ2zl3Ko5GvhZnXtqdvTBh9zTt+86w2/LwbXkXA1fPac9r0bWZNX7frNXh79+7lxhtvZPz48WUmfM3zWbyEv08ybuof2bVZ0fI/3NiF//1gB3lWOy+tPQDAtZ2bEhUawLLEE0Q3CWBK31bcN6gdFi+hwGbHx3K5YePE+ewGe2ewxyR9q11fyNUMXbt2Leq3rzVM4+KiGBcXhVKK4+ezeXbFbrYev8BX5nWApNQs/rx6P+/9dJz4Nk34Yu8ZPvjNNXRt2Zi/rNnHv79N4t939WF0t+ZufiW1z2OSvl0V1vQ95jKEpmk1TERoEx7Ie/f3x2qzczQtCz9vCzuT07ErxfOr9vDxtpMAPPHRDga2j2D+RqPC8MWeM2w9doFWYY24o3/rBnN3sMck/cKavkXnfE3TnPC2eNG+aTBAUdPNgKvCOXY+m9TMPB7/YAd7TyUx4KpwvERYvjW5aNuzmXl8tiOFv0+K5ZPtKcwYehXNQ1y738TTeEzSt9t1TV/TtIoJD/IjPMi4L2XYHyIRBF9vL5ZvSWZTUhoTekXx1f6zvP7lIQDGv/UjAIt+PMoDg9vy7A1d3RZ7TfGYpK9r+pqmVYVjj55b+0QzoXcUIsJX+8+weNMxzmcXsPtkOs0b+3PyYg7/+e4IX+0/yyMjOvBhYjKvTokjIsiPi9n5BPhaPLaHkMckfZuu6TNs2DCefvrpYjcjvfrqqxw8eJC33nqr1G3mzp1LfHw8Y8eOZcmSJYSGhhYr42zUTkexsbF07dqVpUuXVttr0TR3K2zDv7ZzM67t3KzYuhPns7nxn9/za2oWsxK2A3Dn2z/TtWVj1u85Q77NTpcWjWnW2I+4Vk34KSmNaQNiGNIxss73MHQpg4rIGBE5ICKHReSpMsr1FRGbiEys6LblseneO0ydOpWEhIRiyxISEpg6dapL269evfqKhF+effv2Ybfb2bhxI1lZWRXatiJKDgWhae7UKqwR2/84khW/HQBARJAvSsHPSeexKYWXCN5ewuajF/jb2v18ezCVexdt5rpXviU92/mYV3VFuTV9EbEAbwIjMSZJ3ywiq5RSe52U+xvGtIoV2tYVVnMMl7oy4BprnoLTu6p3n817wPV/LXX1xIkTee6558jLy8PPz4+jR4+SkpLCoEGDeOihh9i8eTM5OTlMnDiRF1544YrtY2JiSExMJCIigjlz5vDuu+/SqlUrIiMj6dOnj9NjLlmyhLvuuot9+/axatWqon8wmzdvZtasWWRlZeHn58eXX35Jo0aNePLJJ1m3bh0iwgMPPMDDDz9c7LiJiYk8/vjjfPPNN8yePZuUlBSOHj1KREQEf/7zn7nrrruK/rm88cYbDBhgfOhKDuf8wAMPcNttt7F161YADh06xJQpU9iyZUuV/gSaVkhE6NW6Cb88M4LIYD+nvXtSLubwxZ7T9GkTxhMf7WD/6UzGvLaRu65pw9juLfjrmv2cz8rnmRu6YFeK1mGN2HgwlY7NgukeFeKGV+Va804/4LA59SEikgCMA0om7oeB5UDfSmxbrsJxuxpyTT88PJx+/fqxdu1axo0bR0JCApMnT0ZEmDNnDmFhYdhsNkaMGMHOnTvp2bOn0/1s2bKFhIQEtm3bhtVqpXfv3qUm/WXLlrF+/XoOHDjAG2+8wdSpU8nPz2fy5MksW7aMvn37kpGRQUBAAPPnz+fIkSNs27YNb2/vomGXy7Jlyxa+//57AgICyM7OZv369fj7+3Po0CGmTp1KYmKi0+Gcw8LCCAkJYfv27cTFxbFw4UKmTZtWlbdX05xq2rj0XjwtQwOYNrAtAGsfHcLPSWm8sv4gL609wEtrDxDs541dKW5584di2/lavFh0b18ig/0I9vehWWPn/1RqgitJPwpwHBg9GejvWEBEooDxwLUUT/rlbuuwjweBBwFat259xfrCmn6daS8ro0ZekwqbeAqT/oIFCwD44IMPmD9/PlarlVOnTrF3795Sk/53333H+PHji4ZHvvnmm52W27x5M5GRkbRp04bo6GimT5/OhQsXSE5OpkWLFvTta/ypGzc2xj7fsGEDM2bMKJrBKyys/Imwb775ZgICAgAoKChg5syZbN++HYvFwsGDB4v2W3I4ZzBG2ly4cCGvvPIKy5Ytq9Yx9UVkDPAaYAHeVkpd8QcXkWHAq4APcE4pNbTaAtA8Uv924SQ8eDWvfXmIA6czefS6juQU2PjN4kTOZOQxpGMk6dn57EhO5/a3fy7ablinSBZO68vhs5doGRpAoN+VqdluV1jtCl/vql3XdCXpO8uyJQfseRV4UillK/HfypVtjYVKzQfmgzFGScn1l2/OqiNJ301uueUWHnvsMbZu3UpOTg69e/fmyJEjzJ07l82bN9OkSROmTZt2xTDGJblSq1i6dCn79+8nJiYGgIyMDJYvX06/fv2cbu9s2GUoPsRyWcMr/+Mf/6BZs2bs2LEDu91eNC5/afu99dZbeeGFF7j22mvp06dPtY2p70qzpIiEAm8BY5RSx0WkabUcXPN4IsKj13UstuzHp0Zgc0jYu5LTOXgmk4zcAg6eyWTpLye4878/88PhNJo19iMiyA+rTfHiLd0BSPjlOKO6Nef3H+1g4b396NOmSaXjc+VfRjLQyuF5NJBSokw8kCAiR4GJwFsicouL27rEqsfeASAoKIhhw4Yxffr0ovb1jIwMAgMDCQkJ4cyZM6xZs6bMfQwZMoQVK1aQk5NDZmYmn3766RVl7HY7H374ITt37uTo0aMcPXqUTz75hKVLl9K5c2dSUlLYvHkzYAyLbLVaGTVqFPPmzSu6KFvYvBMTE1PU1u5sJq9C6enptGjRAi8vLxYvXlw0Uqiz4ZzBmKxl9OjRPPTQQ9x7770uv4cuKGqWVErlA4XNko5uBz5WSh0HUEqdrc4AtPrF4iXFaug9okO4tU809w5sy5xbenDPNW34Kck4r89k5HH47CUycguY9O9NTPr3Jj7edpIZ720hO99Gx2ZBVYrFlaS/GeggIm1FxBeYAqxyLKCUaquUilFKxQAfAb9VSq10ZVtXXe6y2bCTPhhNPDt27Cia2So2NpZevXrRrVs3pk+fzsCBA8vcvnA+3bi4OG699VYGDx58RZmNGzcSFRVVND4/GP8s9u7dS1paGsuWLePhhx8mNjaWkSNHkpuby/3330/r1q3p2bMnsbGxLFmyBIDnn3+eWbNmMXjw4DKHQf7tb3/LO++8w9VXX83BgweLvgWUNpwzGNM0igijRo1y/Q0sn7NmyagSZToCTUTkGxHZIiJ3O9uRiDwoIokikpiamlqdMWr1hJeX8MK47uz902i+fnwY/WLC+HDGNXz+yGDuH9SW+we1ZfZNxk1ibcIbEezvU84ey6GUKvcHGAscBH4FnjWXzQBmOCm7CJhY1rbl/fTp00eVtO9Uuvrte1vUoTMZV6yrLXv37nXbsTXnXn75ZfXcc8+Vut7Z3wxIVGWf77dhtOMXPr8L+GeJMm8APwGBQARwCOhY1n6dndea5gq73a5e+eKA2nHiQqllyjuvC39cujlLKbUaWF1i2bxSyk4rb9vK6Ny8MW/e0buqu9HqkfHjx/Prr7/y1VdfVfeuXWmWTMa4eJsFZInIRiAWo4KjadVKRPifkR3LL+gCj7kjV9NKWrFiRU3tuqhZEjiJ0Sx5e4kynwBviIg34IvRK+0fNRWQplUXnfQrSJXSk0Sre1QlZ4VTSllFZCbGjYYWYIFSao+IzDDXz1NK7RORtcBOwI7RHLS7mkLXtBqjk34F+Pv7k5aWRnh4uE78dZxSirS0tKJun5XYvtwmTaXUy8DLlQ5S09xAJ/0KiI6OJjk5Gd0LwzP4+/sTHR3t7jA0rU7RSb8CfHx8aNu2rbvD0DRNq7SGO06xpmlaA6STvqZpWgOik76maVoDIpXt1laTRCQVOOZkVQRwrpbDKY2OxTlPiKWNUiqytoMp47wGz3jf3KGuxFJX4oAqntd1MumXRkQSlVLx7o4DdCyl0bFUTl2KVcdSd+OAqseim3c0TdMaEJ30NU3TGhBPS/rz3R2AAx2LczqWyqlLsepYrlRX4oAqxuJRbfqapmla1XhaTV/TNE2rAp30NU3TGhCPSfoiMkZEDojIYRF5yg3HPyoiu0Rku4gkmsvCRGS9iBwyf1d+tuKyj71ARM6KyG6HZaUeW0SeNt+nAyIyuobjmC0iJ833ZbuIjK3pOMx9txKRr0Vkn4jsEZFZ5vJaf1+qyp3ntj6vy4yl1s/tWjmvXZley90/GGOa/wq0w5iwYgfQtZZjOApElFj2EvCU+fgp4G81dOwhQG9gd3nHBrqa748f0NZ83yw1GMds4HEnZWssDnP/LYDe5uNgjBmrurrjfani63Drua3P67p1btfGee0pNf1+wGGlVJJSKh9IAMa5OSYwYnjHfPwOcEtNHEQptRE47+KxxwEJSqk8pdQR4DDG+1dTcZSmxuIwYzmllNpqPs4E9mFMXl7r70sV1cVzu0Gd12XEUpqa/IzV+HntKUk/Cjjh8DzZXFabFPCFiGwRkQfNZc2UUqfA+GMBTWsxntKO7Y73aqaI7DS/Ihd+7ay1OEQkBugF/Ezdel9c4e649HldNred2zV1XntK0nc2TVVt9zUdqJTqDVwP/E5EhtTy8V1V2+/Vv4CrgDjgFPD32oxDRIKA5cCjSqmMsorWRjyV4O649HldOred2zV5XntK0k8GWjk8jwZSajMApVSK+fsssALjK9QZEWkBYP4+W4shlXbsWn2vlFJnlFI2pZQd+A+Xv1rWeBwi4oPxwXhfKfWxubhOvC8V4Na49HldOned2zV9XntK0t8MdBCRtiLiC0wBVtXWwUUkUESCCx8Do4DdZgz3mMXuAT6prZjKOPYqYIqI+IlIW6AD8EtNBVF4IprGY7wvNR6HiAjwX2CfUuoVh1V14n2pALed2/q8Lps7zu1aOa+r6+p3Tf8AYzGuZP8KPFvLx26HcYV8B7Cn8PhAOPAlcMj8HVZDx1+K8fWyAOM/+31lHRt41nyfDgDX13Aci4FdwE7zBGxR03GY+x6E8TV2J7Dd/BnrjvfFU89tfV7XvXO7Ns5rPQyDpmlaA+IpzTuapmlaNdBJX9M0rQHRSV/TNK0B0Ulf0zStAdFJX9M0rQHRSV/TNK0B0Ulf0zStAfl/MD8jrPpLvD4AAAAASUVORK5CYII=\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(1,2,1)\n",
    "plt.plot(acc, label = 'Train Accuracy')\n",
    "plt.plot(val_acc, label = 'Valid Accuracy')\n",
    "plt.title('Train and Valid Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(loss, label = 'Train Loss')\n",
    "plt.plot(val_loss, label = 'Valid Loss')\n",
    "plt.title('Train and Valid Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}