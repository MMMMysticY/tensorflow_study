# 目录
## chapter1
### tf常用函数
1. tf.constant直接创建一个张量
2. tf.convert_to_tensor直接从numpy转为tf
3. tf.zeros tf.ones tf.fill直接创建tensor
4. tf.random.normal生成正态分布的随机数 tf.random.truncated_normal生成截断正态分布
5. tf.cast强制tensor转换数据类型 tf.reduce_min计算张量维度上元素最小值 tf.reduce_max计算张量维度上元素最大值
6. tf.op op有一些基本的数学运算 add subtract multiply divide square pow sqrt matmul 矩阵乘法
7. tf.Variable 将变量标记为可训练的 会在反向传播中纪录梯度信息 with tf.GradientTape as tape + tape.gradient结构 可以对一系列运算过程进行求梯度
8. tf.one_hot one-hot根据取值映射为索引 对该索引位置置为1 其他部分置为0
9. tf.argmax 返回某维度上最大值的索引
10. tf.reshape np_array[tf.newaxis,...] 都可以扩充维度

[chapter1/tf_basic_functions](chapter1/tf_basic_functions.ipynb)  

### 原生tf2写分类任务
[chapter1/Iris_classify](chapter1/Iris_classify.ipynb)  

## chapter2
### tf常用函数
1. tf.where(条件语句, 真返回A, 假返回B)
2. tf.reduce_mean(tf.square(y_ - y) MSE loss tf.nn.softmax_cross_entropy_with_logits 是softmax+ce
3. l1正则化 l2 正则化 tf.nn.l2_loss(w) tf.reduce_sum(all_regularization) loss = loss_mse + loss_regularization
4. SGD 随机梯度下降 w1.assign_sub(lr * grads) 直接减去梯度乘以学习率

[chapter2/tf_basic_functions](chapter2/tf_basic_functions.ipynb)  

## chapter3
### keras基本模式
1. import
2. train test
3. model = tf.keras.models.Sequential 构建网络的组合 / Model的方法
4. model.compile
5. model.fit
6. model.summary

#### Sequential
Sequential相当于一个容器 内部包含了网络的各层  
网络举例 tf.keras.layers.xxx tf.keras.activations.xxx：
1. Flatten() 拉直
2. Dense() 全连接层
3. Conv2D 卷积层
4. LSTM
5. activations.xxx relu sigmoid等
6. tf.keras.regularizers.L2

#### Model
Model方法使用call进行调用

#### compile
compile构建训练方法 主要参数包括optimizer优化器 loss损失函数 metrics矩阵评价标准

#### optimizer
tf.keras.optimizers.xxx SGD Adagrad Adadelta Adam  
#### loss
tf.keras.losses.xxx mse crossentropy  

#### metrics
accuracy y和y_都是数值表示
categorical_accuracy y是one-hot y_概率分布
sparse_categorical_accuracy y是数值 y_是概率分布
#### fit
fit包括训练集的输入特征 训练集的标签 batch_size epochs 验证集的数据(validation_data) 从验证集划分多少给测试集(validation_split) 多少epoch测试一次(validation_freq)
summary  
打印网络的状况

[chapter3/keras_basic_step](chapter3/keras_basic_step.ipynb)  

### mnist数据集-keras
[chapter3/mnist_dataset_exp](chapter3/mnist_dataset_exp.ipynb)  

### fashion数据集-keras
[chapter3/fashion_dataset_exp](chapter3/fashion_dataset_exp.ipynb)  

## chapter4
### 数据集相关
keras可以不需要dataset类似的东西 直接将数据集变为numpy格式就可以直接进行使用  
#### 数据增强 keras.preprocessing

[chapter4/dataset_methods](chapter4/dataset_methods.ipynb)  

### 参数相关
#### 读取模型
model.load_weights(path) 读取模型
#### 保存模型
keras.callbacks.ModelCheckpoint(file_path, save_weight_only, save_best_only) 保存模型  
history = model.fit(callbacks)  
#### 保存参数
model.trainable_variables 返回模型中可训练的参数

[chapter4/param_methods](chapter4/param_methods.ipynb)  

### 训练过程细节
fit的返回值是一个history对象 history.history中有很多训练过程的细节参数  

### 模型使用
model.predict
[chapter4/param_methods](chapter4/use_model_test.ipynb)  

## chapter5
keras conv2d的维度计算 :  
1. padding = 'valid' shape' = (shape - kernel + 1) / stride 向上取整
2. padding = 'same' shape' = (shape) / stride 向上取整

经典Conv结构：
1. LeNet 卷积网络最开始的模型 共享卷积核 减少网络参数
2. AlexNet 使用relu激活 提高训练速度 使用dropout缓解过拟合
3. VGGNet 小尺寸卷积核减少参数 网络规整适合硬件加速
4. InceptionNet 一层内使用不同尺寸卷积核 使用bn缓解梯度消失
5. ResNet 使用层间残差连接 缓解模型退化 使网络加深成为可能

[chapter5/ComplexNet](chapter5/ComplexNet.ipynb)  

## chaper6
RNN LSTM的使用 注意return_sequences 参数的使用 默认是False 只输出最后一个时间片的Hidden[batch, hidden] True代表输出所有时间片的hidden[batch, seq_len, hidden]  
[chapter6/RNN](chapter6/basic_RNN.ipynb)  
[chapter6/LSTM](chapter6/LSTM_exp.ipynb)  

## BERT Experiments
使用BERT进行基本的fine-tune任务的步骤是：
1. **获取处理数据集** 下载或者加载 这个过程有很多技巧 例如
   - keras.utils.text_dataset_from_directory可以直接加载txt文本(见[text_classification_with_BERT](bert_exp/text_classification_with_BERT.ipynb))
   - tensorflow_datasets库tfds的load方法 可以下载也可以从本地加载(**下载基本在国内不行 所以下载了之后加载比较合适**)(见[Fine tune BERT](bert_exp/Fine%20tune%20BERT.ipynb))
   - 下载了TFRecordDataset对象之后 使用tf.io.FixedLenFeature+tf.io.parse_single_example进行解析(**这个似乎在大规模数据集比较合适**)(见[Fine tune BERT](bert_exp/Fine%20tune%20BERT.ipynb))
2. **处理数据集** 处理数据集有很多方法
   - 下载tf hub的BERT对应的preprocess直接进行处理bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)(见[text_classification_with_BERT](bert_exp/text_classification_with_BERT.ipynb))
   - 使用official.nlp库的方法bert.tokenization.FullTokenizer再加上一些原生方法如RaggedTensor.to_tensor()进行简单的pad或者keras.preprocessing.sequence.pad_sequences进行pad (见[Fine tune BERT](bert_exp/Fine%20tune%20BERT.ipynb))
3. **构建模型** 使用BERT预训练的模型作为pre-train部分 当做一个encoder 其会输出四个结果 (见[text_classification_with_BERT](bert_exp/text_classification_with_BERT.ipynb))
   - default default是默认的也就是pooled_output
   - pooled_output 将input sequence表示为一个整体 维度是[batch_size, hidden] 这是整个句子的一个embedding
   - sequence output 表示了每个token的embedding 维度是[batch_size, seq_len, hidden] 是每个token的embedding
   - encoder_output 表示encoder中每个Transformer blocks的中间隐层状态 [block_num, batch_size, seq_len, hidden] block_num代表了transformer encoder的个数 encoder_outputs[-1] == sequence_output
  
    选择合适的embedding进行后续任务 后续任务就是Model的class compile fit等
4. 保存模型 tf.saved_model.save或者model.save 
5. 加载模型 tf.saved_model.load 这里有个问题(如何定义或者修改模型的Input，在[Fine tune BERT](bert_exp/Fine%20tune%20BERT.ipynb)遇到过一些问题 save之后再load维度不对)
6. 使用nlp.data.classifier_data_lib.TfdsProcessor和TFRecord进行大型数据集的encoding(由于网络问题没做)(见[Fine tune BERT](bert_exp/Fine%20tune%20BERT.ipynb))
