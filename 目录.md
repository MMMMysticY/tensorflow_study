# 目录
## chapter1
### tf常用函数
1. tf.constant直接创建一个张量
2. tf.convert_to_tensor直接从numpy转为tf
3. tf.zeros tf.ones tf.fill直接创建tensor
4. tf.random.normal生成正态分布的随机数 tf.random.truncated_normal生成截断正态分布
5. tf.cast强制tensor转换数据类型 tf.reduce_min计算张量维度上元素最小值 tf.reduce_max计算张量维度上元素最大值
6. tf.op op有一些基本的数学运算 add subtract multiply divide square pow sqrt matmul 矩阵乘法
7. tf.Variable 将变量标记为可训练的 会在反向传播中纪录梯度信息 with tf.GradientTape as tape + tape.gradient结构 可以对一系列运算过程进行求梯度
8. tf.one_hot one-hot根据取值映射为索引 对该索引位置置为1 其他部分置为0
9. tf.argmax 返回某维度上最大值的索引
10. tf.reshape np_array[tf.newaxis,...] 都可以扩充维度

[chapter1/tf_basic_functions](chapter1/tf_basic_functions.ipynb)  

### 原生tf2写分类任务
[chapter1/Iris_classify](chapter1/Iris_classify.ipynb)  

## chapter2
### tf常用函数
1. tf.where(条件语句, 真返回A, 假返回B)
2. tf.reduce_mean(tf.square(y_ - y) MSE loss tf.nn.softmax_cross_entropy_with_logits 是softmax+ce
3. l1正则化 l2 正则化 tf.nn.l2_loss(w) tf.reduce_sum(all_regularization) loss = loss_mse + loss_regularization
4. SGD 随机梯度下降 w1.assign_sub(lr * grads) 直接减去梯度乘以学习率

[chapter2/tf_basic_functions](chapter2/tf_basic_functions.ipynb)  

## chapter3
### keras基本模式
1. import
2. train test
3. model = tf.keras.models.Sequential 构建网络的组合 / Model的方法
4. model.compile
5. model.fit
6. model.summary

#### Sequential
Sequential相当于一个容器 内部包含了网络的各层  
网络举例 tf.keras.layers.xxx tf.keras.activations.xxx：
1. Flatten() 拉直
2. Dense() 全连接层
3. Conv2D 卷积层
4. LSTM
5. activations.xxx relu sigmoid等
6. tf.keras.regularizers.L2

#### Model
Model方法使用call进行调用

#### compile
compile构建训练方法 主要参数包括optimizer优化器 loss损失函数 metrics矩阵评价标准

#### optimizer
tf.keras.optimizers.xxx SGD Adagrad Adadelta Adam  
#### loss
tf.keras.losses.xxx mse crossentropy  

#### metrics
accuracy y和y_都是数值表示
categorical_accuracy y是one-hot y_概率分布
sparse_categorical_accuracy y是数值 y_是概率分布
#### fit
fit包括训练集的输入特征 训练集的标签 batch_size epochs 验证集的数据(validation_data) 从验证集划分多少给测试集(validation_split) 多少epoch测试一次(validation_freq)
summary  
打印网络的状况

[chapter3/keras_basic_step](chapter3/keras_basic_step.ipynb)  

### mnist数据集-keras
[chapter3/mnist_dataset_exp](chapter3/mnist_dataset_exp.ipynb)  

### fashion数据集-keras
[chapter3/fashion_dataset_exp](chapter3/fashion_dataset_exp.ipynb)  

## chapter4
### 数据集相关
keras可以不需要dataset类似的东西 直接将数据集变为numpy格式就可以直接进行使用  
#### 数据增强 keras.preprocessing

[chapter4/dataset_methods](chapter4/dataset_methods.ipynb)  

### 参数相关
#### 读取模型
model.load_weights(path) 读取模型
#### 保存模型
keras.callbacks.ModelCheckpoint(file_path, save_weight_only, save_best_only) 保存模型  
history = model.fit(callbacks)  
#### 保存参数
model.trainable_variables 返回模型中可训练的参数

[chapter4/param_methods](chapter4/param_methods.ipynb)  

### 训练过程细节
fit的返回值是一个history对象 history.history中有很多训练过程的细节参数  

### 模型使用
model.predict
[chapter4/param_methods](chapter4/use_model_test.ipynb)  

## chapter5
keras conv2d的维度计算 :  
1. padding = 'valid' shape' = (shape - kernel + 1) / stride 向上取整
2. padding = 'same' shape' = (shape) / stride 向上取整

经典Conv结构：
1. LeNet 卷积网络最开始的模型 共享卷积核 减少网络参数
2. AlexNet 使用relu激活 提高训练速度 使用dropout缓解过拟合
3. VGGNet 小尺寸卷积核减少参数 网络规整适合硬件加速
4. InceptionNet 一层内使用不同尺寸卷积核 使用bn缓解梯度消失
5. ResNet 使用层间残差连接 缓解模型退化 使网络加深成为可能

[chapter5/ComplexNet](chapter5/ComplexNet.ipynb)  