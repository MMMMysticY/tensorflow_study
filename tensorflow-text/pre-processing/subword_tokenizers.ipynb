{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Subword tokenizer\n",
    "subword的tokenizer介于word-based和character-based之间 word太大了 是有可能跟根据语义细分的 character-based太小了 字母级似乎没有任何意义\n",
    "tf_text提供了三种subword level的 tokenizer\n",
    "1. BertTokenizer是一种high level的接口 其集成了token split算法和WordPieceTokenizer 本质是对每个token进行vocab的int映射\n",
    "2. WordpieceTokenizer是一种low level的接口 它仅仅实现了WordPiece算法 在调用之前必须分隔word 其本质也是token对vocab的int映射\n",
    "3. sentencepieceTokenizer是实现了sentencepiece算法 其根据文本训练出了一个模型进行分词sub-word 分词效果很明显"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import collections\n",
    "import os\n",
    "import pathlib\n",
    "import re\n",
    "import string\n",
    "import sys\n",
    "import tempfile\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_text as text\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "tf.get_logger().setLevel('ERROR')\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1' # 使用 GPU 1\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "physical_devices = tf.config.list_physical_devices(\"GPU\")\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0],True)\n",
    "logical_devices = tf.config.list_logical_devices(\"GPU\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "PosixPath('/home/wy')"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd = pathlib.Path.cwd()\n",
    "pwd"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 数据集"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "data_dir = 'tensorflow_study/tensorflow-text/data_dir/datasets/pt_to_en/'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "en_train_list = list()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "with open(data_dir + 'en.train', 'r') as en_train_file:\n",
    "    while True:\n",
    "        line = en_train_file.readline().strip()\n",
    "        if not line:\n",
    "            break\n",
    "        en_train_list.append(line)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "[\"amongst all the troubling deficits we struggle with today — we think of financial and economic primarily — the ones that concern me most is the deficit of political dialogue — our ability to address modern conflicts as they are , to go to the source of what they 're all about and to understand the key players and to deal with them .\",\n 'we who are diplomats , we are trained to deal with conflicts between states and issues between states .',\n 'and i can tell you , our agenda is full .',\n 'there is trade , there is disarmament , there is cross-border relations .',\n 'but the picture is changing , and we are seeing that there are new key players coming onto the scene .',\n \"`` we loosely call them `` '' groups . '' '' they may represent social , religious , political , economic , military realities . ''\",\n 'and we struggle with how to deal with them .',\n 'the rules of engagement : how to talk , when to talk , and how to deal with them .',\n 'let me show you a slide here which illustrates the character of conflicts since 1946 until today .',\n 'you see the green is a traditional interstate conflict , the ones we used to read about .']"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_train_list[:10]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "pt_train_list = list()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "with open(data_dir + 'pt.train', 'r') as pt_train_file:\n",
    "    while True:\n",
    "        line = pt_train_file.readline().strip()\n",
    "        if not line:\n",
    "            break\n",
    "        pt_train_list.append(line)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "['entre todas as grandes privações com que nos debatemos hoje — pensamos em financeiras e económicas primeiro — aquela que mais me preocupa é a falta de diálogo político — a nossa capacidade de abordar conflitos modernos como eles são , de ir à raiz do que eles são e perceber os agentes-chave e lidar com eles .',\n 'nós que somos diplomatas , somos treinados para lidar com conflitos entre estados e problemas entre estados .',\n 'e posso dizer-vos , a nossa agenda está lotada .',\n 'há o comércio , o desarmamento , as relações inter-fronteiras .',\n 'mas o cenário está a mudar , e estamos a ver que há novos agentes-chave a surgirem .',\n \"`` nós chamamos-lhes , de forma vaga , `` '' grupos '' '' . podem representar realidades sociais , religiosas , políticas , económicas ou militares . ''\",\n 'e debatemo-nos sobre como lidar com elas .',\n 'as regras de interacção : como falar , quando falar e como lidar com elas .',\n 'deixem-me mostrar-vos aqui um diapositivos que ilustra o carácter dos conflitos desde 1946 até hoje .',\n 'o verde é um conflito tradicional inter-estado , aqueles que costumávamos ler nos livros .']"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt_train_list[:10]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51785,)\n"
     ]
    }
   ],
   "source": [
    "print(np.array(en_train_list).shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(10,), dtype=string, numpy=\narray([b\"amongst all the troubling deficits we struggle with today \\xe2\\x80\\x94 we think of financial and economic primarily \\xe2\\x80\\x94 the ones that concern me most is the deficit of political dialogue \\xe2\\x80\\x94 our ability to address modern conflicts as they are , to go to the source of what they 're all about and to understand the key players and to deal with them .\",\n       b'we who are diplomats , we are trained to deal with conflicts between states and issues between states .',\n       b'and i can tell you , our agenda is full .',\n       b'there is trade , there is disarmament , there is cross-border relations .',\n       b'but the picture is changing , and we are seeing that there are new key players coming onto the scene .',\n       b\"`` we loosely call them `` '' groups . '' '' they may represent social , religious , political , economic , military realities . ''\",\n       b'and we struggle with how to deal with them .',\n       b'the rules of engagement : how to talk , when to talk , and how to deal with them .',\n       b'let me show you a slide here which illustrates the character of conflicts since 1946 until today .',\n       b'you see the green is a traditional interstate conflict , the ones we used to read about .'],\n      dtype=object)>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_en = tf.constant(np.array(en_train_list), dtype=tf.string)\n",
    "train_en[:10]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(10,), dtype=string, numpy=\narray([b'entre todas as grandes priva\\xc3\\xa7\\xc3\\xb5es com que nos debatemos hoje \\xe2\\x80\\x94 pensamos em financeiras e econ\\xc3\\xb3micas primeiro \\xe2\\x80\\x94 aquela que mais me preocupa \\xc3\\xa9 a falta de di\\xc3\\xa1logo pol\\xc3\\xadtico \\xe2\\x80\\x94 a nossa capacidade de abordar conflitos modernos como eles s\\xc3\\xa3o , de ir \\xc3\\xa0 raiz do que eles s\\xc3\\xa3o e perceber os agentes-chave e lidar com eles .',\n       b'n\\xc3\\xb3s que somos diplomatas , somos treinados para lidar com conflitos entre estados e problemas entre estados .',\n       b'e posso dizer-vos , a nossa agenda est\\xc3\\xa1 lotada .',\n       b'h\\xc3\\xa1 o com\\xc3\\xa9rcio , o desarmamento , as rela\\xc3\\xa7\\xc3\\xb5es inter-fronteiras .',\n       b'mas o cen\\xc3\\xa1rio est\\xc3\\xa1 a mudar , e estamos a ver que h\\xc3\\xa1 novos agentes-chave a surgirem .',\n       b\"`` n\\xc3\\xb3s chamamos-lhes , de forma vaga , `` '' grupos '' '' . podem representar realidades sociais , religiosas , pol\\xc3\\xadticas , econ\\xc3\\xb3micas ou militares . ''\",\n       b'e debatemo-nos sobre como lidar com elas .',\n       b'as regras de interac\\xc3\\xa7\\xc3\\xa3o : como falar , quando falar e como lidar com elas .',\n       b'deixem-me mostrar-vos aqui um diapositivos que ilustra o car\\xc3\\xa1cter dos conflitos desde 1946 at\\xc3\\xa9 hoje .',\n       b'o verde \\xc3\\xa9 um conflito tradicional inter-estado , aqueles que costum\\xc3\\xa1vamos ler nos livros .'],\n      dtype=object)>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pt = tf.constant(np.array(pt_train_list), dtype=tf.string)\n",
    "train_pt[:10]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "en_dataset = tf.data.TextLineDataset([data_dir+'en.train'])\n",
    "pt_dataset = tf.data.TextLineDataset([data_dir+'pt.train'])\n",
    "# 从文件中读取line生成dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 从所有数据中生成词汇表vocab\n",
    "bert_vocab_from_dataset.bert_vocab_from_dataset()方法"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# bert_vocab_from_dataset的参数\n",
    "bert_tokenizer_params=dict(lower_case=True)\n",
    "reserved_tokens=[\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n",
    "\n",
    "bert_vocab_args = dict(\n",
    "    # 词表大小\n",
    "    vocab_size = 8000,\n",
    "    # 特殊token\n",
    "    reserved_tokens=reserved_tokens,\n",
    "    # 参数\n",
    "    bert_tokenizer_params=bert_tokenizer_params,\n",
    "    # Arguments for `wordpiece_vocab.wordpiece_tokenizer_learner_lib.learn`\n",
    "    learn_params={},\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 39s, sys: 656 ms, total: 1min 40s\n",
      "Wall time: 1min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pt_vocab = bert_vocab.bert_vocab_from_dataset(\n",
    "    pt_dataset.batch(1000).prefetch(2),\n",
    "    **bert_vocab_args\n",
    ")\n",
    "# 生成了葡语的pt_vocab"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7765\n",
      "['[PAD]', '[UNK]', '[START]', '[END]', '!', '#', '$', '%', '&', \"'\"]\n",
      "['no', 'por', 'mais', 'na', 'eu', 'esta', 'muito', 'isso', 'isto', 'sao']\n"
     ]
    }
   ],
   "source": [
    "print(len(pt_vocab))\n",
    "print(pt_vocab[:10])\n",
    "print(pt_vocab[100:110])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "with open(data_dir+'pt_vocab.txt','w') as pt_vocab_file:\n",
    "    for each in pt_vocab:\n",
    "        print(each, file=pt_vocab_file)\n",
    "# 保存vocab词典"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 13s, sys: 344 ms, total: 1min 13s\n",
      "Wall time: 1min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "en_vocab = bert_vocab.bert_vocab_from_dataset(\n",
    "    en_dataset.batch(1000).prefetch(2),\n",
    "    **bert_vocab_args\n",
    ")\n",
    "# 生成了英语的pt_vocab"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7010\n",
      "['[PAD]', '[UNK]', '[START]', '[END]', '!', '#', '$', '%', '&', \"'\"]\n",
      "['as', 'all', 'at', 'one', 'people', 're', 'like', 'if', 'our', 'from']\n"
     ]
    }
   ],
   "source": [
    "print(len(en_vocab))\n",
    "print(en_vocab[:10])\n",
    "print(en_vocab[100:110])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "with open(data_dir+'en_vocab.txt','w') as en_vocab_file:\n",
    "    for each in en_vocab:\n",
    "        print(each, file=en_vocab_file)\n",
    "# 保存vocab词典"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 基于vocab生成tokenizer\n",
    "BertTokenizer方法 和tokenize_strings部分相同"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "pt_tokenizer = text.BertTokenizer(data_dir+'pt_vocab.txt', lower_case=True)\n",
    "en_tokenizer = text.BertTokenizer(data_dir+'en_vocab.txt', lower_case=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "\"amongst all the troubling deficits we struggle with today — we think of financial and economic primarily — the ones that concern me most is the deficit of political dialogue — our ability to address modern conflicts as they are , to go to the source of what they 're all about and to understand the key players and to deal with them .\""
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_train_list[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "test_en_tokenized = en_tokenizer.tokenize(en_train_list[0])\n",
    "# [batch, seq_len, token]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, None, None)\n",
      "(65, None)\n",
      "(1,)\n"
     ]
    }
   ],
   "source": [
    "print(test_en_tokenized.shape)\n",
    "print(test_en_tokenized[0].shape)\n",
    "print(test_en_tokenized[0][0].shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(71,), dtype=int64, numpy=\narray([2568,  101,   71,   56, 1548, 4593, 2159, 6437, 2364,   78, 2003,\n         93,  208,   67,   78,  133,   74, 1332,   72,  638, 4039,   67,\n         71,  615,   75, 3458,  114,  190,   80,   71, 2159, 6437,  893,\n         74,  730, 2654,   67,  108,  859,   73, 1510,  832, 2725,  100,\n         83,   86,   13,   73,  164,   73,   71,  948,   74,   90,   83,\n          9,  105,  101,   95,   72,   73,  281,   71,  679, 6258,   72,\n         73,  725,   93,  124,   15])>"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_en_tokenized[0].values\n",
    "# 拉直成tensor进行处理"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(71,), dtype=string, numpy=\narray([b'amongst', b'all', b'the', b't', b'##ro', b'##ubling', b'de',\n       b'##fic', b'##its', b'we', b'struggle', b'with', b'today',\n       b'\\xe2\\x80\\x94', b'we', b'think', b'of', b'financial', b'and',\n       b'economic', b'primarily', b'\\xe2\\x80\\x94', b'the', b'ones',\n       b'that', b'concern', b'me', b'most', b'is', b'the', b'de',\n       b'##fic', b'##it', b'of', b'political', b'dialogue',\n       b'\\xe2\\x80\\x94', b'our', b'ability', b'to', b'address', b'modern',\n       b'conflicts', b'as', b'they', b'are', b',', b'to', b'go', b'to',\n       b'the', b'source', b'of', b'what', b'they', b\"'\", b're', b'all',\n       b'about', b'and', b'to', b'understand', b'the', b'key', b'players',\n       b'and', b'to', b'deal', b'with', b'them', b'.'], dtype=object)>"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_tokens = tf.gather(en_vocab,test_en_tokenized[0].values)\n",
    "text_tokens\n",
    "# tf.gather进行映射 对应回sub-word"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw sentence:  amongst all the troubling deficits we struggle with today — we think of financial and economic primarily — the ones that concern me most is the deficit of political dialogue — our ability to address modern conflicts as they are , to go to the source of what they 're all about and to understand the key players and to deal with them .\n",
      "after tokenize:  b\"amongst all the t ##ro ##ubling de ##fic ##its we struggle with today \\xe2\\x80\\x94 we think of financial and economic primarily \\xe2\\x80\\x94 the ones that concern me most is the de ##fic ##it of political dialogue \\xe2\\x80\\x94 our ability to address modern conflicts as they are , to go to the source of what they ' re all about and to understand the key players and to deal with them .\"\n"
     ]
    }
   ],
   "source": [
    "print('raw sentence: ', en_train_list[0])\n",
    "print('after tokenize: ',tf.strings.reduce_join(text_tokens, separator=' ', axis=-1).numpy())\n",
    "# 值得注意的是 tokenize之后 troubling -> t ro ubling 很多词都被分隔了"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(65, None)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<tf.RaggedTensor [[b'amongst'],\n [b'all'],\n [b'the'],\n [b'troubling'],\n [b'deficits'],\n [b'we'],\n [b'struggle'],\n [b'with'],\n [b'today'],\n [b'\\xe2\\x80\\x94'],\n [b'we'],\n [b'think'],\n [b'of'],\n [b'financial'],\n [b'and'],\n [b'economic'],\n [b'primarily'],\n [b'\\xe2\\x80\\x94'],\n [b'the'],\n [b'ones'],\n [b'that'],\n [b'concern'],\n [b'me'],\n [b'most'],\n [b'is'],\n [b'the'],\n [b'deficit'],\n [b'of'],\n [b'political'],\n [b'dialogue'],\n [b'\\xe2\\x80\\x94'],\n [b'our'],\n [b'ability'],\n [b'to'],\n [b'address'],\n [b'modern'],\n [b'conflicts'],\n [b'as'],\n [b'they'],\n [b'are'],\n [b','],\n [b'to'],\n [b'go'],\n [b'to'],\n [b'the'],\n [b'source'],\n [b'of'],\n [b'what'],\n [b'they'],\n [b\"'\"],\n [b're'],\n [b'all'],\n [b'about'],\n [b'and'],\n [b'to'],\n [b'understand'],\n [b'the'],\n [b'key'],\n [b'players'],\n [b'and'],\n [b'to'],\n [b'deal'],\n [b'with'],\n [b'them'],\n [b'.']]>"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = en_tokenizer.detokenize(test_en_tokenized[0])\n",
    "print(words.shape)\n",
    "words\n",
    "# detokenize进行还原"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "words_ = tf.strings.reduce_join(words, separator=' ', axis=-1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(), dtype=string, numpy=b\"amongst all the troubling deficits we struggle with today \\xe2\\x80\\x94 we think of financial and economic primarily \\xe2\\x80\\x94 the ones that concern me most is the deficit of political dialogue \\xe2\\x80\\x94 our ability to address modern conflicts as they are , to go to the source of what they ' re all about and to understand the key players and to deal with them .\">"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.strings.reduce_join(words_, separator=' ', axis=-1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 自定义detokenization\n",
    "在上面那个detokenize部分看到 detokenize之后似乎不是很好理解 可以对其进行额外操作 有助于后续任务\n",
    "1. clean text 清除[START] [PAD] [END]等token 因为这些在下游任务没有\n",
    "2. 将一个一个token拼接起来 用空格隔开 join"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "def cleanup_text(token_txt, reserved_tokens=None):\n",
    "    \"\"\"\n",
    "    丢弃保留词 并拉直\n",
    "    :param reserved_tokens: 保留此表\n",
    "    :param token_txt: 待处理文本\n",
    "    :return: 处理后的文本\n",
    "    \"\"\"\n",
    "    if reserved_tokens is None:\n",
    "        reserved_tokens = [\"[PAD]\", \"[START]\", \"[END]\"]\n",
    "    bad_token_re = \"|\".join(reserved_tokens)\n",
    "    # 丢弃词的正则表达式\n",
    "\n",
    "    bad_cells = tf.strings.regex_full_match(token_txt, bad_token_re)\n",
    "    # 得到boolean值的list True位置是符合正则表达的\n",
    "    result = tf.ragged.boolean_mask(token_txt, ~bad_cells)\n",
    "    # 对bad_cell取非 然后使用boolean_mask进行丢弃\n",
    "    result = tf.strings.reduce_join(result, separator=' ', axis=-1)\n",
    "    # 使用reduce_join进行拼接\n",
    "    return  result"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b\"amongst all the troubling deficits we struggle with today \\xe2\\x80\\x94 we think of financial and economic primarily \\xe2\\x80\\x94 the ones that concern me most is the deficit of political dialogue \\xe2\\x80\\x94 our ability to address modern conflicts as they are , to go to the source of what they 're all about and to understand the key players and to deal with them .\"\n",
      " b'we who are diplomats , we are trained to deal with conflicts between states and issues between states .'\n",
      " b'and i can tell you , our agenda is full .'], shape=(3,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "en_examples = None\n",
    "for en_examples in en_dataset.batch(3).take(1):\n",
    "    print(en_examples)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b\"amongst all the troubling deficits we struggle with today \\xe2\\x80\\x94 we think of financial and economic primarily \\xe2\\x80\\x94 the ones that concern me most is the deficit of political dialogue \\xe2\\x80\\x94 our ability to address modern conflicts as they are , to go to the source of what they 're all about and to understand the key players and to deal with them .\"\n",
      " b'we who are diplomats , we are trained to deal with conflicts between states and issues between states .'\n",
      " b'and i can tell you , our agenda is full .']\n"
     ]
    }
   ],
   "source": [
    "print(en_examples.numpy())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, None, None)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<tf.RaggedTensor [[[2568], [101], [71], [56, 1548, 4593], [2159, 6437, 2364], [78], [2003],\n  [93], [208], [67], [78], [133], [74], [1332], [72], [638], [4039], [67],\n  [71], [615], [75], [3458], [114], [190], [80], [71], [2159, 6437, 893],\n  [74], [730], [2654], [67], [108], [859], [73], [1510], [832], [2725],\n  [100], [83], [86], [13], [73], [164], [73], [71], [948], [74], [90], [83],\n  [9], [105], [101], [95], [72], [73], [281], [71], [679], [6258], [72],\n  [73], [725], [93], [124], [15]]                                           ,\n [[78], [136], [86], [40, 2423, 2425, 1361, 803], [13], [78], [86], [1876],\n  [73], [725], [93], [2725], [284], [451], [72], [1127], [284], [451], [15]],\n [[72], [45], [94], [224], [79], [13], [108], [3432], [80], [636], [15]]]>"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_batch = en_tokenizer.tokenize(en_examples)\n",
    "print(token_batch.shape)\n",
    "token_batch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, None)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<tf.RaggedTensor [[2568, 101, 71, 56, 1548, 4593, 2159, 6437, 2364, 78, 2003, 93, 208, 67,\n  78, 133, 74, 1332, 72, 638, 4039, 67, 71, 615, 75, 3458, 114, 190, 80, 71,\n  2159, 6437, 893, 74, 730, 2654, 67, 108, 859, 73, 1510, 832, 2725, 100,\n  83, 86, 13, 73, 164, 73, 71, 948, 74, 90, 83, 9, 105, 101, 95, 72, 73,\n  281, 71, 679, 6258, 72, 73, 725, 93, 124, 15]                             ,\n [78, 136, 86, 40, 2423, 2425, 1361, 803, 13, 78, 86, 1876, 73, 725, 93,\n  2725, 284, 451, 72, 1127, 284, 451, 15]                               ,\n [72, 45, 94, 224, 79, 13, 108, 3432, 80, 636, 15]]>"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 由于最后一维没有用，拼接起来\n",
    "token_batch = token_batch.merge_dims(-2,-1)\n",
    "# merge_dims将倒数第二维和倒数第一维拼起来\n",
    "print(token_batch.shape)\n",
    "token_batch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.RaggedTensor [[b'amongst', b'all', b'the', b'troubling', b'deficits', b'we', b'struggle',\n  b'with', b'today', b'\\xe2\\x80\\x94', b'we', b'think', b'of', b'financial',\n  b'and', b'economic', b'primarily', b'\\xe2\\x80\\x94', b'the', b'ones',\n  b'that', b'concern', b'me', b'most', b'is', b'the', b'deficit', b'of',\n  b'political', b'dialogue', b'\\xe2\\x80\\x94', b'our', b'ability', b'to',\n  b'address', b'modern', b'conflicts', b'as', b'they', b'are', b',', b'to',\n  b'go', b'to', b'the', b'source', b'of', b'what', b'they', b\"'\", b're',\n  b'all', b'about', b'and', b'to', b'understand', b'the', b'key',\n  b'players', b'and', b'to', b'deal', b'with', b'them', b'.']               ,\n [b'we', b'who', b'are', b'diplomats', b',', b'we', b'are', b'trained',\n  b'to', b'deal', b'with', b'conflicts', b'between', b'states', b'and',\n  b'issues', b'between', b'states', b'.']                              ,\n [b'and', b'i', b'can', b'tell', b'you', b',', b'our', b'agenda', b'is',\n  b'full', b'.']                                                        ]>"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = en_tokenizer.detokenize(token_batch)\n",
    "words\n",
    "# detokenize的结果 假设这就是一个翻译的结果 现在要进行clean处理"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b\"amongst all the troubling deficits we struggle with today \\xe2\\x80\\x94 we think of financial and economic primarily \\xe2\\x80\\x94 the ones that concern me most is the deficit of political dialogue \\xe2\\x80\\x94 our ability to address modern conflicts as they are , to go to the source of what they ' re all about and to understand the key players and to deal with them .\"\n",
      " b'we who are diplomats , we are trained to deal with conflicts between states and issues between states .'\n",
      " b'and i can tell you , our agenda is full .']\n"
     ]
    }
   ],
   "source": [
    "print(cleanup_text(words).numpy())\n",
    "# 使用cleanup_text进行处理 可见还是很有效果的 变成了可读性很高的结果"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# 自定义一个customTokenizer实现\n",
    "\n",
    "class CustomTokenizer(tf.Module):\n",
    "  def __init__(self, reserved_tokens, vocab_path):\n",
    "    super(CustomTokenizer,self).__init__()\n",
    "    self.tokenizer = text.BertTokenizer(vocab_path, lower_case=True)\n",
    "    # 初始化一个BertTokenizer\n",
    "    self._reserved_tokens = reserved_tokens\n",
    "    # 保存保留词\n",
    "    self._vocab_path = tf.saved_model.Asset(vocab_path)\n",
    "    # vocab_path\n",
    "\n",
    "    vocab = pathlib.Path(vocab_path).read_text().splitlines()\n",
    "    self.vocab = tf.Variable(vocab)\n",
    "    # vocab的tf对象\n",
    "\n",
    "    ## Create the signatures for export:\n",
    "\n",
    "    # Include a tokenize signature for a batch of strings.\n",
    "    self.tokenize.get_concrete_function(\n",
    "        tf.TensorSpec(shape=[None], dtype=tf.string))\n",
    "\n",
    "    # Include `detokenize` and `lookup` signatures for:\n",
    "    #   * `Tensors` with shapes [tokens] and [batch, tokens]\n",
    "    #   * `RaggedTensors` with shape [batch, tokens]\n",
    "    self.detokenize.get_concrete_function(\n",
    "        tf.TensorSpec(shape=[None, None], dtype=tf.int64))\n",
    "    self.detokenize.get_concrete_function(\n",
    "          tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))\n",
    "\n",
    "    self.lookup.get_concrete_function(\n",
    "        tf.TensorSpec(shape=[None, None], dtype=tf.int64))\n",
    "    self.lookup.get_concrete_function(\n",
    "          tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))\n",
    "\n",
    "    # These `get_*` methods take no arguments\n",
    "    self.get_vocab_size.get_concrete_function()\n",
    "    self.get_vocab_path.get_concrete_function()\n",
    "    self.get_reserved_tokens.get_concrete_function()\n",
    "\n",
    "  @tf.function\n",
    "  def tokenize(self, strings):\n",
    "    enc = self.tokenizer.tokenize(strings)\n",
    "    # 先使用bertTokenizer完成\n",
    "    # Merge the `word` and `word-piece` axes.\n",
    "    enc = enc.merge_dims(-2,-1)\n",
    "    # 由于bertTokenizer处理过后是单个token最后一维没有意义 合并一下\n",
    "    # enc = add_start_end(enc)\n",
    "    # 加上start和end\n",
    "    return enc\n",
    "\n",
    "  @tf.function\n",
    "  def detokenize(self, tokenized):\n",
    "    words = self.tokenizer.detokenize(tokenized)\n",
    "    # 先使用bert解码\n",
    "    return cleanup_text(self._reserved_tokens, words)\n",
    "    # 使用cleanup_text函数后续处理\n",
    "\n",
    "  @tf.function\n",
    "  def lookup(self, token_ids):\n",
    "    return tf.gather(self.vocab, token_ids)\n",
    "    # 找到token_ids对应的文字\n",
    "\n",
    "  @tf.function\n",
    "  def get_vocab_size(self):\n",
    "    return tf.shape(self.vocab)[0]\n",
    "\n",
    "  @tf.function\n",
    "  def get_vocab_path(self):\n",
    "    return self._vocab_path\n",
    "\n",
    "  @tf.function\n",
    "  def get_reserved_tokens(self):\n",
    "    return tf.constant(self._reserved_tokens)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 40,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokenizers = tf.Module()\n",
    "tokenizers.pt = CustomTokenizer(reserved_tokens, data_dir+'pt_vocab.txt')\n",
    "tokenizers.en = CustomTokenizer(reserved_tokens, data_dir+'en_vocab.txt')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 保存模型\n",
    "model_name = 'tensorflow_study/tensorflow-text/model_dir/ted_hrlr_translate_pt_en_converter'\n",
    "tf.saved_model.save(tokenizers, model_name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#加载模型\n",
    "reloaded_tokenizers = tf.saved_model.load(model_name)\n",
    "reloaded_tokenizers.en.get_vocab_size().numpy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokens = reloaded_tokenizers.en.tokenize(['Hello TensorFlow!'])\n",
    "tokens.numpy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 自定义lookup对照表\n",
    "tf.lookup"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "pt_lookup = tf.lookup.StaticVocabularyTable(\n",
    "    num_oov_buckets=1,\n",
    "    initializer=tf.lookup.TextFileInitializer(\n",
    "        filename=data_dir+'pt_vocab.txt',\n",
    "        key_dtype=tf.string,\n",
    "        key_index = tf.lookup.TextFileIndex.WHOLE_LINE,\n",
    "        value_dtype = tf.int64,\n",
    "        value_index=tf.lookup.TextFileIndex.LINE_NUMBER))\n",
    "pt_tokenizer = text.BertTokenizer(pt_lookup)\n",
    "# 使用tf.lookup.StaticVocabularyTable创建一个vocab table进行lookup\n",
    "# 这个方式是使用vocab文件进行创建"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(5,), dtype=int64, numpy=array([7765,   85,   86,   87, 7765])>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt_lookup.lookup(tf.constant(['é', 'um', 'uma', 'para', 'não']))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pt_vocab' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-6-5ff1b4bf98ff>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      2\u001B[0m     \u001B[0mnum_oov_buckets\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m     initializer=tf.lookup.KeyValueTensorInitializer(\n\u001B[0;32m----> 4\u001B[0;31m         \u001B[0mkeys\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mpt_vocab\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      5\u001B[0m         values=tf.range(len(pt_vocab), dtype=tf.int64))) \n\u001B[1;32m      6\u001B[0m \u001B[0mpt_tokenizer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mBertTokenizer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpt_lookup\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'pt_vocab' is not defined"
     ]
    }
   ],
   "source": [
    "pt_lookup = tf.lookup.StaticVocabularyTable(\n",
    "    num_oov_buckets=1,\n",
    "    initializer=tf.lookup.KeyValueTensorInitializer(\n",
    "        keys=pt_vocab,\n",
    "        values=tf.range(len(pt_vocab), dtype=tf.int64)))\n",
    "pt_tokenizer = text.BertTokenizer(pt_lookup)\n",
    "# 这个是使用内存里的变量进行创建"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}