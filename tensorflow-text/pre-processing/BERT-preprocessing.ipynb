{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Text preprocessing to BERT model using TF.Text\n",
    "通过TF.Text的API奖文本进行preprocessing处理 变为整型向量输入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_text as text\n",
    "import functools\n",
    "import os\n",
    "import tensorflow.keras as keras"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "tf.get_logger().setLevel('ERROR')\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1' # 使用 GPU 1\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "physical_devices = tf.config.list_physical_devices(\"GPU\")\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0],True)\n",
    "logical_devices = tf.config.list_logical_devices(\"GPU\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 创建一个demo dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "{'text_a': <tf.Tensor: shape=(), dtype=string, numpy=b'Sponge bob Squarepants is an Avenger'>,\n 'text_b': <tf.Tensor: shape=(), dtype=string, numpy=b'Barack Obama is the President.'>}"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 注意这个examples的目的是要变成BERT训练的样子 那么就是text_a[0]和text_b[0] 拼接作为输入 text_a[1]和text_b[1]作为输入\n",
    "examples = {\n",
    "    \"text_a\": [\n",
    "      \"Sponge bob Squarepants is an Avenger\",\n",
    "      \"Marvel Avengers\"\n",
    "    ],\n",
    "    \"text_b\": [\n",
    "     \"Barack Obama is the President.\",\n",
    "     \"President is the highest office\"\n",
    "  ],\n",
    "}\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(examples)\n",
    "next(iter(dataset))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tokenizing\n",
    "Tokenizing的方法可以使用tokenize_strings内的多种方法，最直接的就是用BertTokenizer 可以自动地将sentence -> subwords/wordpieces\n",
    "BertTokenizer的初始化需要一个vocab文件 这个可以下载 在这里尝试一个创建一个toy vocabulary"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "_VOCAB = [\n",
    "    # Special tokens 特殊token\n",
    "    b\"[UNK]\", b\"[MASK]\", b\"[RANDOM]\", b\"[CLS]\", b\"[SEP]\",\n",
    "    # Suffixes 词根\n",
    "    b\"##ack\", b\"##ama\", b\"##ger\", b\"##gers\", b\"##onge\", b\"##pants\",  b\"##uare\",\n",
    "    b\"##vel\", b\"##ven\", b\"an\", b\"A\", b\"Bar\", b\"Hates\", b\"Mar\", b\"Ob\",\n",
    "    b\"Patrick\", b\"President\", b\"Sp\", b\"Sq\", b\"bob\", b\"box\", b\"has\", b\"highest\",\n",
    "    b\"is\", b\"office\", b\"the\",\n",
    "]\n",
    "_VOCAB_SIZE = len(_VOCAB)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "_START_TOKEN = _VOCAB.index(b\"[CLS]\")\n",
    "_END_TOKEN = _VOCAB.index(b\"[SEP]\")\n",
    "_MASK_TOKEN = _VOCAB.index(b\"[MASK]\")\n",
    "_RANDOM_TOKEN = _VOCAB.index(b\"[RANDOM]\")\n",
    "_UNK_TOKEN = _VOCAB.index(b\"[UNK]\")\n",
    "_MAX_SEQ_LEN = 8\n",
    "_MAX_PREDICTIONS_PER_BATCH = 5"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(31,), dtype=int64, numpy=\narray([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30])>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.range(tf.size(_VOCAB, out_type=tf.int64),dtype=tf.int64,)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "lookup_table = tf.lookup.StaticVocabularyTable(\n",
    "    tf.lookup.KeyValueTensorInitializer(\n",
    "        keys = _VOCAB, # keys就是对应表的key 由string映射到int 那么key就是string\n",
    "        key_dtype=tf.string,\n",
    "        values=tf.range(\n",
    "            tf.size(_VOCAB, out_type=tf.int64), # values就是对应表的value 是int值 用0->VOCAB的长度表示\n",
    "            dtype=tf.int64,\n",
    "        ),\n",
    "        value_dtype=tf.int64,\n",
    "    ),\n",
    "    num_oov_buckets = 1\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.RaggedTensor [[[b'Sp', b'##onge'], [b'bob'], [b'Sq', b'##uare', b'##pants'], [b'is'],\n  [b'an'], [b'A', b'##ven', b'##ger']]                                  ,\n [[b'Mar', b'##vel'], [b'A', b'##ven', b'##gers']]]>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokenizer_str = text.BertTokenizer(lookup_table, token_out_type=tf.string)\n",
    "bert_tokenizer_str.tokenize(examples[\"text_a\"])\n",
    "# 用lookup对象进行BertTokenizer的初始化 设置输出type是string\n",
    "# 可以看到tokenizer将输入进行了分割"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "['Sponge bob Squarepants is an Avenger', 'Marvel Avengers']"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples['text_a']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.RaggedTensor [[[22, 9], [24], [23, 11, 10], [28], [14], [15, 13, 7]],\n [[18, 12], [15, 13, 8]]]>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokenizer_int = text.BertTokenizer(lookup_table, token_out_type=tf.int64)\n",
    "bert_tokenizer_int.tokenize(examples['text_a'])\n",
    "# 设置输出为int\n",
    "# 可以看到对输入进行了分割"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.RaggedTensor [[[22, 9], [24], [23, 11, 10], [28], [14], [15, 13, 7]],\n",
      " [[18, 12], [15, 13, 8]]]>\n",
      "<tf.RaggedTensor [[[16, 5], [19, 6], [28], [30], [21], [0]], [[21], [28], [30], [27], [29]]]>\n"
     ]
    }
   ],
   "source": [
    "segment_a_pieces = bert_tokenizer_int.tokenize(examples['text_a'])\n",
    "segment_b_pieces = bert_tokenizer_int.tokenize(examples['text_b'])\n",
    "print(segment_a_pieces)\n",
    "print(segment_b_pieces)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.RaggedTensor [[22, 9, 24, 23, 11, 10, 28, 14, 15, 13, 7], [18, 12, 15, 13, 8]]>\n",
      "<tf.RaggedTensor [[16, 5, 19, 6, 28, 30, 21, 0], [21, 28, 30, 27, 29]]>\n"
     ]
    }
   ],
   "source": [
    "segment_a = segment_a_pieces.merge_dims(-2, -1)\n",
    "segment_b = segment_b_pieces.merge_dims(-2, -1)\n",
    "print(segment_a)\n",
    "print(segment_b)\n",
    "# 使用merge_dims将最后两维合并，因为不论是不是sub-word都不重要 还原为一个sentence的形式更加重要"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 长度裁剪\n",
    "BERT论文里输入的是两个句子的连接，两个句子的长度和的上限是固定的 所以长于这个的上限要被裁减掉\n",
    "使用text.Trimmer进行裁剪\n",
    "text.RoundRobinTrimmer是为每一段分配平均份额 有可能裁剪句子结尾\n",
    "text.WaterfallTrimmer从最后一个句子的末尾开始裁剪\n",
    "注意trimmer方法是在多个数据间最后一个轴上进行的裁剪"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "data": {
      "text/plain": "[<tf.RaggedTensor [[22, 9, 24],\n  [18, 12, 15]]>,\n <tf.RaggedTensor [[16, 5, 19],\n  [21, 28, 30]]>,\n <tf.RaggedTensor [[22, 9],\n  [18, 12]]>]"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 这个例子中裁剪的维度是最后一维 输入是[batch = 3, sentence_count = 2, sub_word_count = ?]\n",
    "# 那么就是在一个sentence_count维度上 多个batch的元素的和为max_seq_length 即三个裁剪后在sentence_count=1位置和为9等\n",
    "trimmer_test = text.RoundRobinTrimmer(max_seq_length=8)\n",
    "trimmed_test = trimmer_test.trim([segment_a,segment_b, segment_a])\n",
    "trimmed_test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "data": {
      "text/plain": "[<tf.RaggedTensor [[[22, 9], [24], [23, 11, 10], [28], [14], [15]], [[18, 12], [15, 13, 8]]]>]"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trimmer_pieces = text.RoundRobinTrimmer(max_seq_length=9)\n",
    "trimmed_pieces = trimmer_pieces.trim([segment_a_pieces])\n",
    "trimmed_pieces"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "data": {
      "text/plain": "[<tf.RaggedTensor [[22, 9, 24, 23],\n  [18, 12, 15, 13]]>,\n <tf.RaggedTensor [[16, 5, 19, 6],\n  [21, 28, 30, 27]]>]"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trimmer = text.RoundRobinTrimmer(max_seq_length=_MAX_SEQ_LEN)\n",
    "trimmed = trimmer.trim([segment_a, segment_b])\n",
    "trimmed"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## combine segment\n",
    "使用combine_segments方法将裁剪后的在seq_len相同的维度拼接起来 并加上SOS EOS(SEP) 并且得到句子的ids\n",
    "这一步其实就是BERT人物的Next Sentence Prediction"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "data": {
      "text/plain": "(<tf.RaggedTensor [[3, 22, 9, 24, 23, 4, 16, 5, 19, 6, 4],\n  [3, 18, 12, 15, 13, 4, 21, 28, 30, 27, 4]]>,\n <tf.RaggedTensor [[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n  [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]]>)"
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segments_combined, segment_ids = text.combine_segments(\n",
    "    trimmed,\n",
    "    start_of_sequence_id=_START_TOKEN,\n",
    "    end_of_segment_id=_END_TOKEN\n",
    ")\n",
    "segments_combined, segment_ids\n",
    "# [batch, seq_len] 这个seq_len = SOS + sentence1 + EOS + sentence2 + EOS sentence1 + sentence2 < MAX_LEN"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## MLM Masked Language Model\n",
    "1. 选择被mask的tokens item selection text.RandomItemSelector\n",
    "2. 将被mask的tokens选择处理方法 [MASK] [RANDOM]等 Choosing the Masked Value text.MaskValuesChooser\n",
    "3. 使用RandomItemSelector和MaskValueChooser 生成mask_language_model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "# item selection 随机选择被mask的tokens\n",
    "random_selector = text.RandomItemSelector(\n",
    "    max_selections_per_batch=_MAX_PREDICTIONS_PER_BATCH,   # 选择最大的选择长度 就是最多mask多少个tokens\n",
    "    selection_rate=0.2,                                     # 选择mask的比例\n",
    "    unselectable_ids=[_START_TOKEN, _END_TOKEN, _UNK_TOKEN] # 选择不能被mask的特殊tokens\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "selected = random_selector.get_selection_mask(\n",
    "    segments_combined,\n",
    "    axis=1\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.RaggedTensor [[False, False, False, False, False, False, True, False, True, False,\n  False],\n [False, True, False, False, False, False, False, False, False, True,\n  False]]>"
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected\n",
    "# True就是要MASK处理的部分"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 选择mask的tokens的处理方法 [MASK] random_word unchanged\n",
    "# MaskValuesChooser  这个的处理逻辑是按照0.8的概率变成MASK->1 其他0.2的概率变成要么random 要么保留"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.RaggedTensor [[1, 1, 6, 1, 1, 1, 1, 1, 1, 16, 1],\n [29, 1, 1, 1, 13, 1, 21, 28, 1, 1, 1]]>"
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_values_chooser = text.MaskValuesChooser(_VOCAB_SIZE, _MASK_TOKEN, 0.8)\n",
    "mask_values_chooser.get_mask_values(segments_combined)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "masked_token_ids, masked_pos, masked_lm_ids = text.mask_language_model(\n",
    "  segments_combined,\n",
    "  item_selector=random_selector, mask_values_chooser=mask_values_chooser)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw:  <tf.RaggedTensor [[3, 22, 9, 24, 23, 4, 16, 5, 19, 6, 4],\n",
      " [3, 18, 12, 15, 13, 4, 21, 28, 30, 27, 4]]>\n",
      "after mask <tf.RaggedTensor [[3, 22, 1, 25, 23, 4, 16, 5, 19, 6, 4],\n",
      " [3, 18, 12, 15, 13, 4, 1, 28, 30, 0, 4]]>\n"
     ]
    }
   ],
   "source": [
    "print('raw: ', segments_combined)\n",
    "print('after mask',masked_token_ids)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.RaggedTensor [[b'[CLS]', b'Sp', b'[MASK]', b'box', b'Sq', b'[SEP]', b'Bar', b'##ack',\n  b'Ob', b'##ama', b'[SEP]'],\n [b'[CLS]', b'Mar', b'##vel', b'A', b'##ven', b'[SEP]', b'[MASK]', b'is',\n  b'the', b'[UNK]', b'[SEP]']]>"
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "after_mask_ragged = tf.gather(_VOCAB, masked_token_ids)\n",
    "after_mask_ragged\n",
    "# 按照MASK之后进行VOCAB的对照"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw sentence: \n",
      "Sponge bob Squarepants is an Avenger Barack Obama is the President.\n",
      "Marvel Avengers President is the highest office\n",
      "masked language model sentence: \n",
      "b'[CLS] Sp [MASK] box Sq [SEP] Bar ##ack Ob ##ama [SEP]'\n",
      "b'[CLS] Mar ##vel A ##ven [SEP] [MASK] is the [UNK] [SEP]'\n"
     ]
    }
   ],
   "source": [
    "print('raw sentence: ')\n",
    "print(examples['text_a'][0] + ' ' + examples['text_b'][0])\n",
    "print(examples['text_a'][1] + ' ' + examples['text_b'][1])\n",
    "after_mask_string = tf.strings.reduce_join(after_mask_ragged,axis=-1,separator=' ')\n",
    "print('masked language model sentence: ')\n",
    "print(after_mask_string.numpy()[0])\n",
    "print(after_mask_string.numpy()[1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.RaggedTensor [[2, 3],\n [6, 9]]>"
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_pos\n",
    "# 这是masked的位置"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.RaggedTensor [[9, 24],\n [21, 27]]>"
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_lm_ids\n",
    "# masked ids 作为y_true"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[b'##onge' b'bob']\n",
      " [b'President' b'highest']]\n"
     ]
    }
   ],
   "source": [
    "print(tf.gather(_VOCAB, masked_lm_ids).numpy())\n",
    "# masked对应回tokens"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## padding\n",
    "将句子padding到固定的长度变成Tensor而非RaggedTensor\n",
    "text.pad_model_inputs方法"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "masked_token_ids:  <tf.RaggedTensor [[3, 22, 1, 25, 23, 4, 16, 5, 19, 6, 4],\n",
      " [3, 18, 12, 15, 13, 4, 1, 28, 30, 0, 4]]>\n",
      "input_word_ids:  tf.Tensor(\n",
      "[[ 3 22  1 25 23  4 16  5 19  6  4  0  0  0  0  0  0  0  0  0]\n",
      " [ 3 18 12 15 13  4  1 28 30  0  4  0  0  0  0  0  0  0  0  0]], shape=(2, 20), dtype=int64)\n",
      "input_mask:  tf.Tensor(\n",
      "[[1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0]], shape=(2, 20), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# pad masked_input\n",
    "NEW_MAK_SEQ_LEN = 20\n",
    "print('masked_token_ids: ', masked_token_ids)\n",
    "input_word_ids, input_mask = text.pad_model_inputs(\n",
    "    input=masked_token_ids,\n",
    "    max_seq_length=NEW_MAK_SEQ_LEN,\n",
    "    pad_value=0\n",
    ")\n",
    "print('input_word_ids: ', input_word_ids)\n",
    "print('input_mask: ', input_mask)\n",
    "# 这个本质上就是进行后面加0 如果句子长度比MAX_SEQ_LEN长的话还可以进行截断"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "segments_ids:  <tf.RaggedTensor [[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n",
      " [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]]>\n",
      "input_type_ids:  tf.Tensor(\n",
      "[[0 0 0 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0]], shape=(2, 20), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# pad segment 即区分两个句子的向量 生成input_type_ids\n",
    "print('segments_ids: ', segment_ids)\n",
    "input_type_ids, _ = text.pad_model_inputs(\n",
    "    input = segment_ids,\n",
    "    max_seq_length=NEW_MAK_SEQ_LEN\n",
    ")\n",
    "print('input_type_ids: ', input_type_ids)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "masked_position:  <tf.RaggedTensor [[2, 3],\n",
      " [6, 9]]>\n",
      "masked_lm_positions:  tf.Tensor(\n",
      "[[2 3 0 0 0]\n",
      " [6 9 0 0 0]], shape=(2, 5), dtype=int64)\n",
      "masked_lm_weights:  tf.Tensor(\n",
      "[[1 1 0 0 0]\n",
      " [1 1 0 0 0]], shape=(2, 5), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# 对输出进行pad 即对真实的结果进行pad\n",
    "# 对masked position进行pad 第二维是需要的 因为0代表不用预测\n",
    "print('masked_position: ', masked_pos)\n",
    "masked_lm_positions, masked_lm_weights = text.pad_model_inputs(\n",
    "    input = masked_pos,\n",
    "    max_seq_length=_MAX_PREDICTIONS_PER_BATCH\n",
    ")\n",
    "print('masked_lm_positions: ', masked_lm_positions)\n",
    "print('masked_lm_weights: ', masked_lm_weights)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "masked_ids:  <tf.RaggedTensor [[9, 24],\n",
      " [21, 27]]>\n",
      "masked_lm_ids:  tf.Tensor(\n",
      "[[ 9 24  0  0  0]\n",
      " [21 27  0  0  0]], shape=(2, 5), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# 对被masked的tokens的真实id进行pad\n",
    "print('masked_ids: ', masked_lm_ids)\n",
    "masked_lm_ids, _ = text.pad_model_inputs(\n",
    "    input=masked_lm_ids,\n",
    "    max_seq_length=_MAX_PREDICTIONS_PER_BATCH\n",
    ")\n",
    "print('masked_lm_ids: ', masked_lm_ids)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [
    {
     "data": {
      "text/plain": "{'input_word_ids': <tf.Tensor: shape=(2, 20), dtype=int64, numpy=\n array([[ 3, 22,  1, 25, 23,  4, 16,  5, 19,  6,  4,  0,  0,  0,  0,  0,\n          0,  0,  0,  0],\n        [ 3, 18, 12, 15, 13,  4,  1, 28, 30,  0,  4,  0,  0,  0,  0,  0,\n          0,  0,  0,  0]])>,\n 'input_mask': <tf.Tensor: shape=(2, 20), dtype=int64, numpy=\n array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]])>,\n 'input_type_ids': <tf.Tensor: shape=(2, 20), dtype=int64, numpy=\n array([[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]])>,\n 'masked_lm_ids': <tf.Tensor: shape=(2, 5), dtype=int64, numpy=\n array([[ 9, 24,  0,  0,  0],\n        [21, 27,  0,  0,  0]])>,\n 'masked_lm_positions': <tf.Tensor: shape=(2, 5), dtype=int64, numpy=\n array([[2, 3, 0, 0, 0],\n        [6, 9, 0, 0, 0]])>,\n 'masked_lm_weights': <tf.Tensor: shape=(2, 5), dtype=int64, numpy=\n array([[1, 1, 0, 0, 0],\n        [1, 1, 0, 0, 0]])>}"
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_inputs = {\n",
    "    \"input_word_ids\": input_word_ids,\n",
    "    \"input_mask\": input_mask,\n",
    "    \"input_type_ids\": input_type_ids,\n",
    "    \"masked_lm_ids\": masked_lm_ids,\n",
    "    \"masked_lm_positions\": masked_lm_positions,\n",
    "    \"masked_lm_weights\": masked_lm_weights,\n",
    "}\n",
    "model_inputs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 总结方法\n",
    "1. Input是tf.string类型的Tensor 其中有text_a text_b 每个text_a和text_b的维度是[batch,1]\n",
    "2. 将Input进行Tokenizer 按照vocab初始化BertTokenizer 将每个text_a text_b 维度变成[batch, num_words, wordpieces] 即将一个句子分成word 再把word分成wordpieces\n",
    "3. 本任务无需wordpieces单独处理 所以将每个text_a text_b 变成[batch, num_wordpieces]\n",
    "4. 将两个句子裁剪为MAX_SEQ_LEN以内 trim方法 变成[batch, num_wordpieces_a] [batch, num_wordpiece_b] 其中num_wordpieces_a + num_wordpiece_b <= MAX_SEQ_LEN\n",
    "5. 将text_a text_b 进行拼接 变成[batch, seq_len] seq_len = num_wordpieces_a + num_wordpiece_b < MAX_SEQ_LEN 这一步之后得到一个句子和区分text a和b的type_ids\n",
    "6. 进行mask 随机选择mask的位置和mask的value 进行mask 得到masked_input被mask后的输入 mask_pos即mask的位置 mask_ids 被mask的原始token id\n",
    "7. 进行pad到MAX_SEQ_LEN 和 MAX_PREDICT_LEN\n",
    "8. 用上面所有东西作为inputs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "outputs": [],
   "source": [
    "_VOCAB = [\n",
    "    # Special tokens 特殊token\n",
    "    b\"[UNK]\", b\"[MASK]\", b\"[RANDOM]\", b\"[CLS]\", b\"[SEP]\",\n",
    "    # Suffixes 词根\n",
    "    b\"##ack\", b\"##ama\", b\"##ger\", b\"##gers\", b\"##onge\", b\"##pants\",  b\"##uare\",\n",
    "    b\"##vel\", b\"##ven\", b\"an\", b\"A\", b\"Bar\", b\"Hates\", b\"Mar\", b\"Ob\",\n",
    "    b\"Patrick\", b\"President\", b\"Sp\", b\"Sq\", b\"bob\", b\"box\", b\"has\", b\"highest\",\n",
    "    b\"is\", b\"office\", b\"the\",\n",
    "]\n",
    "_VOCAB_SIZE = len(_VOCAB)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "outputs": [],
   "source": [
    "_START_TOKEN = _VOCAB.index(b\"[CLS]\")\n",
    "_END_TOKEN = _VOCAB.index(b\"[SEP]\")\n",
    "_MASK_TOKEN = _VOCAB.index(b\"[MASK]\")\n",
    "_RANDOM_TOKEN = _VOCAB.index(b\"[RANDOM]\")\n",
    "_UNK_TOKEN = _VOCAB.index(b\"[UNK]\")\n",
    "_MAX_SEQ_LEN = 128\n",
    "_MAX_PREDICTIONS_PER_BATCH = 20"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "outputs": [],
   "source": [
    "\n",
    "def bert_pretrain_preprocess(vocab_table, features):\n",
    "    \"\"\"\n",
    "    对features处理为bert的训练输入\n",
    "    :param vocab_table: BertTokenizer进行tokenize使用的vocab_table\n",
    "    :param features: {'text_a': [batch,1], 'text_b':[batch,1]} 其中1是tf.string的一个句子\n",
    "    :return: 处理后的inputs\n",
    "    \"\"\"\n",
    "    # step1 得到Input\n",
    "    text_a = features['text_a']\n",
    "    text_b = features['text_b']\n",
    "\n",
    "    # step2 初始化BertTokenizer将 text_a text_b进行处理为wordpieces\n",
    "    bert_tokenizer = text.BertTokenizer(vocab_lookup_table=vocab_table,token_out_type=tf.int64)\n",
    "    text_a_tokenized = bert_tokenizer.tokenize(text_a)\n",
    "    text_b_tokenized = bert_tokenizer.tokenize(text_b)\n",
    "    # [batch, num_words, wordpieces]\n",
    "\n",
    "    # step3 合并最后两维\n",
    "    text_a_merged = text_a_tokenized.merge_dims(-2,-1)\n",
    "    text_b_merged = text_b_tokenized.merge_dims(-2,-1)\n",
    "    # [batch, num_wordpieces]\n",
    "\n",
    "    # step4 将两个句子按照batch相同的维度裁剪到和小于等于MAX_SEQ_LEN\n",
    "    trimmer = text.RoundRobinTrimmer(max_seq_length=_MAX_SEQ_LEN)\n",
    "    text_a_trimmed, text_b_trimmed = trimmer.trim([text_a_merged, text_b_merged])\n",
    "    # [batch, num_wordpieces_a] [batch, num_wordpieces_b] num_wordpieces_a + num_wordpiece_b <= MAX_SEQ_LEN\n",
    "\n",
    "    # step5 将text_a_trimmed text_b_trimmed进行拼接\n",
    "    combined_segments, segment_ids = text.combine_segments(\n",
    "        segments=[text_a_trimmed, text_b_trimmed],\n",
    "        start_of_sequence_id=_START_TOKEN,\n",
    "        end_of_segment_id=_END_TOKEN\n",
    "    )\n",
    "    # [batch, 3+num_wordpieces_a+num_wordpieces_b] 3 = [SOS] + [EOS] + [EOS]\n",
    "\n",
    "    # step6 进行mask\n",
    "    random_item_selector = text.RandomItemSelector(\n",
    "        max_selections_per_batch=_MAX_PREDICTIONS_PER_BATCH,\n",
    "        selection_rate=0.2,\n",
    "        unselectable_ids=[_START_TOKEN, _END_TOKEN, _UNK_TOKEN]\n",
    "    )\n",
    "\n",
    "    masked_values_chooser = text.MaskValuesChooser(vocab_size=_VOCAB_SIZE,\n",
    "                                                   mask_token=_MASK_TOKEN,\n",
    "                                                   mask_token_rate=0.8,\n",
    "                                                   random_token_rate=0.1)\n",
    "\n",
    "    masked_input_ids, masked_positions, masked_ids = (text.mask_language_model(\n",
    "        combined_segments,\n",
    "        item_selector=random_item_selector,\n",
    "        mask_values_chooser=masked_values_chooser,\n",
    "    ))\n",
    "    # masked_input_ids [batch, 3+num_wordpieces_a+num_wordpieces_b]\n",
    "    # masked_positions [batch, masked_len]\n",
    "    # masked_ids [batch, masked_len]\n",
    "\n",
    "    # step7 进行pad 对输入pad到MAX_SEQ_LEN 对masked部分pad到_MAX_PREDICTIONS_PER_BATCH\n",
    "    input_word_ids, input_mask = text.pad_model_inputs(\n",
    "        input=masked_input_ids,\n",
    "        max_seq_length=_MAX_SEQ_LEN\n",
    "    )\n",
    "    # 对masked_input_ids进行pad 得到输入的句子和mask值 1代表是句子 0代表是padding\n",
    "\n",
    "    input_type_ids, _ = text.pad_model_inputs(\n",
    "        input=segment_ids,\n",
    "        max_seq_length=_MAX_SEQ_LEN\n",
    "    )\n",
    "    # 对sentence_a sentence_b区分的部分进行pad 0代表句子a和padding 1代表句子b 只要在后面pad上0 即可\n",
    "\n",
    "    masked_lm_ids, _ = text.pad_model_inputs(\n",
    "        input=masked_ids,\n",
    "        max_seq_length=_MAX_PREDICTIONS_PER_BATCH\n",
    "    )\n",
    "    # 对masked ids进行pad 得到的是真正要输出的ids\n",
    "\n",
    "    masked_lm_positions, masked_lm_weights = text.pad_model_inputs(\n",
    "        input=masked_positions,\n",
    "        max_seq_length=_MAX_PREDICTIONS_PER_BATCH\n",
    "    )\n",
    "    # 对masked position进行pad masked_lm_positions是pad后的结果  masked_lm_weights中1代表是position位置0代表是pad的部分\n",
    "\n",
    "    # step8 综合上面所有作为字典 作为BERT模型的输入\n",
    "    model_inputs = {\n",
    "        \"input_word_ids\" : input_word_ids,\n",
    "        \"input_mask\" : input_mask,\n",
    "        \"input_type_ids\" : input_type_ids,\n",
    "        \"masked_lm_ids\" : masked_lm_ids,\n",
    "        \"masked_lm_positions\" : masked_lm_positions,\n",
    "        \"masked_lm_weights\" : masked_lm_weights\n",
    "    }\n",
    "    return model_inputs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "outputs": [],
   "source": [
    "lookup_table = tf.lookup.StaticVocabularyTable(\n",
    "    tf.lookup.KeyValueTensorInitializer(\n",
    "        keys = _VOCAB, # keys就是对应表的key 由string映射到int 那么key就是string\n",
    "        key_dtype=tf.string,\n",
    "        values=tf.range(\n",
    "            tf.size(_VOCAB, out_type=tf.int64), # values就是对应表的value 是int值 用0->VOCAB的长度表示\n",
    "            dtype=tf.int64,\n",
    "        ),\n",
    "        value_dtype=tf.int64,\n",
    "    ),\n",
    "    num_oov_buckets = 1\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "outputs": [],
   "source": [
    "# 使用lookup_table初始化bert_pretrain_preprocess函数\n",
    "# 使用map处理dataset中的数据\n",
    "dataset = (\n",
    "    tf.data.Dataset.from_tensors(examples)\n",
    "    .map(functools.partial(bert_pretrain_preprocess, lookup_table))\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "outputs": [
    {
     "data": {
      "text/plain": "{'input_word_ids': <tf.Tensor: shape=(2, 128), dtype=int64, numpy=\n array([[ 3, 22,  1, 24, 23, 11, 10, 28, 14, 15, 13,  7,  4, 16,  5, 19,\n          1, 28, 30,  1,  0,  4,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n        [ 3,  1, 12, 15, 13,  8,  4, 21, 28, 30, 27, 29,  4,  0,  0,  0,\n          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])>,\n 'input_mask': <tf.Tensor: shape=(2, 128), dtype=int64, numpy=\n array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])>,\n 'input_type_ids': <tf.Tensor: shape=(2, 128), dtype=int64, numpy=\n array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])>,\n 'masked_lm_ids': <tf.Tensor: shape=(2, 20), dtype=int64, numpy=\n array([[ 9, 15,  6, 21,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n          0,  0,  0,  0],\n        [18, 12,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n          0,  0,  0,  0]])>,\n 'masked_lm_positions': <tf.Tensor: shape=(2, 20), dtype=int64, numpy=\n array([[ 2,  9, 16, 19,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n          0,  0,  0,  0],\n        [ 1,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n          0,  0,  0,  0]])>,\n 'masked_lm_weights': <tf.Tensor: shape=(2, 20), dtype=int64, numpy=\n array([[1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])>}"
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dataset))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}