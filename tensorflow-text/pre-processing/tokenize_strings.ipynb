{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 使用TF TEXT进行tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import requests\n",
    "import tensorflow as tf\n",
    "import tensorflow_text as tf_text\n",
    "import functools\n",
    "import os\n",
    "import tensorflow.keras as keras"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "tf.get_logger().setLevel('ERROR')\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1' # 使用 GPU 1\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "physical_devices = tf.config.list_physical_devices(\"GPU\")\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0],True)\n",
    "logical_devices = tf.config.list_logical_devices(\"GPU\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## tokenize的API\n",
    "主要的接口是Splitter和SplitterWithOffset API 他们分别只有一个方法split 和 split_with_offset\n",
    "Tokenizer和TokenizerWithOffset是Splitter的实例化 提供了便捷的tokenize和tokenize_with_offsets方法\n",
    "输入是N维矩阵的话输出是N+1维的RaggedTensor 最内层是tokenize的结果\n",
    "同时还有Detokenizer接口 通过tokenizer接口tokenize的N维RaggedTensor会变为N-1维的tensor或者RaggedTensor"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Whole word tokenizers 整个单词进行tokenize\n",
    "1. WhitespaceTokenizer\n",
    "空白符号tokenizer使用ICU定义的空白字符(空格 制表符 换行符)进行分割\n",
    "空白符号tokenizer将标点符号和字母连接在了一起 同时不能处理类似于汉字没有空格分隔的句子\n",
    "2. UnicodeScriptTokenizer\n",
    "UnicodeScriptTokenizer用unicode进行分割 同时将标点符号单独处理\n",
    "UnicodeScriptTokenizer还是使用空白符进行分割 不能分割没有空白符的汉字 但是能将标点符号处理"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.RaggedTensor [[[b'How', b'I', b'can', b'study', b'tensorflow', b'well?'],\n  [b'sorry,', b'maybe', b'next', b'sentence', b'knows', b'the', b'answer.'],\n  [b'\\xe6\\x88\\x91\\xe4\\xb8\\x8d\\xe7\\x9f\\xa5\\xe9\\x81\\x93\\xe5\\x91\\x80\\xe3\\x80\\x82']],\n [[b'Just', b'study', b'the', b'tutorials', b'carefully.'],\n  [b'come', b'on!'], [b'\\xe5\\x8a\\xa0\\xe6\\xb2\\xb9!']]]>"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## WhitespaceTokenizer\n",
    "## 空白符号tokenizer使用ICU定义的空白字符(空格 制表符 换行符)进行分割\n",
    "## 空白符号tokenizer将标点符号和字母连接在了一起 同时不能处理类似于汉字没有空格分隔的句子\n",
    "\n",
    "WSTokenizer = tf_text.WhitespaceTokenizer()\n",
    "WStokens = WSTokenizer.tokenize([[\"How I can study tensorflow well?\", \"sorry, maybe next sentence knows the answer.\",\"我不知道呀。\"],\n",
    "                               [\"Just study the tutorials carefully.\", \"come on!\",\"加油!\"]])\n",
    "WStokens\n",
    "# 生成了一个RaggedTensor\n",
    "# 之前的输入是二维 [batch=2, sentence_count = 2]\n",
    "# 输出是三维[batch=2, sentence_count = 2, seq_len] 最内层是token的分割后表示"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第一个batch内的内容的表示：\n",
      "<tf.RaggedTensor [[b'How', b'I', b'can', b'study', b'tensorflow', b'well?'],\n",
      " [b'sorry,', b'maybe', b'next', b'sentence', b'knows', b'the', b'answer.'],\n",
      " [b'\\xe6\\x88\\x91\\xe4\\xb8\\x8d\\xe7\\x9f\\xa5\\xe9\\x81\\x93\\xe5\\x91\\x80\\xe3\\x80\\x82']]>\n",
      "第一个batch内,第一个句子的表示:\n",
      "tf.Tensor([b'How' b'I' b'can' b'study' b'tensorflow' b'well?'], shape=(6,), dtype=string)\n",
      "这个句子对应string tensor的维度：\n",
      "(6,)\n",
      "第一个batch内,第一个句子的第一个token的表示:\n",
      "tf.Tensor(b'How', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "print('第一个batch内的内容的表示：')\n",
    "print(WStokens[0])\n",
    "print('第一个batch内,第一个句子的表示:')\n",
    "print(WStokens[0][0])\n",
    "print('这个句子对应string tensor的维度：')\n",
    "print(WStokens[0][0].shape)\n",
    "print('第一个batch内,第一个句子的第一个token的表示:')\n",
    "print(WStokens[0][0][0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.RaggedTensor [[[b'How', b'I', b'can', b'study', b'tensorflow', b'well', b'?'],\n  [b'sorry', b',', b'maybe', b'next', b'sentence', b'knows', b'the',\n   b'answer', b'.']                                                 ,\n  [b'\\xe6\\x88\\x91\\xe4\\xb8\\x8d\\xe7\\x9f\\xa5\\xe9\\x81\\x93\\xe5\\x91\\x80',\n   b'\\xe3\\x80\\x82']                                                ],\n [[b'Just', b'study', b'the', b'tutorials', b'carefully', b'.'],\n  [b'come', b'on', b'!'], [b'\\xe5\\x8a\\xa0\\xe6\\xb2\\xb9', b'!']]]>"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## UnicodeScriptTokenizer\n",
    "## UnicodeScriptTokenizer用unicode进行分割 同时将标点符号单独处理\n",
    "## UnicodeScriptTokenizer还是使用空白符进行分割 不能分割没有空白符的汉字 但是能将标点符号处理\n",
    "\n",
    "USTokenizer = tf_text.UnicodeScriptTokenizer()\n",
    "UStokens = USTokenizer.tokenize([[\"How I can study tensorflow well?\", \"sorry, maybe next sentence knows the answer.\",\"我不知道呀。\"],\n",
    "                               [\"Just study the tutorials carefully.\", \"come on!\",\"加油!\"]])\n",
    "UStokens"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Subword tokenizers\n",
    "subword tokenizers可以和较小的词汇表一起使用，允许模型从创建词汇的子词中获得一些信息\n",
    "1. WordpieceTokenizer\n",
    "WordpieceTokenizer是一种数据驱动的tokenization方法 其生成了一组sub-tokens 和语言本身的语素有关\n",
    "WordpieceTokenizer期望接受一个分割后的tokens作为输入进行数据驱动 所以一般先用WhiteSpaceTokenizer或者UnicodeScriptTokenizer\n",
    "本质就是对vocab文件的映射\n",
    "2. BertTokenizer\n",
    "BertTokenizer实现了BERT论文中的实现方法，本质是由WordPieceTokenizer支持的 但是还执行其他任务 如单词规范化和标记化\n",
    "本质还是对vocab文件的映射\n",
    "3. SentencepieceTokenizer\n",
    "SentencepieceTokenizer是基于sentencepiece库的 这个方法是根据输入数据快速迭代的 有很明显的sub-word的效果 见ML_tools仓库"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[b'How', b'I', b'can', b'study', b'tensorflow', b'well', b'?'], [b'sorry', b',', b'maybe', b'next', b'sentence', b'knows', b'the', b'answer', b'.'], [b'\\xe6\\x88\\x91\\xe4\\xb8\\x8d\\xe7\\x9f\\xa5\\xe9\\x81\\x93\\xe5\\x91\\x80', b'\\xe3\\x80\\x82']], [[b'Just', b'study', b'the', b'tutorials', b'carefully', b'.'], [b'come', b'on', b'!'], [b'\\xe5\\x8a\\xa0\\xe6\\xb2\\xb9', b'!']]]]\n"
     ]
    }
   ],
   "source": [
    "## WordpieceTokenizer\n",
    "\n",
    "# 先使用WSTokenizer进行分词处理\n",
    "tokens = USTokenizer.tokenize([[[\"How I can study tensorflow well?\", \"sorry, maybe next sentence knows the answer.\",\"我不知道呀。\"],\n",
    "                               [\"Just study the tutorials carefully.\", \"come on!\",\"加油!\"]]])\n",
    "print(tokens.to_list())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "52382"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://github.com/tensorflow/text/blob/master/tensorflow_text/python/ops/test_data/test_wp_en_vocab.txt?raw=true\"\n",
    "r = requests.get(url)\n",
    "filepath = \"tensorflow_study/tensorflow-text/data_dir/vocab.txt\"\n",
    "open(filepath, 'wb').write(r.content)\n",
    "# 下载vocab.txt 作为数据驱动"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.RaggedTensor [[[[[1], [1], [94], [574], [2358, 687, 1192, 2365], [157], [30]],\n   [[1504], [13], [285], [261], [1757], [996], [71], [430], [15]],\n   [[1], [1]]],\n  [[[1], [574], [71], [56, 1804, 687, 6452], [1927], [15]],\n   [[211], [92], [4]], [[1], [4]]]]]>"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WPTokenizer = tf_text.WordpieceTokenizer(filepath)\n",
    "wordpiecetokens = WPTokenizer.tokenize(tokens)\n",
    "wordpiecetokens\n",
    "# 本质上就是用一个vocab.txt进行映射 将byte映射到int表示上去"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.RaggedTensor [[[b'[UNK]'],\n  [b'you'],\n  [b'know'],\n  [b'you'],\n  [b'can'],\n  [b\"'\"],\n  [b't'],\n  [b'explain'],\n  [b','],\n  [b'but'],\n  [b'you'],\n  [b'feel'],\n  [b'it'],\n  [b'.']]]>"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WPTokenizer.detokenize(wordpiecetokens)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.RaggedTensor [[[[119], [45], [94], [574], [2358, 687, 1192, 2365], [157], [30]],\n  [[1504], [13], [285], [261], [1757], [996], [71], [430], [15]],\n  [[1], [1], [1], [1], [1], [1]]],\n [[[112], [574], [71], [56, 1804, 687, 6452], [1927], [15]],\n  [[211], [92], [4]], [[1], [1], [4]]]]>"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BertTokenizer\n",
    "# BertTokenizer实现了BERT论文中的实现方法，本质是由WordPieceTokenizer支持的 但是还执行其他任务 如单词规范化和标记化\n",
    "BTokenizer = tf_text.BertTokenizer(filepath, lower_case=True)\n",
    "BTokens = BTokenizer.tokenize([[\"How I can study tensorflow well?\", \"sorry, maybe next sentence knows the answer.\",\"我不知道呀。\"],\n",
    "                               [\"Just study the tutorials carefully.\", \"come on!\",\"加油!\"]])\n",
    "BTokens\n",
    "# 本质上还是对vocab的映射"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "## SentencepieceTokenizer\n",
    "## SentencepieceTokenizer由Sentencepiece库支持\n",
    "## 类似于BertTokenizer 其包括正则化和sub-tokens的分割 这个可以见ML_tools部分有\n",
    "\n",
    "url = \"https://github.com/tensorflow/text/blob/master/tensorflow_text/python/ops/test_data/test_oss_model.model?raw=true\"\n",
    "sp_model = requests.get(url).content"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[b'\\xe2\\x96\\x81And', b'\\xe2\\x96\\x81it', b\"'\", b's', b'\\xe2\\x96\\x81tru', b'ly', b'\\xe2\\x96\\x81a', b'\\xe2\\x96\\x81great', b'\\xe2\\x96\\x81honor', b'\\xe2\\x96\\x81to', b'\\xe2\\x96\\x81have', b'\\xe2\\x96\\x81the', b'\\xe2\\x96\\x81', b'op', b'p', b'or', b't', b'un', b'ity', b'\\xe2\\x96\\x81to', b'\\xe2\\x96\\x81come', b'\\xe2\\x96\\x81to', b'\\xe2\\x96\\x81this', b'\\xe2\\x96\\x81', b'st', b'age', b'\\xe2\\x96\\x81', b't', b'w', b'ic', b'e', b'\\xe2\\x96\\x81', b';', b'\\xe2\\x96\\x81I', b\"'\", b'm', b'\\xe2\\x96\\x81ex', b't', b're', b'm', b'e', b'ly', b'\\xe2\\x96\\x81gr', b'ate', b'ful', b'\\xe2\\x96\\x81', b'.']]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tf_text.SentencepieceTokenizer(sp_model,out_type=tf.string)\n",
    "tokens = tokenizer.tokenize([\"And it's truly a great honor to have the opportunity to come to this stage twice ; I'm extremely grateful .\"])\n",
    "print(tokens.to_list())\n",
    "# 下面的\\xe2\\x96\\x81是一个特殊token\n",
    "# 可以看到其有sub-word的能力 如truly->tru + ly"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 其他Tokenizer\n",
    "1. UnicodeCharTokenizer\n",
    "按照UTF-8进行分隔 对没有空格的语言很有用\n",
    "本质和[word_with_unicode]中一样 用unicode编号进行tokenize\n",
    "2. HubModuleTokenizer\n",
    "这是一个部署在TF Hub上的模型 并不支持RaggedTensor 这个对没有空格的启发式语言很有效果\n",
    "这个很棒！https://hub.tensorflow.google.cn/google/zh_segmentation/1 提供了一个应用于中文的按照语义进行分割的tokenizer\n",
    "3. SplitMergeTokenizer\n",
    "SplitMergeTokenizer 和 SplitMergeFromLogitsTokenizer 通过对分割点的显式提供进行分割\n",
    "SplitMergeTokenizer 提供0,1的向量0代表分割点即token的开头1代表不分隔\n",
    "SplitMergeFromLogitsTokenizer 是通过得分进行分割 第一维大于第二维则代表0 否则代表1\n",
    "4. RegexSplitter\n",
    "通过正则表达式进行确定分割点"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.RaggedTensor [[[72, 111, 119, 32, 73, 32, 99, 97, 110, 32, 115, 116, 117, 100, 121, 32,\n   116, 101, 110, 115, 111, 114, 102, 108, 111, 119, 32, 119, 101, 108, 108,\n   63]                                                                      ,\n  [115, 111, 114, 114, 121, 44, 32, 109, 97, 121, 98, 101, 32, 110, 101, 120,\n   116, 32, 115, 101, 110, 116, 101, 110, 99, 101, 32, 107, 110, 111, 119,\n   115, 32, 116, 104, 101, 32, 97, 110, 115, 119, 101, 114, 46]              ,\n  [25105, 19981, 30693, 36947, 21568, 12290]],\n [[74, 117, 115, 116, 32, 115, 116, 117, 100, 121, 32, 116, 104, 101, 32,\n   116, 117, 116, 111, 114, 105, 97, 108, 115, 32, 99, 97, 114, 101, 102,\n   117, 108, 108, 121, 46]                                               ,\n  [99, 111, 109, 101, 32, 111, 110, 33], [21152, 27833, 33]]]>"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UCTokenizer = tf_text.UnicodeCharTokenizer()\n",
    "UCTokens = UCTokenizer.tokenize([[\"How I can study tensorflow well?\", \"sorry, maybe next sentence knows the answer.\",\"我不知道呀。\"],\n",
    "                               [\"Just study the tutorials carefully.\", \"come on!\",\"加油!\"]])\n",
    "UCTokens"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.RaggedTensor [[b'\\xe6\\x96\\xb0\\xe5\\x8d\\x8e\\xe7\\xa4\\xbe', b'\\xe5\\x8c\\x97\\xe4\\xba\\xac',\n  b'\\xe4\\xbb\\x8a\\xe5\\xa4\\xa9', b'\\xe6\\x8a\\xa5\\xe9\\x81\\x93',\n  b'\\xe4\\xba\\x86', b'\\xe4\\xb8\\x80', b'\\xe4\\xb8\\xaa',\n  b'\\xe6\\x96\\xb0\\xe9\\x97\\xbb']]>"
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_HANDLE = \"https://hub.tensorflow.google.cn/google/zh_segmentation/1\"\n",
    "segmenter = tf_text.HubModuleTokenizer(MODEL_HANDLE)\n",
    "tokens = segmenter.tokenize([\"新华社北京今天报道了一个新闻\"])\n",
    "tokens\n",
    "# 这个很难理解"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "data": {
      "text/plain": "'新华社'"
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens.to_list()[0][0].decode('utf-8')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['新华社', '北京', '今天', '报道', '了', '一', '个', '新闻']]\n"
     ]
    }
   ],
   "source": [
    "def decode_list(x):\n",
    "  if type(x) is list:\n",
    "    return list(map(decode_list, x))\n",
    "  return x.decode(\"UTF-8\")\n",
    "\n",
    "def decode_utf8_tensor(x):\n",
    "  return list(map(decode_list, x.to_list()))\n",
    "\n",
    "print(decode_utf8_tensor(tokens))\n",
    "# 可以看到其将一个连续的多个token按照语义切分为了多个部分"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['新华社', '北京', '今天', '报道了', '一个', '新闻']]\n"
     ]
    }
   ],
   "source": [
    "strings_ = [\"新华社北京今天报道了一个新闻\"]\n",
    "labels = [[0, 1, 1, 0, 1,0,1,0,1,1,0,1,0,1]] # 提供的分割点\n",
    "tokenizer = tf_text.SplitMergeTokenizer()\n",
    "tokens = tokenizer.tokenize(strings_, labels)\n",
    "print(decode_utf8_tensor(tokens))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['新华社', '北京']]\n"
     ]
    }
   ],
   "source": [
    "strings = [[\"新华社北京\"]]\n",
    "labels = [[[5.0, -3.2], [0.2, 12.0], [0.0, 11.0], [2.2, -1.0], [-3.0, 3.0]]] # 提供的分割点的概率\n",
    "tokenizer = tf_text.SplitMergeFromLogitsTokenizer()\n",
    "tokens = tokenizer.tokenize(strings, labels)\n",
    "print(decode_utf8_tensor(tokens))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[b'What', b'you', b'know', b'you', b\"can't\", b'explain,', b'but', b'you', b'feel', b'it.']]\n"
     ]
    }
   ],
   "source": [
    "splitter = tf_text.RegexSplitter(\"\\s\")\n",
    "tokens = splitter.split([\"What you know you can't explain, but you feel it.\"], )\n",
    "print(tokens.to_list())\n",
    "# 使用\\s即每个字符串进行一次分隔"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## offset\n",
    "如果要知道每个被分隔的token在原始句子的位置 就可以使用几乎所有tokenizer的tokenize_with_offsets方法\n",
    "左闭右开的"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[b'\\xe6\\x96\\xb0\\xe5\\x8d\\x8e\\xe7\\xa4\\xbe', b'\\xe5\\x8c\\x97\\xe4\\xba\\xac', b'\\xe4\\xbb\\x8a\\xe5\\xa4\\xa9', b'\\xe6\\x8a\\xa5\\xe9\\x81\\x93', b'\\xe4\\xba\\x86', b'\\xe4\\xb8\\x80', b'\\xe4\\xb8\\xaa', b'\\xe6\\x96\\xb0\\xe9\\x97\\xbb', b'\\xef\\xbc\\x8c', b'\\xe4\\xbb\\xa4', b'\\xe5\\xa4\\xa7\\xe5\\xae\\xb6', b'\\xe5\\xa4\\xa7\\xe4\\xb8\\xba', b'\\xe5\\x90\\x83\\xe6\\x83\\x8a', b'\\xef\\xbc\\x81']]\n",
      "[['新华社', '北京', '今天', '报道', '了', '一', '个', '新闻', '，', '令', '大家', '大为', '吃惊', '！']]\n",
      "[[0, 9, 15, 21, 27, 30, 33, 36, 42, 45, 48, 54, 60, 66]]\n",
      "[[9, 15, 21, 27, 30, 33, 36, 42, 45, 48, 54, 60, 66, 69]]\n"
     ]
    }
   ],
   "source": [
    "(tokens_, start_offsets, end_offsets) = segmenter.tokenize_with_offsets(['新华社北京今天报道了一个新闻，令大家大为吃惊！'])\n",
    "print(tokens_.to_list())\n",
    "print(decode_utf8_tensor(tokens_))\n",
    "print(start_offsets.to_list())\n",
    "print(end_offsets.to_list())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[b'Everything', b'not', b'saved', b'will', b'be', b'lost', b'.']]\n",
      "[[0, 11, 15, 21, 26, 29, 33]]\n",
      "[[10, 14, 20, 25, 28, 33, 34]]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tf_text.UnicodeScriptTokenizer()\n",
    "(tokens, start_offsets, end_offsets) = tokenizer.tokenize_with_offsets(['Everything not saved will be lost.'])\n",
    "print(tokens.to_list())\n",
    "print(start_offsets.to_list())\n",
    "print(end_offsets.to_list())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## detokenization\n",
    "tokenize的逆操作 但是并不是所有的tokenizer都有这个方法\n",
    "同时tokenize和detokenization可能是有损的 并不一定完全还原"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[69, 118, 101, 114, 121, 116, 104, 105, 110, 103, 32, 110, 111, 116, 32, 115, 97, 118, 101, 100, 32, 119, 105, 108, 108, 32, 98, 101, 32, 108, 111, 115, 116, 46]]\n",
      "[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33]]\n",
      "[[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34]]\n",
      "tf.Tensor([b'Everything not saved will be lost.'], shape=(1,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tf_text.UnicodeCharTokenizer()\n",
    "(tokens, start_offsets, end_offsets) = tokenizer.tokenize_with_offsets(['Everything not saved will be lost.'])\n",
    "print(tokens.to_list())\n",
    "print(start_offsets.to_list())\n",
    "print(end_offsets.to_list())\n",
    "strings = tokenizer.detokenize(tokens)\n",
    "print(strings)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 对tf.data.Dataset对象使用tokenizer\n",
    "使用map(lambda x: tokenizer.tokenize(x))方法"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[72, 111, 119, 32, 73, 32, 99, 97, 110, 32, 115, 116, 117, 100, 121, 32, 116, 101, 110, 115, 111, 114, 102, 108, 111, 119, 32, 119, 101, 108, 108, 63], [115, 111, 114, 114, 121, 44, 32, 109, 97, 121, 98, 101, 32, 110, 101, 120, 116, 32, 115, 101, 110, 116, 101, 110, 99, 101, 32, 107, 110, 111, 119, 115, 32, 116, 104, 101, 32, 97, 110, 115, 119, 101, 114, 46], [25105, 19981, 30693, 36947, 21568, 12290]]\n",
      "[[74, 117, 115, 116, 32, 115, 116, 117, 100, 121, 32, 116, 104, 101, 32, 116, 117, 116, 111, 114, 105, 97, 108, 115, 32, 99, 97, 114, 101, 102, 117, 108, 108, 121, 46], [99, 111, 109, 101, 32, 111, 110, 33], [21152, 27833, 33]]\n"
     ]
    }
   ],
   "source": [
    "docs = tf.data.Dataset.from_tensor_slices([[\"How I can study tensorflow well?\", \"sorry, maybe next sentence knows the answer.\",\"我不知道呀。\"],\n",
    "                               [\"Just study the tutorials carefully.\", \"come on!\",\"加油!\"]])\n",
    "tokenizer = tf_text.UnicodeCharTokenizer()\n",
    "tokenized_docs = docs.map(lambda x: tokenizer.tokenize(x))\n",
    "iterator = iter(tokenized_docs)\n",
    "print(next(iterator).to_list())\n",
    "print(next(iterator).to_list())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}