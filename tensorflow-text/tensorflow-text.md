- [tensorflow-textéƒ¨åˆ†ç›®å½•](#tensorflow-textéƒ¨åˆ†ç›®å½•)
  - [unicode](#unicode)
    - [unicodeçš„tfè¡¨ç¤º](#unicodeçš„tfè¡¨ç¤º)
    - [tf.stringså¤„ç†unicode](#tfstringså¤„ç†unicode)
    - [å¯¹å¸¦æœ‰batchçš„å†…å®¹å¤„ç† RaggedTensor](#å¯¹å¸¦æœ‰batchçš„å†…å®¹å¤„ç†-raggedtensor)
    - [å¯¹unicodeå¯¹è±¡çš„æ“ä½œ](#å¯¹unicodeå¯¹è±¡çš„æ“ä½œ)
    - [è·å¾—unicodeè¡¨ç¤ºçš„characteræ‰€åœ¨çš„è¯­è¨€](#è·å¾—unicodeè¡¨ç¤ºçš„characteræ‰€åœ¨çš„è¯­è¨€)
  - [word embedding](#word-embedding)
  - [decodingè§£ç æ–¹å¼](#decodingè§£ç æ–¹å¼)
  - [PRE-PROCESSING æ–‡æœ¬é¢„å¤„ç†](#pre-processing-æ–‡æœ¬é¢„å¤„ç†)
    - [tokenize stringå¯¹stringè¿›è¡Œtokenåˆ†éš”](#tokenize-stringå¯¹stringè¿›è¡Œtokenåˆ†éš”)
      - [tokenizeçš„API](#tokenizeçš„api)
      - [Whole word tokenizers æ•´ä¸ªå•è¯è¿›è¡Œtokenize](#whole-word-tokenizers-æ•´ä¸ªå•è¯è¿›è¡Œtokenize)
      - [Subword tokenizers](#subword-tokenizers)
      - [å…¶ä»–Tokenizer](#å…¶ä»–tokenizer)
      - [offset](#offset)
      - [detokenization](#detokenization)
      - [å¯¹tf.data.Datasetå¯¹è±¡ä½¿ç”¨tokenizer](#å¯¹tfdatadatasetå¯¹è±¡ä½¿ç”¨tokenizer)
    - [subword tokenizer](#subword-tokenizer)
      - [é€šè¿‡tf.data.Datasetç”Ÿæˆbert_vocab](#é€šè¿‡tfdatadatasetç”Ÿæˆbert_vocab)
      - [åŸºäºvocabç”ŸæˆBertTokenizer](#åŸºäºvocabç”Ÿæˆberttokenizer)
    - [BERT preprocessing](#bert-preprocessing)
  - [BERT Experiments](#bert-experiments)
# tensorflow-textéƒ¨åˆ†ç›®å½•

## unicode
è§[unicode part](basic_concepts/work_with_unicode.ipynb)  
NLPæ¨¡å‹ç»å¸¸å¤„ç†ä¸åŒçš„è¯­è¨€ï¼Œä¸åŒçš„è¯­è¨€åˆæœ‰ä¸åŒçš„è¯å…¸  Unicodeæ˜¯é’ˆå¯¹äºå‡ ä¹æ‰€æœ‰è¯­è¨€éƒ½å¯ä»¥ç”¨å…¶è¿›è¡Œè¡¨ç¤ºæ–‡å­—çš„æ–¹æ³•  
unicode characteræ˜¯0-0x0FFFFçš„intå€¼ unicode stringæ˜¯ä¸€ä¸²0æˆ–è€…unicode character  

### unicodeçš„tfè¡¨ç¤º
1. ç›´æ¥é€šè¿‡constantè¡¨ç¤º è¿™æ ·çš„è¡¨ç¤ºçš„ç»“æœæ˜¯ä¸€ä¸ªbyteå½¢å¼ b'\ \ \'çš„å½¢å¼
```python
tf.constant(u"abc")
tf.constant([u"abc", u"def"])
```
2. é€šè¿‡unicode codepointè¡¨ç¤º
```python
tf.constant([ord(s) for s in u"è‡ªç„¶è¯­è¨€å¤„ç†"])
```
### tf.stringså¤„ç†unicode
1. tf.strings.unicode_decode:å°†string scalarè½¬ä¸ºcode pointsçš„å‘é‡
```python
text_utf8 = tf.constant("è‡ªç„¶è¯­è¨€å¤„ç†") # <tf.Tensor: shape=(), dtype=string, numpy=b'\xe8\x87\xaa\xe7\x84\xb6\xe8\xaf\xad\xe8\xa8\x80\xe5\xa4\x84\xe7\x90\x86'>
tf.strings.unicode_decode(text_utf8, input_encoding='UTF-8') # <tf.Tensor: shape=(6,), dtype=int32, numpy=array([33258, 28982, 35821, 35328, 22788, 29702], dtype=int32)>
```
2. tf.strings.unicode_encode:å°†code pointså‘é‡è½¬ä¸º string scalar
```python
text_chars = tf.constant([ord(s) for s in u"è‡ªç„¶è¯­è¨€å¤„ç†"]) # <tf.Tensor: shape=(6,), dtype=int32, numpy=array([33258, 28982, 35821, 35328, 22788, 29702], dtype=int32)>
tf.strings.unicode_encode(text_chars, output_encoding='UTF-8') # <tf.Tensor: shape=(), dtype=string, numpy=b'\xe8\x87\xaa\xe7\x84\xb6\xe8\xaf\xad\xe8\xa8\x80\xe5\xa4\x84\xe7\x90\x86'>
```
3. tf.strings.unicode_transcode:å°†string scalarè½¬ä¸ºå…¶ä»–å½¢å¼çš„ç¼–ç  å¦‚utf-8 -> utf-16-be
```python
text_utf16be = tf.constant(u"è‡ªç„¶è¯­è¨€å¤„ç†".encode("UTF-16-BE")) # <tf.Tensor: shape=(), dtype=string, numpy=b'\x81\xeaq6\x8b\xed\x8a\x00Y\x04t\x06'>
tf.strings.unicode_transcode(text_utf16be, input_encoding='UTF-16-BE', output_encoding='UTF-8') # tf.strings.unicode_transcode(text_utf16be, input_encoding='UTF-16-BE', output_encoding='UTF-8')
```

### å¯¹å¸¦æœ‰batchçš„å†…å®¹å¤„ç† RaggedTensor
```python
batch_utf8 = [s.encode('UTF-8') for s in [u'hÃƒllo', u'What is the weather tomorrow', u'GÃ¶Ã¶dnight', u'ğŸ˜Š']]
'''
[b'h\xc3\x83llo',
 b'What is the weather tomorrow',
 b'G\xc3\xb6\xc3\xb6dnight',
 b'\xf0\x9f\x98\x8a']
'''
batch_chars_ragged = tf.strings.unicode_decode(batch_utf8, input_encoding='UTF-8')
'''
<tf.RaggedTensor [[104, 195, 108, 108, 111],
 [87, 104, 97, 116, 32, 105, 115, 32, 116, 104, 101, 32, 119, 101, 97, 116,
  104, 101, 114, 32, 116, 111, 109, 111, 114, 114, 111, 119]               ,
 [71, 246, 246, 100, 110, 105, 103, 104, 116], [128522]]>
'''
```

### å¯¹unicodeå¯¹è±¡çš„æ“ä½œ
1. character length é€šè¿‡ä¸åŒçš„unitç»Ÿè®¡unicodeçš„é•¿åº¦æœ‰å¤šå°‘units tf.strings.length
2. character substrings å–unicodeçš„substring tf.strings.substr
3. split unicode strings å¯¹unicode stringè¿›è¡Œåˆ‡æ–­ tf.strings.unicode_split
4. byte offset for characters  è·å¾—byte offset tf.strings.unicode_decode_with_offsets

### è·å¾—unicodeè¡¨ç¤ºçš„characteræ‰€åœ¨çš„è¯­è¨€
```python
# unicode 33464ä»£è¡¨æ±‰å­—èŠ¸ 1041ä»£è¡¨è¥¿é‡Œå°”è¯­Ğ‘
# å¯ä»¥ç›´æ¥å¤„ç†listå¯¹è±¡
uscript = tf.strings.unicode_script([33464, 1041])  # ['èŠ¸', 'Ğ‘']
print(uscript.numpy())  # [17, 8] == [USCRIPT_HAN, USCRIPT_CYRILLIC]
# unicode_scriptä¹‹åå¾—åˆ°äº†17->æ±‰è¯­ 8->è¥¿é‡Œå°”è¯­
```

## word embedding
è§[word embedding](basic_concepts/word_embedding.ipynb)  
word embeddingçš„æ ¸å¿ƒæ€æƒ³æ˜¯å°†tokenè½¬åŒ–ä¸ºä¸€ä¸ªç¨ å¯†çš„å‘é‡è¡¨ç¤ºï¼Œè¿›è€Œå°±èƒ½è¿›è¡Œç½‘ç»œè®­ç»ƒ è¿˜å¯ä»¥æŸ¥çœ‹tokençš„ç›¸å…³æ€§ ç›¸ä¼¼åº¦ç­‰  
æ³¨æ„ä¸¤ä¸ªAPI:
1. TextVectorization keras.layers.TextVectorization é€šè¿‡æ–‡æœ¬çš„è®­ç»ƒå¾—åˆ°ä¸€ä¸ªä»tokenæ˜ å°„åˆ°intå€¼çš„è¡¨ ä¹‹åé€šè¿‡å…¶è®¡ç®—ï¼Œå¯ä»¥å°†æ–‡æœ¬tokenç»„æˆçš„å¥å­å˜æˆintå‘é‡
2. Embedding keras.layers.Embedding å°†intå‘é‡è¡¨ç¤ºçš„å¥å­ï¼Œè½¬åŒ–ä¸ºç¨ å¯†å‘é‡è¡¨ç¤º è¿›è¡Œåç»­ç½‘ç»œçš„ä½œç”¨

## decodingè§£ç æ–¹å¼
è§[Decoding_methods](basic_concepts/decoding.ipynb)  
è§£ç æ–¹å¼æœ‰ï¼š
1. Greedy
2. Beam Search Beam searché€šè¿‡æœç´¢æ¯ä¸ªæ—¶é—´æ­¥æ¦‚ç‡æœ€é«˜çš„num_beamsè¿›è¡Œåç»­æœç´¢ï¼Œè¿™æ ·é˜²æ­¢åœ¨è¿‡ç¨‹ä¸­ä¸¢å¼ƒæ€»ä½“æ¦‚ç‡æœ€é«˜çš„ç»“æœ
3. Top-k top-ké‡‡æ ·ä»…é‡‡ç”¨æ¦‚ç‡æœ€é«˜çš„Kä¸ªå†…éƒ¨è¿›è¡Œæ¦‚ç‡é‡æ–°åˆ†é… è¿›è¡Œç”Ÿæˆ
4. Top-p top-pé‡‡æ ·ä½¿ç”¨ç´¯è®¡æ¦‚ç‡å’Œè¶…è¿‡pçš„æœ€å°çš„æ¦‚ç‡ä½œä¸ºç»“æœ

åœ¨from official.nlp.modeling.ops import sampling_module å’Œ from official.nlp.modeling.ops import beam_search ä¸­æœ‰é«˜åº¦é›†æˆçš„API  


## PRE-PROCESSING æ–‡æœ¬é¢„å¤„ç†

text preprocessingæ˜¯ä¸€ç§å°†åŸå§‹æ–‡æœ¬è½¬åŒ–ä¸ºå¯ä»¥è¾“å…¥æ¨¡å‹çš„intå½¢å¼çš„å‘é‡  
æ–‡æœ¬é¢„å¤„ç†ä¸ä½³çš„å±å®³æœ‰ä¸‰ç‚¹ï¼š
1. **Training-serving skew è®­ç»ƒ-æœåŠ¡çš„å·®å¼‚**ã€‚ å¦‚æœåœ¨è®­ç»ƒå’Œä½¿ç”¨çš„å„ä¸ªä¸åŒé˜¶æ®µä½¿ç”¨ä¸åŒçš„è¶…å‚æ•° tokenization tokençš„æ–¹æ³•å’Œé¢„å¤„ç†çš„æ–¹æ³•ï¼Œä¼šç»™æ¨¡å‹å¸¦æ¥æ¯ç­æ€§çš„æ•ˆæœã€‚
2. **Efficiency and flexibility é«˜æ•ˆå¯æ‰©å±•**ã€‚å½“preprocessingç¦»çº¿è¿›è¡Œæ—¶(å°†å¤„ç†åçš„è¾“å‡ºå†™åˆ°ç£ç›˜ä¸­ï¼Œä¹‹åé‡æ–°è¯»å…¥)è¿™æ ·ä¼šäº§ç”Ÿé¢å¤–çš„æˆæœ¬ï¼›å¦‚æœéœ€è¦åŠ¨æ€è¿›è¡Œé¢„å¤„ç†å†³ç­– ç¦»çº¿preprocessingä¹Ÿä¸åˆé€‚ã€‚
3. **Complex model interface å¤æ‚æ¨¡å‹æ¨ç†**ã€‚å½“æ–‡æœ¬æ¨¡å‹çš„è¾“å…¥æ˜¯çº¯æ–‡æœ¬æ—¶ï¼Œæ¨¡å‹æ›´æ˜“äºå¤„ç†ã€‚å½“æ¨¡å‹è¾“å…¥éœ€è¦é¢å¤–çš„ç¼–ç æ­¥éª¤æ—¶ï¼Œæ¨¡å‹å°±ä¸æ˜“ç†è§£ã€‚é™ä½é¢„å¤„ç†å¤æ‚æ€§å¯¹è°ƒè¯•ã€æœåŠ¡å’Œè¯„ä¼°å¾ˆé‡è¦ã€‚

### tokenize stringå¯¹stringè¿›è¡Œtokenåˆ†éš”
è§[tokenize strings](pre-processing/tokenize_strings.ipynb)  
#### tokenizeçš„API
ä¸»è¦çš„æ¥å£æ˜¯Splitterå’ŒSplitterWithOffset API ä»–ä»¬åˆ†åˆ«åªæœ‰ä¸€ä¸ªæ–¹æ³•split å’Œ split_with_offset  
**Tokenizerå’ŒTokenizerWithOffsetæ˜¯Splitterçš„å®ä¾‹åŒ– æä¾›äº†ä¾¿æ·çš„tokenizeå’Œtokenize_with_offsetsæ–¹æ³•**  
è¾“å…¥æ˜¯Nç»´çŸ©é˜µçš„è¯è¾“å‡ºæ˜¯N+1ç»´çš„RaggedTensor æœ€å†…å±‚æ˜¯tokenizeçš„ç»“æœ  
åŒæ—¶è¿˜æœ‰Detokenizeræ¥å£ é€šè¿‡tokenizeræ¥å£tokenizeçš„Nç»´RaggedTensorä¼šå˜ä¸ºN-1ç»´çš„tensoræˆ–è€…RaggedTensor  
#### Whole word tokenizers æ•´ä¸ªå•è¯è¿›è¡Œtokenize
1. WhitespaceTokenizer  
ç©ºç™½ç¬¦å·tokenizerä½¿ç”¨ICUå®šä¹‰çš„ç©ºç™½å­—ç¬¦(ç©ºæ ¼ åˆ¶è¡¨ç¬¦ æ¢è¡Œç¬¦)è¿›è¡Œåˆ†å‰²   
**ç©ºç™½ç¬¦å·tokenizerå°†æ ‡ç‚¹ç¬¦å·å’Œå­—æ¯è¿æ¥åœ¨äº†ä¸€èµ· åŒæ—¶ä¸èƒ½å¤„ç†ç±»ä¼¼äºæ±‰å­—æ²¡æœ‰ç©ºæ ¼åˆ†éš”çš„å¥å­**   
2. UnicodeScriptTokenizer   
UnicodeScriptTokenizerç”¨unicodeè¿›è¡Œåˆ†å‰² åŒæ—¶å°†æ ‡ç‚¹ç¬¦å·å•ç‹¬å¤„ç†   
**UnicodeScriptTokenizerè¿˜æ˜¯ä½¿ç”¨ç©ºç™½ç¬¦è¿›è¡Œåˆ†å‰² ä¸èƒ½åˆ†å‰²æ²¡æœ‰ç©ºç™½ç¬¦çš„æ±‰å­—** ä½†æ˜¯èƒ½å°†æ ‡ç‚¹ç¬¦å·å¤„ç†   
#### Subword tokenizers
subword tokenizerså¯ä»¥å’Œè¾ƒå°çš„è¯æ±‡è¡¨ä¸€èµ·ä½¿ç”¨ï¼Œå…è®¸æ¨¡å‹ä»åˆ›å»ºè¯æ±‡çš„å­è¯ä¸­è·å¾—ä¸€äº›ä¿¡æ¯  
1. WordpieceTokenizer  
WordpieceTokenizeræ˜¯ä¸€ç§æ•°æ®é©±åŠ¨çš„tokenizationæ–¹æ³• å…¶ç”Ÿæˆäº†ä¸€ç»„sub-tokens å’Œè¯­è¨€æœ¬èº«çš„è¯­ç´ æœ‰å…³  
WordpieceTokenizeræœŸæœ›æ¥å—ä¸€ä¸ªåˆ†å‰²åçš„tokensä½œä¸ºè¾“å…¥è¿›è¡Œæ•°æ®é©±åŠ¨ æ‰€ä»¥ä¸€èˆ¬å…ˆç”¨WhiteSpaceTokenizeræˆ–è€…UnicodeScriptTokenizer  
æœ¬è´¨å°±æ˜¯å¯¹vocabæ–‡ä»¶çš„æ˜ å°„  
2. BertTokenizer
BertTokenizerå®ç°äº†BERTè®ºæ–‡ä¸­çš„å®ç°æ–¹æ³•ï¼Œæœ¬è´¨æ˜¯ç”±WordPieceTokenizeræ”¯æŒçš„ ä½†æ˜¯è¿˜æ‰§è¡Œå…¶ä»–ä»»åŠ¡ å¦‚å•è¯è§„èŒƒåŒ–å’Œæ ‡è®°åŒ–  
å€¼å¾—æ³¨æ„çš„æ˜¯BertTokenizerä¸­æœ‰å¯èƒ½å¯¹ä¸€ä¸ªwordè¿›è¡Œåˆ†å‰²åˆ°sub-wordï¼Œä¾‹å¦‚"Average" -> "A" "##ven" "##ger"  
è¿™æ˜¯å› ä¸ºvocabè¡¨ä¸­æœ‰è¯æ ¹çš„æ¦‚å¿µ ##+characters è¡¨ç¤ºäº†ä¸€ä¸ªè¯æ ¹ é‚£ä¹ˆå°±å¯ä»¥å°†ä¸€ä¸ªè¯è¿›è¡Œåˆ†å‰²  
è¾“å…¥æ˜¯[batch, num_tokens(ä¸€ä¸ªå¥å­ä¸­è¯çš„ä¸ªæ•°)]  
è¾“å‡ºæ˜¯[batch, num_tokens(ä¸€ä¸ªå¥å­ä¸­è¯çš„ä¸ªæ•° è¿™ä¸ªå’Œè¾“å…¥å®Œå…¨ç›¸åŒ), num_wordpieces(**è¿™ä¸ªç»´åº¦æ˜¯æ¯ä¸ªwordç”¨åˆ†æˆäº†å‡ ç»´ æœ‰å¯èƒ½æ˜¯1 å³æ²¡æœ‰åˆ†å‰²ä¸ºsub-word ä½†æ˜¯ä¹Ÿä¼šè¢«åˆ†æˆè¯æ ¹ é‚£ä¹ˆå°±æ˜¯å¤šç»´**)]  
éƒ¨åˆ†ä»»åŠ¡éœ€è¦ä¿ç•™num_wordpiecesç»´ï¼Œå¤§éƒ¨åˆ†ä»»åŠ¡ä¸éœ€è¦ï¼Œå°±å¯ä»¥ç›´æ¥å°†å…¶åˆå¹¶ä¸º[batch, new_num_tokens] æ–¹æ³•ä¸ºmerge_dims(-2,-1)  
æœ¬è´¨è¿˜æ˜¯å¯¹vocabæ–‡ä»¶çš„æ˜ å°„(æ³¨æ„è¯æ ¹ ##+å­—æ¯çš„æ¨¡å¼)  
3. SentencepieceTokenizer
SentencepieceTokenizeræ˜¯åŸºäºsentencepieceåº“çš„ è¿™ä¸ªæ–¹æ³•æ˜¯æ ¹æ®è¾“å…¥æ•°æ®å¿«é€Ÿè¿­ä»£çš„ æœ‰å¾ˆæ˜æ˜¾çš„sub-wordçš„æ•ˆæœ è§ML_toolsä»“åº“[ML_tools](https://github.com/MMMMysticY/ML_tools/tree/master/NLP/sentencepiece)
#### å…¶ä»–Tokenizer
1. UnicodeCharTokenizer  
æŒ‰ç…§UTF-8è¿›è¡Œåˆ†éš” å¯¹æ²¡æœ‰ç©ºæ ¼çš„è¯­è¨€å¾ˆæœ‰ç”¨  
æœ¬è´¨å’Œ[word_with_unicode](basic_concepts/work_with_unicode.ipynb)ä¸­ä¸€æ · ç”¨unicodeç¼–å·è¿›è¡Œtokenize  
2. HubModuleTokenizer  
è¿™æ˜¯ä¸€ä¸ªéƒ¨ç½²åœ¨TF Hubä¸Šçš„æ¨¡å‹ å¹¶ä¸æ”¯æŒRaggedTensor è¿™ä¸ªå¯¹æ²¡æœ‰ç©ºæ ¼çš„å¯å‘å¼è¯­è¨€å¾ˆæœ‰æ•ˆæœ  
**è¿™ä¸ªå¾ˆæ£’ï¼https://hub.tensorflow.google.cn/google/zh_segmentation/1 æä¾›äº†ä¸€ä¸ªåº”ç”¨äºä¸­æ–‡çš„æŒ‰ç…§è¯­ä¹‰è¿›è¡Œåˆ†å‰²çš„tokenizer**  
3. SplitMergeTokenizer  
SplitMergeTokenizer å’Œ SplitMergeFromLogitsTokenizer é€šè¿‡å¯¹åˆ†å‰²ç‚¹çš„æ˜¾å¼æä¾›è¿›è¡Œåˆ†å‰²  
SplitMergeTokenizer æä¾›0,1çš„å‘é‡0ä»£è¡¨åˆ†å‰²ç‚¹å³tokençš„å¼€å¤´1ä»£è¡¨ä¸åˆ†éš”  
SplitMergeFromLogitsTokenizer æ˜¯é€šè¿‡å¾—åˆ†è¿›è¡Œåˆ†å‰² ç¬¬ä¸€ç»´å¤§äºç¬¬äºŒç»´åˆ™ä»£è¡¨0 å¦åˆ™ä»£è¡¨1  
4. RegexSplitter  
é€šè¿‡æ­£åˆ™è¡¨è¾¾å¼è¿›è¡Œç¡®å®šåˆ†å‰²ç‚¹  

#### offset
å¦‚æœè¦çŸ¥é“æ¯ä¸ªè¢«åˆ†éš”çš„tokenåœ¨åŸå§‹å¥å­çš„ä½ç½® å°±å¯ä»¥ä½¿ç”¨å‡ ä¹æ‰€æœ‰tokenizerçš„tokenize_with_offsetsæ–¹æ³•  
å·¦é—­å³å¼€çš„  

#### detokenization
tokenizeçš„é€†æ“ä½œ ä½†æ˜¯å¹¶ä¸æ˜¯æ‰€æœ‰çš„tokenizeréƒ½æœ‰è¿™ä¸ªæ–¹æ³•  
åŒæ—¶**tokenizeå’Œdetokenizationå¯èƒ½æ˜¯æœ‰æŸçš„ å¹¶ä¸ä¸€å®šå®Œå…¨è¿˜åŸ**  

#### å¯¹tf.data.Datasetå¯¹è±¡ä½¿ç”¨tokenizer
ä½¿ç”¨map(lambda x: tokenizer.tokenize(x))æ–¹æ³•  

### subword tokenizer
æœ‰å‡ ä¸ªå®ç”¨çš„æ–¹æ³•ï¼š
#### é€šè¿‡tf.data.Datasetç”Ÿæˆbert_vocab
```python
from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab
pt_vocab = bert_vocab.bert_vocab_from_dataset()
```
è§[bert_vocab](pre-processing/subword_tokenizers.ipynb)  
#### åŸºäºvocabç”ŸæˆBertTokenizer
è¿™ä¸ªå’Œä¸Šä¸€éƒ¨åˆ†subwordå°èŠ‚ä¸­BertTokenizerä¸€æ ·  
ä½†æ˜¯å€¼å¾—æ³¨æ„å‡ ä¸ªæ–¹æ³•  
```python
tf.gather(vocab_txt, token_int) # è¿™ä¸ªæ–¹æ³•å¯ä»¥ç®€å•åœ°å°†intå€¼æ˜ å°„åˆ°vocab_txtæ–‡ä»¶ä¸­çš„å­—ç¬¦ä¸Š
tf.strings.reduce_join(text_tokens, separator=' ', axis=-1) # è¿™ä¸ªæ–¹æ³•å¯ä»¥å°†ç»´åº¦å†…çš„å„ä¸ªå€¼åˆå¹¶ ä»¥ç©ºæ ¼åˆ†éš”
(RaggedTensor).merge_dims(-2,-1) # è¿™ä¸ªæ–¹æ³•å¾ˆæœ‰ç”¨ å› ä¸ºBERT tokenizeä¹‹åçš„ç»“æœæ˜¯ä¸€ä¸ªRaggedTensoræ˜¯ [batch, seq_len, N] æœ€åè¿™ä¸ªNå¾ˆå¤šæƒ…å†µä¸‹ç­‰äº1åœ¨æœ‰äº›æƒ…å†µæ²¡æœ‰æ„ä¹‰ å¯ä»¥ç›´æ¥åˆå¹¶

bad_cells = tf.strings.regex_full_match(token_txt, bad_token_re)
result = tf.ragged.boolean_mask(token_txt, ~bad_cells)
# è¿™ä¸ªæ–¹æ³•å¯ä»¥è¿›è¡Œæ­£åˆ™åŒ¹é…åˆ å»ç‰¹å®štokens
```
è¿˜æœ‰CustomTokenizeræ–¹æ³•  
è§[BertTokenizeråç»­å¤„ç†](pre-processing/subword_tokenizers.ipynb)  

### BERT preprocessing
ä½¿ç”¨ä¸°å¯Œçš„tensorflow_textçš„api å®ŒæˆBERTä»»åŠ¡ å³Masked language model + next sentence prediction  
åŸºæœ¬æ–¹æ³•æ˜¯ï¼š
1. Inputæ˜¯tf.stringç±»å‹çš„**Tensor** å…¶ä¸­æœ‰text_a text_b  
   ç»´åº¦ï¼šæ¯ä¸ªtext_aå’Œtext_bçš„ç»´åº¦æ˜¯[batch, 1]  
   æ–¹æ³•ï¼štf.data.Dataset.from_tensors   
2. å°†Inputè¿›è¡ŒTokenizer **æŒ‰ç…§vocabåˆå§‹åŒ–BertTokenizer**  
   ç»´åº¦ï¼šæ¯ä¸ªtext_a text_b ç»´åº¦å˜æˆ[batch, num_words, wordpieces] å³å°†ä¸€ä¸ªå¥å­åˆ†æˆword å†æŠŠwordåˆ†æˆwordpieces  
   æ–¹æ³•ï¼štf.lookup.StaticVocabularyTableåˆå§‹åŒ–lookupå¯¹è±¡ï¼Œ ä½œä¸ºå‚æ•°åˆå§‹åŒ–text.BertTokenizer ä¹‹åè°ƒç”¨tokenizeæ–¹æ³•  
3. æœ¬ä»»åŠ¡æ— éœ€wordpieceså•ç‹¬å¤„ç† æ‰€ä»¥å°†æ¯ä¸ªtext_a text_bæœ€åä¸¤ç»´åˆå¹¶  
   ç»´åº¦ï¼štext_a text_bç»´åº¦å˜æˆ[batch, num_wordpieces] 
   æ–¹æ³•ï¼šmerge_dims(-2,-1)  
4. å°†ä¸¤ä¸ªå¥å­è£å‰ªä¸ºMAX_SEQ_LENä»¥å†…  
   ç»´åº¦ï¼štext_aå˜æˆ[batch, num_wordpieces_a] text_bå˜æˆ[batch, num_wordpiece_b] å…¶ä¸­num_wordpieces_a + num_wordpiece_b <= MAX_SEQ_LEN  
   æ–¹æ³•ï¼štext.RoundRobinTrimmer trimæ–¹æ³•  
5. å°†text_a text_b è¿›è¡Œæ‹¼æ¥ å¹¶åŠ ä¸ŠSOSå’ŒEOS  
   ç»´åº¦ï¼šè¾“å‡ºçš„combined_segmentsæ˜¯text_aå’Œtext_båˆå¹¶çš„æ•´ä½“ç»“æœ [batch, seq_len] seq_len = 3 + num_wordpieces_a + num_wordpiece_b  åŒæ—¶è¿˜æœ‰ä¸€ä¸ªè¾“å‡ºsegment_idsè¿›è¡Œtext_aå’Œtext_bçš„åŒºåˆ†ï¼Œ0ä»£è¡¨text_a 1ä»£è¡¨text_b ç»´åº¦ä¹Ÿæ˜¯[batch, seq_len]  
   æ–¹æ³•ï¼štext.combine_segments  
6. è¿›è¡Œmask éšæœºé€‰æ‹©maskçš„ä½ç½®å’Œmaskçš„value è¿›è¡Œmask  
   ç»´åº¦ï¼šmasked_input_idsæ˜¯combined_segmentsè¿›è¡Œmaskçš„ç»“æœ ç»´åº¦æ˜¯[batch, seq_len] ä¸å˜ å› ä¸ºåªæœ‰maskè¡Œä¸º masked_positionså’Œmasked_idsæ˜¯è¢«maskçš„ä½ç½®å’ŒåŸå§‹çœŸå®çš„idsï¼Œç»´åº¦æ˜¯[batch, masked_len]  
   æ–¹æ³•ï¼šmaskä½ç½®çš„éšæœºé€‰æ‹©å™¨text.RandomItemSelector maskä½ç½®çš„å¤„ç†æ–¹æ³•å€¼çš„é€‰å–text.MaskValuesChooser è¿›è¡Œmaskçš„æ–¹æ³•text.mask_language_model  
7. è¿›è¡Œpadåˆ°MAX_SEQ_LEN å’Œ MAX_PREDICTION_LEN  
   ç»´åº¦ï¼šmasked_input_ids å’Œ segment_ids padåˆ° MAX_SEQ_LEN masked_positionså’Œmasked_ids padåˆ°MAX_PREDICTION_LEN  
   æ–¹æ³•ï¼štext.pad_model_inputs  
8. ç”¨ä¸Šé¢æ‰€æœ‰ä¸œè¥¿ä½œä¸ºinputs

è¯¦ç»†è§[BERT-preprocessing](pre-processing/BERT-preprocessing.ipynb)  

## BERT Experiments
ä½¿ç”¨BERTè¿›è¡ŒåŸºæœ¬çš„fine-tuneä»»åŠ¡çš„æ­¥éª¤æ˜¯ï¼š
1. **è·å–å¤„ç†æ•°æ®é›†** ä¸‹è½½æˆ–è€…åŠ è½½ è¿™ä¸ªè¿‡ç¨‹æœ‰å¾ˆå¤šæŠ€å·§ ä¾‹å¦‚
   - keras.utils.text_dataset_from_directoryå¯ä»¥ç›´æ¥åŠ è½½txtæ–‡æœ¬(è§[text_classification_with_BERT](tensorflow-text/bert_exp/text_classification_with_BERT.ipynb))
   - tensorflow_datasetsåº“tfdsçš„loadæ–¹æ³• å¯ä»¥ä¸‹è½½ä¹Ÿå¯ä»¥ä»æœ¬åœ°åŠ è½½(**ä¸‹è½½åŸºæœ¬åœ¨å›½å†…ä¸è¡Œ æ‰€ä»¥ä¸‹è½½äº†ä¹‹ååŠ è½½æ¯”è¾ƒåˆé€‚**)(è§[Fine tune BERT](tensorflow-text/bert_exp/Fine%20tune%20BERT.ipynb))
   - ä¸‹è½½äº†TFRecordDatasetå¯¹è±¡ä¹‹å ä½¿ç”¨tf.io.FixedLenFeature+tf.io.parse_single_exampleè¿›è¡Œè§£æ(**è¿™ä¸ªä¼¼ä¹åœ¨å¤§è§„æ¨¡æ•°æ®é›†æ¯”è¾ƒåˆé€‚**)(è§[Fine tune BERT](tensorflow-text/bert_exp/Fine%20tune%20BERT.ipynb))
2. **å¤„ç†æ•°æ®é›†** å¤„ç†æ•°æ®é›†æœ‰å¾ˆå¤šæ–¹æ³•
   - ä¸‹è½½tf hubçš„BERTå¯¹åº”çš„preprocessç›´æ¥è¿›è¡Œå¤„ç†bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)(è§[text_classification_with_BERT](tensorflow-text/bert_exp/text_classification_with_BERT.ipynb))
   - ä½¿ç”¨official.nlpåº“çš„æ–¹æ³•bert.tokenization.FullTokenizerå†åŠ ä¸Šä¸€äº›åŸç”Ÿæ–¹æ³•å¦‚RaggedTensor.to_tensor()è¿›è¡Œç®€å•çš„padæˆ–è€…keras.preprocessing.sequence.pad_sequencesè¿›è¡Œpad (è§[Fine tune BERT](tensorflow-text/bert_exp/Fine%20tune%20BERT.ipynb))
3. **æ„å»ºæ¨¡å‹** ä½¿ç”¨BERTé¢„è®­ç»ƒçš„æ¨¡å‹ä½œä¸ºpre-trainéƒ¨åˆ† å½“åšä¸€ä¸ªencoder å…¶ä¼šè¾“å‡ºå››ä¸ªç»“æœ (è§[text_classification_with_BERT](tensorflow-text/bert_exp/text_classification_with_BERT.ipynb))
   - default defaultæ˜¯é»˜è®¤çš„ä¹Ÿå°±æ˜¯pooled_output
   - pooled_output å°†input sequenceè¡¨ç¤ºä¸ºä¸€ä¸ªæ•´ä½“ ç»´åº¦æ˜¯[batch_size, hidden] è¿™æ˜¯æ•´ä¸ªå¥å­çš„ä¸€ä¸ªembedding
   - sequence output è¡¨ç¤ºäº†æ¯ä¸ªtokençš„embedding ç»´åº¦æ˜¯[batch_size, seq_len, hidden] æ˜¯æ¯ä¸ªtokençš„embedding
   - encoder_output è¡¨ç¤ºencoderä¸­æ¯ä¸ªTransformer blocksçš„ä¸­é—´éšå±‚çŠ¶æ€ [block_num, batch_size, seq_len, hidden] block_numä»£è¡¨äº†transformer encoderçš„ä¸ªæ•° encoder_outputs[-1] == sequence_output
  
    é€‰æ‹©åˆé€‚çš„embeddingè¿›è¡Œåç»­ä»»åŠ¡ åç»­ä»»åŠ¡å°±æ˜¯Modelçš„class compile fitç­‰
4. ä¿å­˜æ¨¡å‹ tf.saved_model.saveæˆ–è€…model.save 
5. åŠ è½½æ¨¡å‹ tf.saved_model.load è¿™é‡Œæœ‰ä¸ªé—®é¢˜(å¦‚ä½•å®šä¹‰æˆ–è€…ä¿®æ”¹æ¨¡å‹çš„Inputï¼Œåœ¨[Fine tune BERT](tensorflow-text/bert_exp/Fine%20tune%20BERT.ipynb)é‡åˆ°è¿‡ä¸€äº›é—®é¢˜ saveä¹‹åå†loadç»´åº¦ä¸å¯¹)
6. ä½¿ç”¨nlp.data.classifier_data_lib.TfdsProcessorå’ŒTFRecordè¿›è¡Œå¤§å‹æ•°æ®é›†çš„encoding(ç”±äºç½‘ç»œé—®é¢˜æ²¡åš)(è§[Fine tune BERT](tensorflow-text/bert_exp/Fine%20tune%20BERT.ipynb))
