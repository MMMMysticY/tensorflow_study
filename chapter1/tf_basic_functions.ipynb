{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# tf常用函数\n",
    "1. tf.constant直接创建一个张量\n",
    "2. tf.convert_to_tensor直接从numpy转为tf\n",
    "3. tf.zeros  tf.ones  tf.fill直接创建tensor\n",
    "4. tf.random.normal生成正态分布的随机数 tf.random.truncated_normal生成截断正态分布\n",
    "5. tf.cast强制tensor转换数据类型 tf.reduce_min计算张量维度上元素最小值 tf.reduce_max计算张量维度上元素最大值\n",
    "6. tf.op op有一些基本的数学运算 add subtract multiply divide square pow sqrt matmul 矩阵乘法\n",
    "7. tf.Variable 将变量标记为可训练的 会在反向传播中纪录梯度信息 with tf.GradientTape as tape + tape.gradient结构 可以对一系列运算过程进行求梯度\n",
    "8. tf.one_hot one-hot根据取值映射为索引 对该索引位置置为1 其他部分置为0\n",
    "9. tf.argmax 返回某维度上最大值的索引"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1' # 使用 GPU 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1 5], shape=(2,), dtype=int64)\n",
      "(2,)\n",
      "<dtype: 'int64'>\n"
     ]
    },
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(2,), dtype=int64, numpy=array([1, 5])>"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tf.constant直接创建一个张量\n",
    "a = tf.constant([1,5], dtype=tf.int64)\n",
    "print(a)\n",
    "print(a.shape)\n",
    "print(a.dtype)\n",
    "a"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "tf.Tensor(\n",
      "[[0 8 5]\n",
      " [8 7 5]], shape=(2, 3), dtype=int64)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(2, 3), dtype=int64, numpy=\narray([[0, 8, 5],\n       [8, 7, 5]])>"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert_to_tensor直接从numpy转为tf\n",
    "a_np = np.random.randint(low = 0, high=10, size=(2,3))\n",
    "a_tensor = tf.convert_to_tensor(a_np, dtype=tf.int64)\n",
    "print(type(a_tensor))\n",
    "print(a_tensor)\n",
    "a_tensor"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]], shape=(3, 2), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1.]], shape=(2, 5), dtype=float32)\n",
      "tf.Tensor([[5 5 5]], shape=(1, 3), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# tf.zeros  tf.ones  tf.fill都可以直接创建tensor\n",
    "b = tf.zeros((3,2))\n",
    "print(b)\n",
    "c = tf.ones((2,5))\n",
    "print(c)\n",
    "d = tf.fill((1,3),5)\n",
    "print(d)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0.5992623   0.19809349]\n",
      " [-1.2651374   0.58489203]\n",
      " [-3.0886664  -1.4638785 ]], shape=(3, 2), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[-0.60741556  0.15623452  0.2989469 ]\n",
      " [-0.8096293  -0.76912254 -0.09063936]], shape=(2, 3), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[7.4545097  3.932078   4.3188677 ]\n",
      " [3.05964    1.1418569  2.958293  ]\n",
      " [0.48840642 2.9316854  0.7476938 ]], shape=(3, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# tf.random.normal生成正态分布的随机数 tf.random.truncated_normal生成截断正态分布 能把正态分布的值限制在二倍标准差内\n",
    "# tf.random.uniform生成均匀分布的随机数\n",
    "e = tf.random.normal((3,2))\n",
    "print(e)\n",
    "f = tf.random.truncated_normal((2,3))\n",
    "print(f)\n",
    "g = tf.random.uniform((3,3),0,10)\n",
    "print(g)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([2. 3. 4.], shape=(3,), dtype=float32)\n",
      "tf.Tensor(2, shape=(), dtype=int64)\n",
      "tf.Tensor(4, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# tf.cast强制tensor转换数据类型 tf.reduce_min计算张量维度上元素最小值 tf.reduce_max计算张量维度上元素最大值\n",
    "x1 = tf.constant([2,3,4], dtype=tf.int64)\n",
    "x1_float = tf.cast(x1,dtype=tf.float32)\n",
    "print(x1_float)\n",
    "print(tf.reduce_min(x1))\n",
    "print(tf.reduce_max(x1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-0.5238342  -1.5508617 ]\n",
      " [ 0.04858428 -0.38857266]\n",
      " [-0.38953766 -0.00669476]], shape=(3, 2), dtype=float32)\n",
      "tf.Tensor([-1.037348   -0.16999419 -0.19811621], shape=(3,), dtype=float32)\n",
      "tf.Tensor([-0.8647876 -1.9461292], shape=(2,), dtype=float32)\n",
      "tf.Tensor(-2.810917, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# tf.reduce_op代表对tensor进行op操作\n",
    "x2 = tf.random.normal((3,2))\n",
    "print(x2)                          # x2 是[shape1, shape2] 那么 axis = 0 操作就是在shape1操作 变成[, shape2] axis = 1 操作就是对shape2操作变成[shape1, ]\n",
    "print(tf.reduce_mean(x2, axis=1)) # axis = 1 代表在横向进行操作 保留第一维 所以是横向操作\n",
    "print(tf.reduce_sum(x2, axis=0)) # axis = 0 代表在纵向进行操作 保留第二维 所以是纵向操作\n",
    "print(tf.reduce_sum(x2))        # 不指定axis 代表对所有元素操作"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "# tf.Variable 将变量标记为可训练的 会在反向传播中纪录梯度信息\n",
    "# 这样就可以在反向传播中梯度下降更新了\n",
    "w = tf.Variable(tf.random.normal((3,2)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[6. 6. 6. 6. 6.]\n",
      " [6. 6. 6. 6. 6.]\n",
      " [6. 6. 6. 6. 6.]], shape=(3, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# tf.op op有一些基本的数学运算 add subtract multiply divide  维度相同的张量能进行四则运算\n",
    "# square pow sqrt\n",
    "# matmul 矩阵乘法\n",
    "\n",
    "a = tf.ones([3,2])\n",
    "b = tf.fill([2,5], 3.)\n",
    "print(tf.matmul(a, b))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TensorSliceDataset element_spec=(TensorSpec(shape=(), dtype=tf.int32, name=None), TensorSpec(shape=(), dtype=tf.int32, name=None))>\n",
      "(<tf.Tensor: shape=(), dtype=int32, numpy=12>, <tf.Tensor: shape=(), dtype=int32, numpy=0>)\n",
      "(<tf.Tensor: shape=(), dtype=int32, numpy=23>, <tf.Tensor: shape=(), dtype=int32, numpy=1>)\n",
      "(<tf.Tensor: shape=(), dtype=int32, numpy=10>, <tf.Tensor: shape=(), dtype=int32, numpy=0>)\n",
      "(<tf.Tensor: shape=(), dtype=int32, numpy=17>, <tf.Tensor: shape=(), dtype=int32, numpy=1>)\n"
     ]
    }
   ],
   "source": [
    "# tf.data.Dataset.from_tensor_slices((feature, labels))\n",
    "features = tf.constant([12,23,10,17])\n",
    "labels = tf.constant([0,1,0,1])\n",
    "dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "print(dataset)\n",
    "for element in dataset:\n",
    "    print(element)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(26.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# with tf.GradientTape as tape + tape.gradient结构 可以对一系列运算过程进行求梯度\n",
    "with tf.GradientTape() as tape:\n",
    "    w = tf.Variable(tf.constant(3.0))\n",
    "    w = tf.add(w,tf.Variable(tf.constant(10.0)))\n",
    "    loss = tf.pow(w,2)\n",
    "\n",
    "grad = tape.gradient(loss,w) # tape.gradient求target对source的导数\n",
    "print(grad)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]], shape=(5, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# tf.one_hot one-hot根据取值映射为索引 对该索引位置置为1 其他部分置为0\n",
    "labels = tf.constant([1,0,2,-1,5])\n",
    "output = tf.one_hot(labels,depth=5)\n",
    "print(output)\n",
    "# 默认axis = -1 则输出的维度是 (len(indices), depth)\n",
    "# depth代表了可取的下标范围[0,..., depth-1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0.22107945 0.10134416 0.67757636], shape=(3,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# tf.nn.softmax\n",
    "score = tf.constant([2.31, 1.53, 3.43])\n",
    "prob = tf.nn.softmax(score)\n",
    "print(prob)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=() dtype=int32, numpy=3>\n"
     ]
    }
   ],
   "source": [
    "# tf.assign_sub(自减值)\n",
    "w = tf.Variable(4)\n",
    "w.assign_sub(1)\n",
    "print(w)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 3)\n",
      "tf.Tensor([3 3 3], shape=(3,), dtype=int64)\n",
      "tf.Tensor([2 2 2 2], shape=(4,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# tf.argmax 返回某维度上最大值的索引\n",
    "test_mat = tf.constant([[1,2,3], [2,3,4], [2,3,5], [10,11,12]])\n",
    "print(test_mat.shape)\n",
    "# [batch, feature]\n",
    "print(tf.argmax(input = test_mat, axis = 0))\n",
    "print(tf.argmax(input=test_mat, axis=1))\n",
    "# axis = 1就是feature维 在feature维进行argmax feature维度消失 其他维度不变"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}