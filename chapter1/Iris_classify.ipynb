{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# 进行鸢尾花分类任务"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.datasets import load_iris\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1' # 使用 GPU 1\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "# if gpus:\n",
    "#   try:\n",
    "#     tf.config.experimental.set_virtual_device_configuration(\n",
    "#         gpus[0],[tf.config.experimental.VirtualDeviceConfiguration(memory_limit=5120)])\n",
    "#   except RuntimeError as e:\n",
    "#     print(e)\n",
    "physical_devices = tf.config.list_physical_devices(\"GPU\")\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0],True)\n",
    "logical_devices = tf.config.list_logical_devices(\"GPU\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# 载入数据\n",
    "x_data = load_iris().data\n",
    "y_data = load_iris().target"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_shape:  (150, 4)\n",
      "y_shape:  (150,)\n"
     ]
    }
   ],
   "source": [
    "print('x_shape: ', x_data.shape)\n",
    "print('y_shape: ', y_data.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "  feature1 feature2 feature3 feature4 label\n0      5.1      3.5      1.4      0.2     0\n1      4.9      3.0      1.4      0.2     0\n2      4.7      3.2      1.3      0.2     0\n3      4.6      3.1      1.5      0.2     0\n4      5.0      3.6      1.4      0.2     0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead tr th {\n        text-align: left;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr>\n      <th></th>\n      <th>feature1</th>\n      <th>feature2</th>\n      <th>feature3</th>\n      <th>feature4</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5.1</td>\n      <td>3.5</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4.9</td>\n      <td>3.0</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4.7</td>\n      <td>3.2</td>\n      <td>1.3</td>\n      <td>0.2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4.6</td>\n      <td>3.1</td>\n      <td>1.5</td>\n      <td>0.2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5.0</td>\n      <td>3.6</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_pd = pd.DataFrame(x_data, columns=[['feature1', 'feature2', 'feature3', 'feature4']])\n",
    "data_pd['label'] = y_data\n",
    "data_pd.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "         feature1    feature2    feature3    feature4       label\ncount  150.000000  150.000000  150.000000  150.000000  150.000000\nmean     5.843333    3.057333    3.758000    1.199333    1.000000\nstd      0.828066    0.435866    1.765298    0.762238    0.819232\nmin      4.300000    2.000000    1.000000    0.100000    0.000000\n25%      5.100000    2.800000    1.600000    0.300000    0.000000\n50%      5.800000    3.000000    4.350000    1.300000    1.000000\n75%      6.400000    3.300000    5.100000    1.800000    2.000000\nmax      7.900000    4.400000    6.900000    2.500000    2.000000",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead tr th {\n        text-align: left;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr>\n      <th></th>\n      <th>feature1</th>\n      <th>feature2</th>\n      <th>feature3</th>\n      <th>feature4</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>150.000000</td>\n      <td>150.000000</td>\n      <td>150.000000</td>\n      <td>150.000000</td>\n      <td>150.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>5.843333</td>\n      <td>3.057333</td>\n      <td>3.758000</td>\n      <td>1.199333</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.828066</td>\n      <td>0.435866</td>\n      <td>1.765298</td>\n      <td>0.762238</td>\n      <td>0.819232</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>4.300000</td>\n      <td>2.000000</td>\n      <td>1.000000</td>\n      <td>0.100000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>5.100000</td>\n      <td>2.800000</td>\n      <td>1.600000</td>\n      <td>0.300000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>5.800000</td>\n      <td>3.000000</td>\n      <td>4.350000</td>\n      <td>1.300000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>6.400000</td>\n      <td>3.300000</td>\n      <td>5.100000</td>\n      <td>1.800000</td>\n      <td>2.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>7.900000</td>\n      <td>4.400000</td>\n      <td>6.900000</td>\n      <td>2.500000</td>\n      <td>2.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_pd.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# 数据集乱序\n",
    "np.random.seed(66)\n",
    "np.random.shuffle(x_data)\n",
    "np.random.seed(66)\n",
    "np.random.shuffle(y_data)\n",
    "tf.random.set_seed(66)\n",
    "# seed相同保证了shuffle过后x和y仍然一一对应"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# 划分数据集\n",
    "x_train = x_data[:-30]\n",
    "y_train = y_data[:-30]\n",
    "x_test = x_data[-30:]\n",
    "y_test = y_data[-30:]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[6.2, 2.9, 4.3, 1.3],\n       [6.8, 2.8, 4.8, 1.4],\n       [6.5, 2.8, 4.6, 1.5],\n       [5.7, 3.8, 1.7, 0.3],\n       [6.6, 2.9, 4.6, 1.3],\n       [5.7, 3. , 4.2, 1.2],\n       [5.1, 3.5, 1.4, 0.2],\n       [5.5, 4.2, 1.4, 0.2],\n       [4.3, 3. , 1.1, 0.1],\n       [7.2, 3. , 5.8, 1.6],\n       [6.5, 3. , 5.5, 1.8],\n       [6.9, 3.1, 5.1, 2.3],\n       [5. , 3.3, 1.4, 0.2],\n       [6. , 3. , 4.8, 1.8],\n       [5.8, 2.7, 5.1, 1.9],\n       [4.4, 2.9, 1.4, 0.2],\n       [5.5, 2.4, 3.8, 1.1],\n       [6.3, 2.5, 4.9, 1.5],\n       [7.3, 2.9, 6.3, 1.8],\n       [7.6, 3. , 6.6, 2.1],\n       [5. , 3.6, 1.4, 0.2],\n       [6.1, 2.8, 4.7, 1.2],\n       [6.1, 2.9, 4.7, 1.4],\n       [6.1, 3. , 4.9, 1.8],\n       [5.6, 3. , 4.5, 1.5],\n       [7.2, 3.6, 6.1, 2.5],\n       [5.1, 3.8, 1.5, 0.3],\n       [4.5, 2.3, 1.3, 0.3],\n       [6.1, 2.6, 5.6, 1.4],\n       [6.5, 3. , 5.2, 2. ],\n       [5. , 3. , 1.6, 0.2],\n       [5.4, 3.9, 1.7, 0.4],\n       [6.3, 2.9, 5.6, 1.8],\n       [5.1, 3.8, 1.6, 0.2],\n       [5.4, 3. , 4.5, 1.5],\n       [5. , 3.4, 1.6, 0.4],\n       [5.9, 3. , 4.2, 1.5],\n       [5.3, 3.7, 1.5, 0.2],\n       [6.7, 3. , 5. , 1.7],\n       [5.7, 2.8, 4.1, 1.3],\n       [5.1, 3.7, 1.5, 0.4],\n       [6.9, 3.1, 4.9, 1.5],\n       [6.5, 3. , 5.8, 2.2],\n       [5.8, 2.7, 3.9, 1.2],\n       [6.3, 3.3, 4.7, 1.6],\n       [5.6, 2.5, 3.9, 1.1],\n       [5.8, 4. , 1.2, 0.2],\n       [6. , 3.4, 4.5, 1.6],\n       [6.2, 2.2, 4.5, 1.5],\n       [6.7, 3.1, 5.6, 2.4],\n       [6.4, 3.2, 5.3, 2.3],\n       [4.9, 3.6, 1.4, 0.1],\n       [5.8, 2.7, 5.1, 1.9],\n       [6.2, 2.8, 4.8, 1.8],\n       [6.7, 3.3, 5.7, 2.5],\n       [7.9, 3.8, 6.4, 2. ],\n       [4.6, 3.6, 1. , 0.2],\n       [7.4, 2.8, 6.1, 1.9],\n       [4.6, 3.2, 1.4, 0.2],\n       [7.7, 3. , 6.1, 2.3],\n       [4.6, 3.1, 1.5, 0.2],\n       [5. , 3.5, 1.6, 0.6],\n       [6.3, 2.7, 4.9, 1.8],\n       [5.2, 3.4, 1.4, 0.2],\n       [4.4, 3. , 1.3, 0.2],\n       [7.7, 2.6, 6.9, 2.3],\n       [5.5, 2.4, 3.7, 1. ],\n       [4.7, 3.2, 1.6, 0.2],\n       [4.9, 3. , 1.4, 0.2],\n       [6.5, 3.2, 5.1, 2. ],\n       [6.4, 2.7, 5.3, 1.9],\n       [4.8, 3.1, 1.6, 0.2],\n       [4.8, 3. , 1.4, 0.3],\n       [6.4, 2.8, 5.6, 2.1],\n       [5.7, 4.4, 1.5, 0.4],\n       [5.6, 3. , 4.1, 1.3],\n       [5.8, 2.7, 4.1, 1. ],\n       [6. , 2.2, 4. , 1. ],\n       [6.3, 3.3, 6. , 2.5],\n       [5.1, 3.3, 1.7, 0.5],\n       [4.8, 3.4, 1.9, 0.2],\n       [5.9, 3. , 5.1, 1.8],\n       [6.9, 3.1, 5.4, 2.1],\n       [6.7, 3.3, 5.7, 2.1],\n       [5. , 2.3, 3.3, 1. ],\n       [5.7, 2.9, 4.2, 1.3],\n       [5.5, 2.5, 4. , 1.3],\n       [5.4, 3.4, 1.5, 0.4],\n       [5.2, 2.7, 3.9, 1.4],\n       [7. , 3.2, 4.7, 1.4],\n       [6.4, 3.1, 5.5, 1.8],\n       [5.5, 3.5, 1.3, 0.2],\n       [7.1, 3. , 5.9, 2.1],\n       [5.4, 3.7, 1.5, 0.2],\n       [6.1, 3. , 4.6, 1.4],\n       [4.8, 3.4, 1.6, 0.2],\n       [6.8, 3. , 5.5, 2.1],\n       [5.7, 2.8, 4.5, 1.3],\n       [6.7, 3.1, 4.4, 1.4],\n       [7.2, 3.2, 6. , 1.8],\n       [6.7, 2.5, 5.8, 1.8],\n       [5.1, 2.5, 3. , 1.1],\n       [6.7, 3.1, 4.7, 1.5],\n       [5.1, 3.5, 1.4, 0.3],\n       [4.9, 2.5, 4.5, 1.7],\n       [5.5, 2.3, 4. , 1.3],\n       [5.6, 2.7, 4.2, 1.3],\n       [5. , 3.4, 1.5, 0.2],\n       [5.6, 2.8, 4.9, 2. ],\n       [5.7, 2.5, 5. , 2. ],\n       [5.4, 3.9, 1.3, 0.4],\n       [6. , 2.7, 5.1, 1.6],\n       [5.9, 3.2, 4.8, 1.8],\n       [6.3, 2.8, 5.1, 1.5],\n       [7.7, 3.8, 6.7, 2.2],\n       [6.3, 2.3, 4.4, 1.3],\n       [6. , 2.9, 4.5, 1.5],\n       [4.4, 3.2, 1.3, 0.2],\n       [6.2, 3.4, 5.4, 2.3],\n       [5.6, 2.9, 3.6, 1.3]])"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "mm = MinMaxScaler()\n",
    "mm.fit(x_train)\n",
    "x_train = mm.transform(x_train)\n",
    "x_test = mm.transform(x_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.52777778, 0.31818182, 0.55932203, 0.5       ],\n       [0.69444444, 0.27272727, 0.6440678 , 0.54166667],\n       [0.61111111, 0.27272727, 0.61016949, 0.58333333],\n       [0.38888889, 0.72727273, 0.11864407, 0.08333333],\n       [0.63888889, 0.31818182, 0.61016949, 0.5       ],\n       [0.38888889, 0.36363636, 0.54237288, 0.45833333],\n       [0.22222222, 0.59090909, 0.06779661, 0.04166667],\n       [0.33333333, 0.90909091, 0.06779661, 0.04166667],\n       [0.        , 0.36363636, 0.01694915, 0.        ],\n       [0.80555556, 0.36363636, 0.81355932, 0.625     ],\n       [0.61111111, 0.36363636, 0.76271186, 0.70833333],\n       [0.72222222, 0.40909091, 0.69491525, 0.91666667],\n       [0.19444444, 0.5       , 0.06779661, 0.04166667],\n       [0.47222222, 0.36363636, 0.6440678 , 0.70833333],\n       [0.41666667, 0.22727273, 0.69491525, 0.75      ],\n       [0.02777778, 0.31818182, 0.06779661, 0.04166667],\n       [0.33333333, 0.09090909, 0.47457627, 0.41666667],\n       [0.55555556, 0.13636364, 0.66101695, 0.58333333],\n       [0.83333333, 0.31818182, 0.89830508, 0.70833333],\n       [0.91666667, 0.36363636, 0.94915254, 0.83333333],\n       [0.19444444, 0.63636364, 0.06779661, 0.04166667],\n       [0.5       , 0.27272727, 0.62711864, 0.45833333],\n       [0.5       , 0.31818182, 0.62711864, 0.54166667],\n       [0.5       , 0.36363636, 0.66101695, 0.70833333],\n       [0.36111111, 0.36363636, 0.59322034, 0.58333333],\n       [0.80555556, 0.63636364, 0.86440678, 1.        ],\n       [0.22222222, 0.72727273, 0.08474576, 0.08333333],\n       [0.05555556, 0.04545455, 0.05084746, 0.08333333],\n       [0.5       , 0.18181818, 0.77966102, 0.54166667],\n       [0.61111111, 0.36363636, 0.71186441, 0.79166667],\n       [0.19444444, 0.36363636, 0.10169492, 0.04166667],\n       [0.30555556, 0.77272727, 0.11864407, 0.125     ],\n       [0.55555556, 0.31818182, 0.77966102, 0.70833333],\n       [0.22222222, 0.72727273, 0.10169492, 0.04166667],\n       [0.30555556, 0.36363636, 0.59322034, 0.58333333],\n       [0.19444444, 0.54545455, 0.10169492, 0.125     ],\n       [0.44444444, 0.36363636, 0.54237288, 0.58333333],\n       [0.27777778, 0.68181818, 0.08474576, 0.04166667],\n       [0.66666667, 0.36363636, 0.6779661 , 0.66666667],\n       [0.38888889, 0.27272727, 0.52542373, 0.5       ],\n       [0.22222222, 0.68181818, 0.08474576, 0.125     ],\n       [0.72222222, 0.40909091, 0.66101695, 0.58333333],\n       [0.61111111, 0.36363636, 0.81355932, 0.875     ],\n       [0.41666667, 0.22727273, 0.49152542, 0.45833333],\n       [0.55555556, 0.5       , 0.62711864, 0.625     ],\n       [0.36111111, 0.13636364, 0.49152542, 0.41666667],\n       [0.41666667, 0.81818182, 0.03389831, 0.04166667],\n       [0.47222222, 0.54545455, 0.59322034, 0.625     ],\n       [0.52777778, 0.        , 0.59322034, 0.58333333],\n       [0.66666667, 0.40909091, 0.77966102, 0.95833333],\n       [0.58333333, 0.45454545, 0.72881356, 0.91666667],\n       [0.16666667, 0.63636364, 0.06779661, 0.        ],\n       [0.41666667, 0.22727273, 0.69491525, 0.75      ],\n       [0.52777778, 0.27272727, 0.6440678 , 0.70833333],\n       [0.66666667, 0.5       , 0.79661017, 1.        ],\n       [1.        , 0.72727273, 0.91525424, 0.79166667],\n       [0.08333333, 0.63636364, 0.        , 0.04166667],\n       [0.86111111, 0.27272727, 0.86440678, 0.75      ],\n       [0.08333333, 0.45454545, 0.06779661, 0.04166667],\n       [0.94444444, 0.36363636, 0.86440678, 0.91666667],\n       [0.08333333, 0.40909091, 0.08474576, 0.04166667],\n       [0.19444444, 0.59090909, 0.10169492, 0.20833333],\n       [0.55555556, 0.22727273, 0.66101695, 0.70833333],\n       [0.25      , 0.54545455, 0.06779661, 0.04166667],\n       [0.02777778, 0.36363636, 0.05084746, 0.04166667],\n       [0.94444444, 0.18181818, 1.        , 0.91666667],\n       [0.33333333, 0.09090909, 0.45762712, 0.375     ],\n       [0.11111111, 0.45454545, 0.10169492, 0.04166667],\n       [0.16666667, 0.36363636, 0.06779661, 0.04166667],\n       [0.61111111, 0.45454545, 0.69491525, 0.79166667],\n       [0.58333333, 0.22727273, 0.72881356, 0.75      ],\n       [0.13888889, 0.40909091, 0.10169492, 0.04166667],\n       [0.13888889, 0.36363636, 0.06779661, 0.08333333],\n       [0.58333333, 0.27272727, 0.77966102, 0.83333333],\n       [0.38888889, 1.        , 0.08474576, 0.125     ],\n       [0.36111111, 0.36363636, 0.52542373, 0.5       ],\n       [0.41666667, 0.22727273, 0.52542373, 0.375     ],\n       [0.47222222, 0.        , 0.50847458, 0.375     ],\n       [0.55555556, 0.5       , 0.84745763, 1.        ],\n       [0.22222222, 0.5       , 0.11864407, 0.16666667],\n       [0.13888889, 0.54545455, 0.15254237, 0.04166667],\n       [0.44444444, 0.36363636, 0.69491525, 0.70833333],\n       [0.72222222, 0.40909091, 0.74576271, 0.83333333],\n       [0.66666667, 0.5       , 0.79661017, 0.83333333],\n       [0.19444444, 0.04545455, 0.38983051, 0.375     ],\n       [0.38888889, 0.31818182, 0.54237288, 0.5       ],\n       [0.33333333, 0.13636364, 0.50847458, 0.5       ],\n       [0.30555556, 0.54545455, 0.08474576, 0.125     ],\n       [0.25      , 0.22727273, 0.49152542, 0.54166667],\n       [0.75      , 0.45454545, 0.62711864, 0.54166667],\n       [0.58333333, 0.40909091, 0.76271186, 0.70833333],\n       [0.33333333, 0.59090909, 0.05084746, 0.04166667],\n       [0.77777778, 0.36363636, 0.83050847, 0.83333333],\n       [0.30555556, 0.68181818, 0.08474576, 0.04166667],\n       [0.5       , 0.36363636, 0.61016949, 0.54166667],\n       [0.13888889, 0.54545455, 0.10169492, 0.04166667],\n       [0.69444444, 0.36363636, 0.76271186, 0.83333333],\n       [0.38888889, 0.27272727, 0.59322034, 0.5       ],\n       [0.66666667, 0.40909091, 0.57627119, 0.54166667],\n       [0.80555556, 0.45454545, 0.84745763, 0.70833333],\n       [0.66666667, 0.13636364, 0.81355932, 0.70833333],\n       [0.22222222, 0.13636364, 0.33898305, 0.41666667],\n       [0.66666667, 0.40909091, 0.62711864, 0.58333333],\n       [0.22222222, 0.59090909, 0.06779661, 0.08333333],\n       [0.16666667, 0.13636364, 0.59322034, 0.66666667],\n       [0.33333333, 0.04545455, 0.50847458, 0.5       ],\n       [0.36111111, 0.22727273, 0.54237288, 0.5       ],\n       [0.19444444, 0.54545455, 0.08474576, 0.04166667],\n       [0.36111111, 0.27272727, 0.66101695, 0.79166667],\n       [0.38888889, 0.13636364, 0.6779661 , 0.79166667],\n       [0.30555556, 0.77272727, 0.05084746, 0.125     ],\n       [0.47222222, 0.22727273, 0.69491525, 0.625     ],\n       [0.44444444, 0.45454545, 0.6440678 , 0.70833333],\n       [0.55555556, 0.27272727, 0.69491525, 0.58333333],\n       [0.94444444, 0.72727273, 0.96610169, 0.875     ],\n       [0.55555556, 0.04545455, 0.57627119, 0.5       ],\n       [0.47222222, 0.31818182, 0.59322034, 0.58333333],\n       [0.02777778, 0.45454545, 0.05084746, 0.04166667],\n       [0.52777778, 0.54545455, 0.74576271, 0.91666667],\n       [0.36111111, 0.31818182, 0.44067797, 0.5       ]])"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "array([1, 1, 1, 0, 1, 1, 0, 0, 0, 2, 2, 2, 0, 2, 2, 0, 1, 1, 2, 2, 0, 1,\n       1, 2, 1, 2, 0, 0, 2, 2, 0, 0, 2, 0, 1, 0, 1, 0, 1, 1, 0, 1, 2, 1,\n       1, 1, 0, 1, 1, 2, 2, 0, 2, 2, 2, 2, 0, 2, 0, 2, 0, 0, 2, 0, 0, 2,\n       1, 0, 0, 2, 2, 0, 0, 2, 0, 1, 1, 1, 2, 0, 0, 2, 2, 2, 1, 1, 1, 0,\n       1, 1, 2, 0, 2, 0, 1, 0, 2, 1, 1, 2, 2, 1, 1, 0, 2, 1, 1, 0, 2, 2,\n       0, 1, 1, 2, 2, 1, 1, 0, 2, 1])"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 4)\n",
      "(32,)\n"
     ]
    }
   ],
   "source": [
    "for step, (x,y) in enumerate(train_dataset):\n",
    "    print(x.shape)\n",
    "    print(y.shape)\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "w1 = tf.Variable(tf.random.truncated_normal([4, 3], stddev=0.1, dtype=tf.float64), dtype=tf.float64)\n",
    "b1 = tf.Variable(tf.random.truncated_normal([3], stddev=0.1, dtype=tf.float64), dtype=tf.float64)\n",
    "# 就是一个y = w1*x + b1的拟合 x维度是[batch, feature = 4]\n",
    "# 与w运算之后为[batch, 3]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "EPOCH = 500\n",
    "LR = 0.1\n",
    "train_loss_results = list()\n",
    "test_acc = list()\n",
    "total_loss = 0.0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 343, loss: 0.7205855410507238\n",
      "epoch: 344, loss: 0.7203114857798766\n",
      "epoch: 345, loss: 0.7200376320278203\n",
      "epoch: 346, loss: 0.7197639799385656\n",
      "epoch: 347, loss: 0.7194905296561316\n",
      "epoch: 348, loss: 0.7192172813245205\n",
      "epoch: 349, loss: 0.7189442350876963\n",
      "epoch: 350, loss: 0.7186713910895624\n",
      "epoch: 351, loss: 0.7183987494739387\n",
      "epoch: 352, loss: 0.7181263103845414\n",
      "epoch: 353, loss: 0.7178540739649603\n",
      "epoch: 354, loss: 0.7175820403586384\n",
      "epoch: 355, loss: 0.7173102097088513\n",
      "epoch: 356, loss: 0.7170385821586862\n",
      "epoch: 357, loss: 0.7167671578510222\n",
      "epoch: 358, loss: 0.7164959369285099\n",
      "epoch: 359, loss: 0.716224919533552\n",
      "epoch: 360, loss: 0.7159541058082848\n",
      "epoch: 361, loss: 0.7156834958945574\n",
      "epoch: 362, loss: 0.7154130899339141\n",
      "epoch: 363, loss: 0.7151428880675762\n",
      "epoch: 364, loss: 0.7148728904364228\n",
      "epoch: 365, loss: 0.7146030971809734\n",
      "epoch: 366, loss: 0.71433350844137\n",
      "epoch: 367, loss: 0.7140641243573599\n",
      "epoch: 368, loss: 0.7137949450682785\n",
      "epoch: 369, loss: 0.7135259707130316\n",
      "epoch: 370, loss: 0.7132572014300802\n",
      "epoch: 371, loss: 0.7129886373574226\n",
      "epoch: 372, loss: 0.7127202786325791\n",
      "epoch: 373, loss: 0.712452125392576\n",
      "epoch: 374, loss: 0.7121841777739297\n",
      "epoch: 375, loss: 0.7119164359126312\n",
      "epoch: 376, loss: 0.7116488999441314\n",
      "epoch: 377, loss: 0.7113815700033251\n",
      "epoch: 378, loss: 0.7111144462245381\n",
      "epoch: 379, loss: 0.7108475287415106\n",
      "epoch: 380, loss: 0.7105808176873845\n",
      "epoch: 381, loss: 0.7103143131946883\n",
      "epoch: 382, loss: 0.7100480153953244\n",
      "epoch: 383, loss: 0.7097819244205544\n",
      "epoch: 384, loss: 0.7095160404009867\n",
      "epoch: 385, loss: 0.7092503634665627\n",
      "epoch: 386, loss: 0.7089848937465438\n",
      "epoch: 387, loss: 0.7087196313694996\n",
      "epoch: 388, loss: 0.7084545764632943\n",
      "epoch: 389, loss: 0.7081897291550752\n",
      "epoch: 390, loss: 0.70792508957126\n",
      "epoch: 391, loss: 0.7076606578375255\n",
      "epoch: 392, loss: 0.7073964340787957\n",
      "epoch: 393, loss: 0.7071324184192301\n",
      "epoch: 394, loss: 0.7068686109822131\n",
      "epoch: 395, loss: 0.7066050118903419\n",
      "epoch: 396, loss: 0.7063416212654164\n",
      "epoch: 397, loss: 0.7060784392284287\n",
      "epoch: 398, loss: 0.7058154658995518\n",
      "epoch: 399, loss: 0.7055527013981298\n",
      "epoch: 400, loss: 0.705290145842668\n",
      "epoch: 401, loss: 0.7050277993508225\n",
      "epoch: 402, loss: 0.7047656620393907\n",
      "epoch: 403, loss: 0.704503734024302\n",
      "epoch: 404, loss: 0.7042420154206076\n",
      "epoch: 405, loss: 0.7039805063424724\n",
      "epoch: 406, loss: 0.7037192069031653\n",
      "epoch: 407, loss: 0.7034581172150501\n",
      "epoch: 408, loss: 0.7031972373895777\n",
      "epoch: 409, loss: 0.7029365675372771\n",
      "epoch: 410, loss: 0.7026761077677467\n",
      "epoch: 411, loss: 0.7024158581896469\n",
      "epoch: 412, loss: 0.7021558189106918\n",
      "epoch: 413, loss: 0.7018959900376408\n",
      "epoch: 414, loss: 0.7016363716762919\n",
      "epoch: 415, loss: 0.7013769639314739\n",
      "epoch: 416, loss: 0.7011177669070381\n",
      "epoch: 417, loss: 0.7008587807058527\n",
      "epoch: 418, loss: 0.7006000054297945\n",
      "epoch: 419, loss: 0.7003414411797428\n",
      "epoch: 420, loss: 0.700083088055572\n",
      "epoch: 421, loss: 0.6998249461561455\n",
      "epoch: 422, loss: 0.6995670155793094\n",
      "epoch: 423, loss: 0.6993092964218859\n",
      "epoch: 424, loss: 0.6990517887796669\n",
      "epoch: 425, loss: 0.6987944927474085\n",
      "epoch: 426, loss: 0.6985374084188257\n",
      "epoch: 427, loss: 0.6982805358865849\n",
      "epoch: 428, loss: 0.6980238752423001\n",
      "epoch: 429, loss: 0.697767426576527\n",
      "epoch: 430, loss: 0.6975111899787572\n",
      "epoch: 431, loss: 0.6972551655374134\n",
      "epoch: 432, loss: 0.6969993533398445\n",
      "epoch: 433, loss: 0.6967437534723209\n",
      "epoch: 434, loss: 0.6964883660200288\n",
      "epoch: 435, loss: 0.6962331910670669\n",
      "epoch: 436, loss: 0.6959782286964409\n",
      "epoch: 437, loss: 0.6957234789900596\n",
      "epoch: 438, loss: 0.6954689420287304\n",
      "epoch: 439, loss: 0.6952146178921558\n",
      "epoch: 440, loss: 0.6949605066589285\n",
      "epoch: 441, loss: 0.694706608406528\n",
      "epoch: 442, loss: 0.6944529232113171\n",
      "epoch: 443, loss: 0.6941994511485379\n",
      "epoch: 444, loss: 0.6939461922923085\n",
      "epoch: 445, loss: 0.6936931467156191\n",
      "epoch: 446, loss: 0.6934403144903297\n",
      "epoch: 447, loss: 0.6931876956871662\n",
      "epoch: 448, loss: 0.6929352903757172\n",
      "epoch: 449, loss: 0.6926830986244317\n",
      "epoch: 450, loss: 0.6924311205006161\n",
      "epoch: 451, loss: 0.6921793560704308\n",
      "epoch: 452, loss: 0.6919278053988885\n",
      "epoch: 453, loss: 0.6916764685498515\n",
      "epoch: 454, loss: 0.6914253455860286\n",
      "epoch: 455, loss: 0.6911744365689738\n",
      "epoch: 456, loss: 0.690923741559083\n",
      "epoch: 457, loss: 0.6906732606155934\n",
      "epoch: 458, loss: 0.6904229937965798\n",
      "epoch: 459, loss: 0.690172941158954\n",
      "epoch: 460, loss: 0.6899231027584627\n",
      "epoch: 461, loss: 0.6896734786496851\n",
      "epoch: 462, loss: 0.6894240688860322\n",
      "epoch: 463, loss: 0.6891748735197449\n",
      "epoch: 464, loss: 0.6889258926018927\n",
      "epoch: 465, loss: 0.6886771261823723\n",
      "epoch: 466, loss: 0.688428574309906\n",
      "epoch: 467, loss: 0.6881802370320417\n",
      "epoch: 468, loss: 0.6879321143951502\n",
      "epoch: 469, loss: 0.687684206444426\n",
      "epoch: 470, loss: 0.6874365132238847\n",
      "epoch: 471, loss: 0.6871890347763638\n",
      "epoch: 472, loss: 0.6869417711435208\n",
      "epoch: 473, loss: 0.6866947223658333\n",
      "epoch: 474, loss: 0.686447888482598\n",
      "epoch: 475, loss: 0.6862012695319308\n",
      "epoch: 476, loss: 0.6859548655507656\n",
      "epoch: 477, loss: 0.6857086765748548\n",
      "epoch: 478, loss: 0.6854627026387687\n",
      "epoch: 479, loss: 0.6852169437758956\n",
      "epoch: 480, loss: 0.6849714000184413\n",
      "epoch: 481, loss: 0.6847260713974292\n",
      "epoch: 482, loss: 0.6844809579427015\n",
      "epoch: 483, loss: 0.6842360596829176\n",
      "epoch: 484, loss: 0.6839913766455553\n",
      "epoch: 485, loss: 0.6837469088569114\n",
      "epoch: 486, loss: 0.6835026563421012\n",
      "epoch: 487, loss: 0.6832586191250599\n",
      "epoch: 488, loss: 0.6830147972285423\n",
      "epoch: 489, loss: 0.6827711906741241\n",
      "epoch: 490, loss: 0.6825277994822017\n",
      "epoch: 491, loss: 0.6822846236719939\n",
      "epoch: 492, loss: 0.6820416632615423\n",
      "epoch: 493, loss: 0.6817989182677113\n",
      "epoch: 494, loss: 0.6815563887061911\n",
      "epoch: 495, loss: 0.6813140745914962\n",
      "epoch: 496, loss: 0.6810719759369682\n",
      "epoch: 497, loss: 0.6808300927547764\n",
      "epoch: 498, loss: 0.6805884250559187\n",
      "epoch: 499, loss: 0.6803469728502229\n",
      "epoch: 0, loss: 0.6794029699105938\n",
      "epoch: 1, loss: 0.6770080809667627\n",
      "epoch: 2, loss: 0.6746347358256525\n",
      "epoch: 3, loss: 0.6722829190650027\n",
      "epoch: 4, loss: 0.6699526046043387\n",
      "epoch: 5, loss: 0.6676437559011946\n",
      "epoch: 6, loss: 0.6653563261898703\n",
      "epoch: 7, loss: 0.6630902587562956\n",
      "epoch: 8, loss: 0.660845487243168\n",
      "epoch: 9, loss: 0.6586219359800819\n",
      "epoch: 10, loss: 0.656419520333867\n",
      "epoch: 11, loss: 0.6542381470748364\n",
      "epoch: 12, loss: 0.6520777147550733\n",
      "epoch: 13, loss: 0.6499381140952954\n",
      "epoch: 14, loss: 0.6478192283772075\n",
      "epoch: 15, loss: 0.6457209338385996\n",
      "epoch: 16, loss: 0.643643100068765\n",
      "epoch: 17, loss: 0.6415855904021068\n",
      "epoch: 18, loss: 0.6395482623080684\n",
      "epoch: 19, loss: 0.6375309677757705\n",
      "epoch: 20, loss: 0.635533553691961\n",
      "epoch: 21, loss: 0.6335558622110908\n",
      "epoch: 22, loss: 0.6315977311165111\n",
      "epoch: 23, loss: 0.6296589941719601\n",
      "epoch: 24, loss: 0.627739481462657\n",
      "epoch: 25, loss: 0.6258390197254597\n",
      "epoch: 26, loss: 0.6239574326676665\n",
      "epoch: 27, loss: 0.6220945412741467\n",
      "epoch: 28, loss: 0.6202501641025955\n",
      "epoch: 29, loss: 0.6184241175667805\n",
      "epoch: 30, loss: 0.6166162162077362\n",
      "epoch: 31, loss: 0.6148262729529227\n",
      "epoch: 32, loss: 0.613054099363431\n",
      "epoch: 33, loss: 0.6112995058693601\n",
      "epoch: 34, loss: 0.6095623019935422\n",
      "epoch: 35, loss: 0.6078422965638268\n",
      "epoch: 36, loss: 0.6061392979141668\n",
      "epoch: 37, loss: 0.6044531140747762\n",
      "epoch: 38, loss: 0.6027835529516502\n",
      "epoch: 39, loss: 0.6011304224957587\n",
      "epoch: 40, loss: 0.5994935308622322\n",
      "epoch: 41, loss: 0.5978726865598765\n",
      "epoch: 42, loss: 0.596267698591354\n",
      "epoch: 43, loss: 0.5946783765843769\n",
      "epoch: 44, loss: 0.5931045309142594\n",
      "epoch: 45, loss: 0.5915459728181744\n",
      "epoch: 46, loss: 0.5900025145014622\n",
      "epoch: 47, loss: 0.5884739692363314\n",
      "epoch: 48, loss: 0.5869601514532903\n",
      "epoch: 49, loss: 0.5854608768256404\n",
      "epoch: 50, loss: 0.583975962347357\n",
      "epoch: 51, loss: 0.5825052264046761\n",
      "epoch: 52, loss: 0.5810484888416937\n",
      "epoch: 53, loss: 0.5796055710202828\n",
      "epoch: 54, loss: 0.5781762958746173\n",
      "epoch: 55, loss: 0.5767604879605857\n",
      "epoch: 56, loss: 0.5753579735003681\n",
      "epoch: 57, loss: 0.5739685804224408\n",
      "epoch: 58, loss: 0.5725921383972585\n",
      "epoch: 59, loss: 0.571228478868863\n",
      "epoch: 60, loss: 0.5698774350826474\n",
      "epoch: 61, loss: 0.5685388421095029\n",
      "epoch: 62, loss: 0.5672125368665627\n",
      "epoch: 63, loss: 0.5658983581347463\n",
      "epoch: 64, loss: 0.5645961465733037\n",
      "epoch: 65, loss: 0.5633057447315437\n",
      "epoch: 66, loss: 0.5620269970579254\n",
      "epoch: 67, loss: 0.5607597499066839\n",
      "epoch: 68, loss: 0.55950385154215\n",
      "epoch: 69, loss: 0.5582591521409193\n",
      "epoch: 70, loss: 0.5570255037920148\n",
      "epoch: 71, loss: 0.5558027604951823\n",
      "epoch: 72, loss: 0.5545907781574498\n",
      "epoch: 73, loss: 0.5533894145880753\n",
      "epoch: 74, loss: 0.5521985294919987\n",
      "epoch: 75, loss: 0.5510179844619136\n",
      "epoch: 76, loss: 0.5498476429690584\n",
      "epoch: 77, loss: 0.5486873703528306\n",
      "epoch: 78, loss: 0.5475370338093154\n",
      "epoch: 79, loss: 0.5463965023788199\n",
      "epoch: 80, loss: 0.5452656469324925\n",
      "epoch: 81, loss: 0.5441443401581084\n",
      "epoch: 82, loss: 0.5430324565450965\n",
      "epoch: 83, loss: 0.5419298723688731\n",
      "epoch: 84, loss: 0.5408364656745529\n",
      "epoch: 85, loss: 0.5397521162600937\n",
      "epoch: 86, loss: 0.5386767056589373\n",
      "epoch: 87, loss: 0.537610117122197\n",
      "epoch: 88, loss: 0.5365522356004445\n",
      "epoch: 89, loss: 0.5355029477251434\n",
      "epoch: 90, loss: 0.5344621417897729\n",
      "epoch: 91, loss: 0.5334297077306861\n",
      "epoch: 92, loss: 0.5324055371077374\n",
      "epoch: 93, loss: 0.5313895230847203\n",
      "epoch: 94, loss: 0.5303815604096462\n",
      "epoch: 95, loss: 0.529381545394897\n",
      "epoch: 96, loss: 0.5283893758972822\n",
      "epoch: 97, loss: 0.5274049512980252\n",
      "epoch: 98, loss: 0.5264281724827073\n",
      "epoch: 99, loss: 0.5254589418211928\n",
      "epoch: 100, loss: 0.5244971631475536\n",
      "epoch: 101, loss: 0.5235427417400205\n",
      "epoch: 102, loss: 0.5225955843009726\n",
      "epoch: 103, loss: 0.5216555989369902\n",
      "epoch: 104, loss: 0.5207226951389783\n",
      "epoch: 105, loss: 0.5197967837623853\n",
      "epoch: 106, loss: 0.5188777770075226\n",
      "epoch: 107, loss: 0.5179655884000032\n",
      "epoch: 108, loss: 0.5170601327713077\n",
      "epoch: 109, loss: 0.5161613262394895\n",
      "epoch: 110, loss: 0.5152690861900291\n",
      "epoch: 111, loss: 0.5143833312568454\n",
      "epoch: 112, loss: 0.5135039813034734\n",
      "epoch: 113, loss: 0.5126309574044138\n",
      "epoch: 114, loss: 0.5117641818266626\n",
      "epoch: 115, loss: 0.5109035780114259\n",
      "epoch: 116, loss: 0.510049070556025\n",
      "epoch: 117, loss: 0.5092005851959958\n",
      "epoch: 118, loss: 0.5083580487873891\n",
      "epoch: 119, loss: 0.5075213892892726\n",
      "epoch: 120, loss: 0.5066905357464386\n",
      "epoch: 121, loss: 0.5058654182723215\n",
      "epoch: 122, loss: 0.505045968032124\n",
      "epoch: 123, loss: 0.5042321172261592\n",
      "epoch: 124, loss: 0.5034237990734038\n",
      "epoch: 125, loss: 0.5026209477952686\n",
      "epoch: 126, loss: 0.5018234985995843\n",
      "epoch: 127, loss: 0.5010313876648052\n",
      "epoch: 128, loss: 0.5002445521244282\n",
      "epoch: 129, loss: 0.49946293005163156\n",
      "epoch: 130, loss: 0.498686460444128\n",
      "epoch: 131, loss: 0.49791508320923616\n",
      "epoch: 132, loss: 0.49714873914916724\n",
      "epoch: 133, loss: 0.4963873699465271\n",
      "epoch: 134, loss: 0.4956309181500326\n",
      "epoch: 135, loss: 0.4948793271604409\n",
      "epoch: 136, loss: 0.49413254121669115\n",
      "epoch: 137, loss: 0.49339050538225593\n",
      "epoch: 138, loss: 0.49265316553170213\n",
      "epoch: 139, loss: 0.4919204683374594\n",
      "epoch: 140, loss: 0.4911923612567938\n",
      "epoch: 141, loss: 0.4904687925189858\n",
      "epoch: 142, loss: 0.48974971111270993\n",
      "epoch: 143, loss: 0.4890350667736145\n",
      "epoch: 144, loss: 0.48832480997209954\n",
      "epoch: 145, loss: 0.48761889190129026\n",
      "epoch: 146, loss: 0.48691726446520484\n",
      "epoch: 147, loss: 0.48621988026711327\n",
      "epoch: 148, loss: 0.48552669259808556\n",
      "epoch: 149, loss: 0.484837655425728\n",
      "epoch: 150, loss: 0.4841527233831028\n",
      "epoch: 151, loss: 0.48347185175783075\n",
      "epoch: 152, loss: 0.4827949964813749\n",
      "epoch: 153, loss: 0.4821221141185007\n",
      "epoch: 154, loss: 0.4814531618569122\n",
      "epoch: 155, loss: 0.4807880974970613\n",
      "epoch: 156, loss: 0.4801268794421275\n",
      "epoch: 157, loss: 0.47946946668816576\n",
      "epoch: 158, loss: 0.4788158188144206\n",
      "epoch: 159, loss: 0.4781658959738031\n",
      "epoch: 160, loss: 0.47751965888352943\n",
      "epoch: 161, loss: 0.47687706881591796\n",
      "epoch: 162, loss: 0.4762380875893422\n",
      "epoch: 163, loss: 0.47560267755933855\n",
      "epoch: 164, loss: 0.4749708016098646\n",
      "epoch: 165, loss: 0.4743424231447074\n",
      "epoch: 166, loss: 0.47371750607903906\n",
      "epoch: 167, loss: 0.4730960148311148\n",
      "epoch: 168, loss: 0.4724779143141167\n",
      "epoch: 169, loss: 0.4718631699281339\n",
      "epoch: 170, loss: 0.47125174755228383\n",
      "epoch: 171, loss: 0.4706436135369665\n",
      "epoch: 172, loss: 0.47003873469625335\n",
      "epoch: 173, loss: 0.4694370783004075\n",
      "epoch: 174, loss: 0.46883861206853206\n",
      "epoch: 175, loss: 0.46824330416134596\n",
      "epoch: 176, loss: 0.4676511231740844\n",
      "epoch: 177, loss: 0.4670620381295221\n",
      "epoch: 178, loss: 0.4664760184711168\n",
      "epoch: 179, loss: 0.46589303405627147\n",
      "epoch: 180, loss: 0.4653130551497133\n",
      "epoch: 181, loss: 0.46473605241698684\n",
      "epoch: 182, loss: 0.46416199691806004\n",
      "epoch: 183, loss: 0.4635908601010411\n",
      "epoch: 184, loss: 0.4630226137960031\n",
      "epoch: 185, loss: 0.4624572302089172\n",
      "epoch: 186, loss: 0.4618946819156893\n",
      "epoch: 187, loss: 0.4613349418563004\n",
      "epoch: 188, loss: 0.46077798332904885\n",
      "epoch: 189, loss: 0.46022377998489156\n",
      "epoch: 190, loss: 0.45967230582188345\n",
      "epoch: 191, loss: 0.45912353517971294\n",
      "epoch: 192, loss: 0.4585774427343316\n",
      "epoch: 193, loss: 0.4580340034926774\n",
      "epoch: 194, loss: 0.4574931927874885\n",
      "epoch: 195, loss: 0.4569549862722062\n",
      "epoch: 196, loss: 0.45641935991596694\n",
      "epoch: 197, loss: 0.45588628999867953\n",
      "epoch: 198, loss: 0.45535575310618814\n",
      "epoch: 199, loss: 0.4548277261255186\n",
      "epoch: 200, loss: 0.45430218624020496\n",
      "epoch: 201, loss: 0.453779110925699\n",
      "epoch: 202, loss: 0.4532584779448567\n",
      "epoch: 203, loss: 0.45274026534350276\n",
      "epoch: 204, loss: 0.45222445144607204\n",
      "epoch: 205, loss: 0.4517110148513246\n",
      "epoch: 206, loss: 0.45119993442813466\n",
      "epoch: 207, loss: 0.45069118931135205\n",
      "epoch: 208, loss: 0.45018475889773407\n",
      "epoch: 209, loss: 0.44968062284194676\n",
      "epoch: 210, loss: 0.4491787610526349\n",
      "epoch: 211, loss: 0.4486791536885585\n",
      "epoch: 212, loss: 0.448181781154796\n",
      "epoch: 213, loss: 0.44768662409901105\n",
      "epoch: 214, loss: 0.44719366340778377\n",
      "epoch: 215, loss: 0.44670288020300364\n",
      "epoch: 216, loss: 0.44621425583832386\n",
      "epoch: 217, loss: 0.44572777189567625\n",
      "epoch: 218, loss: 0.4452434101818445\n",
      "epoch: 219, loss: 0.44476115272509575\n",
      "epoch: 220, loss: 0.44428098177186903\n",
      "epoch: 221, loss: 0.4438028797835196\n",
      "epoch: 222, loss: 0.4433268294331185\n",
      "epoch: 223, loss: 0.44285281360230516\n",
      "epoch: 224, loss: 0.44238081537819385\n",
      "epoch: 225, loss: 0.4419108180503315\n",
      "epoch: 226, loss: 0.44144280510770695\n",
      "epoch: 227, loss: 0.44097676023580995\n",
      "epoch: 228, loss: 0.44051266731373995\n",
      "epoch: 229, loss: 0.4400505104113619\n",
      "epoch: 230, loss: 0.4395902737865116\n",
      "epoch: 231, loss: 0.43913194188224564\n",
      "epoch: 232, loss: 0.4386754993241382\n",
      "epoch: 233, loss: 0.4382209309176225\n",
      "epoch: 234, loss: 0.43776822164537654\n",
      "epoch: 235, loss: 0.4373173566647516\n",
      "epoch: 236, loss: 0.43686832130524456\n",
      "epoch: 237, loss: 0.43642110106601006\n",
      "epoch: 238, loss: 0.43597568161341627\n",
      "epoch: 239, loss: 0.4355320487786378\n",
      "epoch: 240, loss: 0.4350901885552917\n",
      "epoch: 241, loss: 0.4346500870971094\n",
      "epoch: 242, loss: 0.4342117307156485\n",
      "epoch: 243, loss: 0.43377510587804147\n",
      "epoch: 244, loss: 0.43334019920478156\n",
      "epoch: 245, loss: 0.432906997467544\n",
      "epoch: 246, loss: 0.4324754875870441\n",
      "epoch: 247, loss: 0.4320456566309293\n",
      "epoch: 248, loss: 0.4316174918117052\n",
      "epoch: 249, loss: 0.43119098048469673\n",
      "epoch: 250, loss: 0.4307661101460403\n",
      "epoch: 251, loss: 0.4303428684307108\n",
      "epoch: 252, loss: 0.42992124311057817\n",
      "epoch: 253, loss: 0.4295012220924972\n",
      "epoch: 254, loss: 0.4290827934164271\n",
      "epoch: 255, loss: 0.42866594525358126\n",
      "epoch: 256, loss: 0.4282506659046076\n",
      "epoch: 257, loss: 0.4278369437977969\n",
      "epoch: 258, loss: 0.4274247674873212\n",
      "epoch: 259, loss: 0.4270141256514989\n",
      "epoch: 260, loss: 0.4266050070910884\n",
      "epoch: 261, loss: 0.42619740072760914\n",
      "epoch: 262, loss: 0.4257912956016886\n",
      "epoch: 263, loss: 0.42538668087143583\n",
      "epoch: 264, loss: 0.4249835458108413\n",
      "epoch: 265, loss: 0.4245818798082012\n",
      "epoch: 266, loss: 0.4241816723645673\n",
      "epoch: 267, loss: 0.42378291309222105\n",
      "epoch: 268, loss: 0.42338559171317125\n",
      "epoch: 269, loss: 0.4229896980576763\n",
      "epoch: 270, loss: 0.422595222062789\n",
      "epoch: 271, loss: 0.42220215377092396\n",
      "epoch: 272, loss: 0.42181048332844795\n",
      "epoch: 273, loss: 0.42142020098429206\n",
      "epoch: 274, loss: 0.42103129708858533\n",
      "epoch: 275, loss: 0.4206437620913097\n",
      "epoch: 276, loss: 0.42025758654097634\n",
      "epoch: 277, loss: 0.4198727610833215\n",
      "epoch: 278, loss: 0.41948927646002354\n",
      "epoch: 279, loss: 0.41910712350743934\n",
      "epoch: 280, loss: 0.4187262931553599\n",
      "epoch: 281, loss: 0.4183467764257859\n",
      "epoch: 282, loss: 0.4179685644317216\n",
      "epoch: 283, loss: 0.4175916483759867\n",
      "epoch: 284, loss: 0.4172160195500478\n",
      "epoch: 285, loss: 0.4168416693328662\n",
      "epoch: 286, loss: 0.4164685891897645\n",
      "epoch: 287, loss: 0.41609677067130957\n",
      "epoch: 288, loss: 0.41572620541221317\n",
      "epoch: 289, loss: 0.41535688513024893\n",
      "epoch: 290, loss: 0.4149888016251857\n",
      "epoch: 291, loss: 0.41462194677773734\n",
      "epoch: 292, loss: 0.4142563125485278\n",
      "epoch: 293, loss: 0.41389189097707274\n",
      "epoch: 294, loss: 0.41352867418077544\n",
      "epoch: 295, loss: 0.41316665435393857\n",
      "epoch: 296, loss: 0.41280582376679\n",
      "epoch: 297, loss: 0.4124461747645243\n",
      "epoch: 298, loss: 0.41208769976635734\n",
      "epoch: 299, loss: 0.411730391264596\n",
      "epoch: 300, loss: 0.41137424182372107\n",
      "epoch: 301, loss: 0.411019244079484\n",
      "epoch: 302, loss: 0.410665390738017\n",
      "epoch: 303, loss: 0.41031267457495657\n",
      "epoch: 304, loss: 0.4099610884345795\n",
      "epoch: 305, loss: 0.4096106252289521\n",
      "epoch: 306, loss: 0.4092612779370914\n",
      "epoch: 307, loss: 0.4089130396041397\n",
      "epoch: 308, loss: 0.4085659033405496\n",
      "epoch: 309, loss: 0.4082198623212827\n",
      "epoch: 310, loss: 0.4078749097850187\n",
      "epoch: 311, loss: 0.4075310390333769\n",
      "epoch: 312, loss: 0.40718824343014837\n",
      "epoch: 313, loss: 0.4068465164005397\n",
      "epoch: 314, loss: 0.40650585143042783\n",
      "epoch: 315, loss: 0.40616624206562507\n",
      "epoch: 316, loss: 0.4058276819111555\n",
      "epoch: 317, loss: 0.4054901646305409\n",
      "epoch: 318, loss: 0.4051536839450982\n",
      "epoch: 319, loss: 0.4048182336332457\n",
      "epoch: 320, loss: 0.40448380752982005\n",
      "epoch: 321, loss: 0.404150399525403\n",
      "epoch: 322, loss: 0.4038180035656573\n",
      "epoch: 323, loss: 0.40348661365067245\n",
      "epoch: 324, loss: 0.4031562238343198\n",
      "epoch: 325, loss: 0.4028268282236166\n",
      "epoch: 326, loss: 0.40249842097809896\n",
      "epoch: 327, loss: 0.4021709963092041\n",
      "epoch: 328, loss: 0.4018445484796612\n",
      "epoch: 329, loss: 0.40151907180289037\n",
      "epoch: 330, loss: 0.4011945606424108\n",
      "epoch: 331, loss: 0.40087100941125675\n",
      "epoch: 332, loss: 0.4005484125714018\n",
      "epoch: 333, loss: 0.4002267646331912\n",
      "epoch: 334, loss: 0.39990606015478236\n",
      "epoch: 335, loss: 0.39958629374159294\n",
      "epoch: 336, loss: 0.3992674600457564\n",
      "epoch: 337, loss: 0.39894955376558516\n",
      "epoch: 338, loss: 0.398632569645042\n",
      "epoch: 339, loss: 0.3983165024732176\n",
      "epoch: 340, loss: 0.3980013470838164\n",
      "epoch: 341, loss: 0.39768709835464816\n",
      "epoch: 342, loss: 0.39737375120712815\n",
      "epoch: 343, loss: 0.39706130060578293\n",
      "epoch: 344, loss: 0.39674974155776366\n",
      "epoch: 345, loss: 0.3964390691123655\n",
      "epoch: 346, loss: 0.39612927836055384\n",
      "epoch: 347, loss: 0.3958203644344975\n",
      "epoch: 348, loss: 0.39551232250710694\n",
      "epoch: 349, loss: 0.39520514779158056\n",
      "epoch: 350, loss: 0.39489883554095506\n",
      "epoch: 351, loss: 0.3945933810476638\n",
      "epoch: 352, loss: 0.39428877964309994\n",
      "epoch: 353, loss: 0.393985026697186\n",
      "epoch: 354, loss: 0.3936821176179486\n",
      "epoch: 355, loss: 0.39338004785109976\n",
      "epoch: 356, loss: 0.3930788128796234\n",
      "epoch: 357, loss: 0.39277840822336674\n",
      "epoch: 358, loss: 0.39247882943863843\n",
      "epoch: 359, loss: 0.3921800721178112\n",
      "epoch: 360, loss: 0.3918821318889293\n",
      "epoch: 361, loss: 0.3915850044153225\n",
      "epoch: 362, loss: 0.39128868539522443\n",
      "epoch: 363, loss: 0.3909931705613954\n",
      "epoch: 364, loss: 0.39069845568075146\n",
      "epoch: 365, loss: 0.39040453655399726\n",
      "epoch: 366, loss: 0.39011140901526425\n",
      "epoch: 367, loss: 0.3898190689317536\n",
      "epoch: 368, loss: 0.3895275122033838\n",
      "epoch: 369, loss: 0.3892367347624421\n",
      "epoch: 370, loss: 0.3889467325732421\n",
      "epoch: 371, loss: 0.3886575016317842\n",
      "epoch: 372, loss: 0.3883690379654213\n",
      "epoch: 373, loss: 0.38808133763252867\n",
      "epoch: 374, loss: 0.38779439672217797\n",
      "epoch: 375, loss: 0.3875082113538154\n",
      "epoch: 376, loss: 0.3872227776769446\n",
      "epoch: 377, loss: 0.3869380918708126\n",
      "epoch: 378, loss: 0.3866541501441007\n",
      "epoch: 379, loss: 0.386370948734619\n",
      "epoch: 380, loss: 0.3860884839090047\n",
      "epoch: 381, loss: 0.3858067519624241\n",
      "epoch: 382, loss: 0.3855257492182791\n",
      "epoch: 383, loss: 0.3852454720279167\n",
      "epoch: 384, loss: 0.38496591677034253\n",
      "epoch: 385, loss: 0.3846870798519377\n",
      "epoch: 386, loss: 0.3844089577061798\n",
      "epoch: 387, loss: 0.3841315467933667\n",
      "epoch: 388, loss: 0.3838548436003444\n",
      "epoch: 389, loss: 0.38357884464023795\n",
      "epoch: 390, loss: 0.38330354645218606\n",
      "epoch: 391, loss: 0.3830289456010785\n",
      "epoch: 392, loss: 0.3827550386772973\n",
      "epoch: 393, loss: 0.38248182229646094\n",
      "epoch: 394, loss: 0.38220929309917173\n",
      "epoch: 395, loss: 0.38193744775076655\n",
      "epoch: 396, loss: 0.3816662829410699\n",
      "epoch: 397, loss: 0.38139579538415125\n",
      "epoch: 398, loss: 0.3811259818180842\n",
      "epoch: 399, loss: 0.38085683900470924\n",
      "epoch: 400, loss: 0.38058836372939925\n",
      "epoch: 401, loss: 0.38032055280082816\n",
      "epoch: 402, loss: 0.38005340305074176\n",
      "epoch: 403, loss: 0.37978691133373177\n",
      "epoch: 404, loss: 0.3795210745270132\n",
      "epoch: 405, loss: 0.37925588953020284\n",
      "epoch: 406, loss: 0.3789913532651022\n",
      "epoch: 407, loss: 0.3787274626754822\n",
      "epoch: 408, loss: 0.3784642147268701\n",
      "epoch: 409, loss: 0.3782016064063398\n",
      "epoch: 410, loss: 0.37793963472230463\n",
      "epoch: 411, loss: 0.3776782967043116\n",
      "epoch: 412, loss: 0.37741758940283915\n",
      "epoch: 413, loss: 0.3771575098890973\n",
      "epoch: 414, loss: 0.37689805525482956\n",
      "epoch: 415, loss: 0.3766392226121173\n",
      "epoch: 416, loss: 0.37638100909318745\n",
      "epoch: 417, loss: 0.37612341185022075\n",
      "epoch: 418, loss: 0.37586642805516396\n",
      "epoch: 419, loss: 0.375610054899543\n",
      "epoch: 420, loss: 0.3753542895942793\n",
      "epoch: 421, loss: 0.37509912936950746\n",
      "epoch: 422, loss: 0.3748445714743958\n",
      "epoch: 423, loss: 0.3745906131769683\n",
      "epoch: 424, loss: 0.3743372517639292\n",
      "epoch: 425, loss: 0.37408448454048937\n",
      "epoch: 426, loss: 0.3738323088301947\n",
      "epoch: 427, loss: 0.37358072197475656\n",
      "epoch: 428, loss: 0.37332972133388437\n",
      "epoch: 429, loss: 0.3730793042851196\n",
      "epoch: 430, loss: 0.37282946822367236\n",
      "epoch: 431, loss: 0.3725802105622592\n",
      "epoch: 432, loss: 0.37233152873094355\n",
      "epoch: 433, loss: 0.37208342017697726\n",
      "epoch: 434, loss: 0.37183588236464427\n",
      "epoch: 435, loss: 0.37158891277510603\n",
      "epoch: 436, loss: 0.3713425089062491\n",
      "epoch: 437, loss: 0.3710966682725335\n",
      "epoch: 438, loss: 0.3708513884048436\n",
      "epoch: 439, loss: 0.3706066668503409\n",
      "epoch: 440, loss: 0.3703625011723174\n",
      "epoch: 441, loss: 0.3701188889500515\n",
      "epoch: 442, loss: 0.3698758277786658\n",
      "epoch: 443, loss: 0.369633315268985\n",
      "epoch: 444, loss: 0.36939134904739745\n",
      "epoch: 445, loss: 0.3691499267557166\n",
      "epoch: 446, loss: 0.36890904605104474\n",
      "epoch: 447, loss: 0.368668704605638\n",
      "epoch: 448, loss: 0.3684289001067733\n",
      "epoch: 449, loss: 0.3681896302566161\n",
      "epoch: 450, loss: 0.36795089277209014\n",
      "epoch: 451, loss: 0.36771268538474855\n",
      "epoch: 452, loss: 0.367475005840646\n",
      "epoch: 453, loss: 0.3672378519002129\n",
      "epoch: 454, loss: 0.3670012213381304\n",
      "epoch: 455, loss: 0.3667651119432069\n",
      "epoch: 456, loss: 0.36652952151825613\n",
      "epoch: 457, loss: 0.3662944478799768\n",
      "epoch: 458, loss: 0.36605988885883256\n",
      "epoch: 459, loss: 0.3658258422989341\n",
      "epoch: 460, loss: 0.36559230605792276\n",
      "epoch: 461, loss: 0.3653592780068544\n",
      "epoch: 462, loss: 0.36512675603008554\n",
      "epoch: 463, loss: 0.36489473802516015\n",
      "epoch: 464, loss: 0.36466322190269757\n",
      "epoch: 465, loss: 0.3644322055862823\n",
      "epoch: 466, loss: 0.3642016870123542\n",
      "epoch: 467, loss: 0.36397166413010057\n",
      "epoch: 468, loss: 0.36374213490134866\n",
      "epoch: 469, loss: 0.36351309730045966\n",
      "epoch: 470, loss: 0.36328454931422444\n",
      "epoch: 471, loss: 0.36305648894175907\n",
      "epoch: 472, loss: 0.3628289141944029\n",
      "epoch: 473, loss: 0.36260182309561617\n",
      "epoch: 474, loss: 0.36237521368088044\n",
      "epoch: 475, loss: 0.3621490839975984\n",
      "epoch: 476, loss: 0.361923432104996\n",
      "epoch: 477, loss: 0.36169825607402495\n",
      "epoch: 478, loss: 0.36147355398726644\n",
      "epoch: 479, loss: 0.3612493239388356\n",
      "epoch: 480, loss: 0.36102556403428754\n",
      "epoch: 481, loss: 0.36080227239052376\n",
      "epoch: 482, loss: 0.3605794471356998\n",
      "epoch: 483, loss: 0.36035708640913405\n",
      "epoch: 484, loss: 0.3601351883612171\n",
      "epoch: 485, loss: 0.3599137511533221\n",
      "epoch: 486, loss: 0.35969277295771634\n",
      "epoch: 487, loss: 0.3594722519574737\n",
      "epoch: 488, loss: 0.3592521863463874\n",
      "epoch: 489, loss: 0.35903257432888447\n",
      "epoch: 490, loss: 0.3588134141199406\n",
      "epoch: 491, loss: 0.3585947039449961\n",
      "epoch: 492, loss: 0.35837644203987223\n",
      "epoch: 493, loss: 0.3581586266506892\n",
      "epoch: 494, loss: 0.35794125603378435\n",
      "epoch: 495, loss: 0.3577243284556311\n",
      "epoch: 496, loss: 0.3575078421927596\n",
      "epoch: 497, loss: 0.3572917955316769\n",
      "epoch: 498, loss: 0.35707618676878883\n",
      "epoch: 499, loss: 0.35686101421032257\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCH):\n",
    "    total_loss = 0.0\n",
    "    for step, (x, y) in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = tf.matmul(x, w1) + b1\n",
    "            y_pred_prob = tf.nn.softmax(y_pred)\n",
    "            y_one_hot = tf.one_hot(y, depth=3, dtype=tf.float64)\n",
    "            loss = tf.reduce_mean(tf.square(y_one_hot - y_pred_prob))\n",
    "            total_loss = total_loss + loss.numpy()\n",
    "\n",
    "        grads = tape.gradient(loss, [w1, b1])\n",
    "        # 求梯度\n",
    "        w1.assign_sub(LR * grads[0])\n",
    "        b1.assign_sub(LR * grads[1])\n",
    "        # 更新参数\n",
    "    print('epoch: {}, loss: {}'.format(epoch, total_loss))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "total_correct = 0\n",
    "total_num = 0\n",
    "for x_test, y_test in test_dataset:\n",
    "    y = tf.matmul(x_test, w1) + b1\n",
    "    y = tf.nn.softmax(y)\n",
    "    pred = tf.argmax(y, axis=1)\n",
    "    pred = tf.cast(pred, dtype=y_test.dtype)\n",
    "    correct = tf.equal(pred, y_test)\n",
    "    correct = tf.cast(correct, dtype=tf.int32)\n",
    "    correct = tf.reduce_sum(correct)\n",
    "    total_correct = total_correct + int(correct)\n",
    "    total_num = total_num + x_test.shape[0]\n",
    "\n",
    "acc = total_correct / total_num\n",
    "print(acc)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}